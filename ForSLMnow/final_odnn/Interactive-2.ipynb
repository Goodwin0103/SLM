{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "已连接到 odnn_venv (Python 3.13.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d243f360-328c-4ea7-8753-364ee2d9fa86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device: cuda:1\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from scipy.io import savemat\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from ODNN_functions import (\n",
    "    create_evaluation_regions,\n",
    "    generate_complex_weights,\n",
    "    generate_fields_ts,\n",
    ")\n",
    "from odnn_generate_label import (\n",
    "    compute_label_centers,\n",
    "    compose_labels_from_patterns,\n",
    "    generate_detector_patterns,\n",
    ")\n",
    "from odnn_io import load_complex_modes_from_mat\n",
    "from odnn_model import D2NNModel\n",
    "from odnn_processing import prepare_sample\n",
    "from odnn_training_eval import (\n",
    "    build_superposition_eval_context,\n",
    "    compute_model_prediction_metrics,\n",
    "    evaluate_spot_metrics,\n",
    "    format_metric_report,\n",
    "    generate_superposition_sample,\n",
    "    infer_superposition_output,\n",
    ")\n",
    "from odnn_training_io import save_masks_one_file_per_layer, save_to_mat_light_plus\n",
    "from odnn_training_visualization import (\n",
    "    export_superposition_slices,\n",
    "    plot_amplitude_comparison_grid,\n",
    "    plot_reconstruction_vs_input,\n",
    "    plot_sys_vs_label_strict,\n",
    "    save_superposition_visuals,\n",
    "    visualize_model_slices,\n",
    ")\n",
    "\n",
    "SEED = 424242\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# 让 cuDNN/算子走确定性分支\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.use_deterministic_algorithms(True)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:1')           # 或者 'cuda:0'\n",
    "    print('Using Device:', device)\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('Using Device: CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bf9382-5728-4c5f-86ee-f45952b4adad",
   "metadata": {},
   "outputs": [],
   "source": [
    "field_size = 25  #the field size in eigenmodes_OM4 is 50 pixels\n",
    "layer_size = 100 #400#300#100\n",
    "num_data = 1000 # options: 1. random datas 2.eigenmodes\n",
    "num_modes = 6 #the mode number of MMF 3 6 10\n",
    "circle_focus_radius = 13 # radius when using uniform circular detectors\n",
    "circle_detectsize = 26  # square window size for circular detectors\n",
    "eigenmode_focus_radius = 12.5  # radius when using eigenmode patterns\n",
    "eigenmode_detectsize = 27    # square window size for eigenmode patterns\n",
    "focus_radius = circle_focus_radius\n",
    "detectsize = circle_detectsize\n",
    "batch_size = 16\n",
    "\n",
    "# Evaluation selection: \"eigenmode\" uses the base modes, \"superposition\" samples random mixtures\n",
    "evaluation_mode = \"eigenmode\"  # options: \"eigenmode\", \"superposition\"\n",
    "num_superposition_eval_samples = 1000\n",
    "run_superposition_debug = True\n",
    "save_superposition_plots = True\n",
    "save_superposition_slices = True\n",
    "label_pattern_mode = \"eigenmode\"  # options: \"eigenmode\", \"circle\"\n",
    "# Define multiple D2NN models \n",
    "num_layer_option = [3]   #, 3]#, 4]  # Define the different layer-number ODNN\n",
    "all_losses = [] #the loss for each epoch of each ODNN model\n",
    "all_phase_masks = [] #the phase masks field of each ODNN model\n",
    "all_predictions = [] #the output light field of each ODNN model\n",
    "model_metrics: list[dict] = []\n",
    "all_amplitudes_diff: list[np.ndarray] = []\n",
    "all_average_amplitudes_diff: list[float] = []\n",
    "all_amplitudes_relative_diff: list[float] = []\n",
    "all_complex_weights_pred: list[np.ndarray] = []\n",
    "all_image_data_pred: list[np.ndarray] = []\n",
    "all_cc_real: list[np.ndarray] = []\n",
    "all_cc_imag: list[np.ndarray] = []\n",
    "all_cc_recon_amp: list[np.ndarray] = []\n",
    "all_cc_recon_phase: list[np.ndarray] = []\n",
    "# SLM\n",
    "z_layers   = 40e-6        # 原 47.571e-3  -> 40 μm\n",
    "pixel_size = 1e-6\n",
    "z_prop     = 120e-6        # 原 16.74e-2   -> 60 μm plus 40（最后一层到相机）\n",
    "wavelength = 1568e-9      # 原 1568     -> 1550 nm\n",
    "z_input_to_first = 40e-6  # 40 μm # 新增：输入面到第一层的传播距离"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b031fb99-a215-4d47-bd18-a0b2ee5abc9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded modes shape: (25, 25, 6) dtype: complex64\n"
     ]
    }
   ],
   "source": [
    "eigenmodes_OM4 = load_complex_modes_from_mat(\n",
    "    'mmf_6modes_25_PD_1.15.mat',\n",
    "    key='modes_field'\n",
    ")\n",
    "# (H, W, M)\n",
    "print(\"Loaded modes shape:\", eigenmodes_OM4.shape, \"dtype:\", eigenmodes_OM4.dtype)\n",
    "\n",
    "# 取前 num_modes 个 → (H, W, M_sel) → (M_sel, H, W)\n",
    "MMF_data = eigenmodes_OM4[:, :, :num_modes].transpose(2, 0, 1)\n",
    "MMF_data_amp_norm = (np.abs(MMF_data) - np.min(np.abs(MMF_data))) / (np.max(np.abs(MMF_data)) - np.min(np.abs(MMF_data)))\n",
    "\n",
    "MMF_data = MMF_data_amp_norm * np.exp(1j * np.angle(MMF_data))\n",
    "\n",
    "#要是以后确定了用4我在想要不要去掉其他选项\n",
    "phase_option = 4\n",
    "#phase_option 1: (0,0,...,0)\n",
    "#phase_option 2: (0,2pi,...,2pi)\n",
    "#phase_option 3: (0,pi,...,2pi)\n",
    "#phase_option 4: eigenmodes\n",
    "#phase_option 5: (0,pi,...,pi)\n",
    "\n",
    "if phase_option in [1, 2, 3, 5]:\n",
    "    amplitudes,phases = generate_complex_weights(num_data,num_modes,phase_option)\n",
    "\n",
    "if phase_option == 4:\n",
    "    num_data = num_modes # use the eigenmodes to train ODNN\n",
    "    amplitudes = np.eye(num_modes)#[[1,0,0][0,1,0][0,0,1]]\n",
    "    phases = np.eye(num_modes)\n",
    "\n",
    "amplitudes_phases_ori = np.hstack((amplitudes[:, :], phases[:, 1:]))  # amplitudes (l2 norm) phases\n",
    "amplitudes_phases = np.hstack((amplitudes[:, :], phases[:, 1:]/(2*np.pi)))  # amplitudes (l2 norm) phases (0-1)\n",
    "\n",
    "# Generate complex weights vector with specified amplitudes and phases\n",
    "complex_weights = amplitudes * np.exp(1j * phases)\n",
    "MMF_data_ts = torch.from_numpy(MMF_data)\n",
    "complex_weights_ts = torch.from_numpy(complex_weights)\n",
    "image_data = generate_fields_ts(complex_weights_ts, MMF_data_ts, num_data, num_modes, field_size).to(torch.complex64)\n",
    "image_test_data = image_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ee96ef-95e9-40e7-92a6-7033790727f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "相邻图案边缘间距： 行=16.00, 列=5.50\n",
      "相邻图案中心间距： 行=42.00, 列=31.50\n",
      "中心坐标： [(29, 18), (29, 50), (29, 82), (71, 18), (71, 50), (71, 82)]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "pred_case = 1: only amplitudes prediction\n",
    "pred_case = 2: only phases prediction\n",
    "pred_case = 3: amplitudes and phases prediction\n",
    "pred_case = 4: amplitudes and phases prediction (extra energy phase area)\n",
    "'''\n",
    "#\n",
    "pred_case = 1\n",
    "label_data = torch.zeros([num_data,1,layer_size,layer_size])\n",
    "label_size = layer_size\n",
    "\n",
    "if pred_case == 1: # 3\n",
    "    num_detector = num_modes\n",
    "    detector_focus_radius = focus_radius\n",
    "    detector_detectsize = detectsize\n",
    "    if label_pattern_mode == \"eigenmode\":\n",
    "        pattern_stack = np.transpose(np.abs(MMF_data), (1, 2, 0))\n",
    "        pattern_h, pattern_w, _ = pattern_stack.shape\n",
    "        if pattern_h > label_size or pattern_w > label_size:\n",
    "            raise ValueError(\n",
    "                f\"Eigenmode pattern size ({pattern_h}x{pattern_w}) exceeds label canvas {label_size}.\"\n",
    "            )\n",
    "        layout_radius = math.ceil(max(pattern_h, pattern_w) / 2)\n",
    "        detector_focus_radius = eigenmode_focus_radius\n",
    "        detector_detectsize = eigenmode_detectsize\n",
    "    elif label_pattern_mode == \"circle\":\n",
    "        circle_radius = circle_focus_radius\n",
    "        pattern_size = circle_radius * 2\n",
    "        if pattern_size % 2 == 0:\n",
    "            pattern_size += 1  \n",
    "        pattern_stack = generate_detector_patterns(pattern_size, pattern_size, num_detector, shape=\"circle\")\n",
    "        layout_radius = circle_radius\n",
    "        detector_focus_radius = circle_radius\n",
    "        detector_detectsize = circle_detectsize\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown label_pattern_mode: {label_pattern_mode}\")\n",
    "\n",
    "    centers, _, _ = compute_label_centers(label_size, label_size, num_detector, layout_radius)\n",
    "    mode_label_maps = [\n",
    "        compose_labels_from_patterns(\n",
    "            label_size,\n",
    "            label_size,\n",
    "            pattern_stack,\n",
    "            centers,\n",
    "            Index=i + 1,\n",
    "            visualize=False,\n",
    "        )\n",
    "        for i in range(num_detector)\n",
    "    ]\n",
    "    MMF_Label_data = torch.from_numpy(\n",
    "        np.stack(mode_label_maps, axis=2).astype(np.float32)\n",
    "    )\n",
    "    amplitude_weights = torch.from_numpy(amplitudes_phases[:num_data, 0:num_modes]).float()\n",
    "    combined_labels = (\n",
    "        amplitude_weights[:, None, None, :] * MMF_Label_data.unsqueeze(0)\n",
    "    ).sum(dim=3)\n",
    "    label_data[:, 0, :, :] = combined_labels\n",
    "    focus_radius = detector_focus_radius\n",
    "    detectsize = detector_detectsize\n",
    "\n",
    "label_test_data = label_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cd1554-135f-4326-8b1b-0a254e08d7d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detection Regions: [(6, 32, 16, 42), (37, 63, 16, 42), (68, 94, 16, 42), (6, 32, 58, 84), (37, 63, 58, 84), (68, 94, 58, 84)]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = [\n",
    "    prepare_sample(image_data[i], label_data[i], layer_size) for i in range(len(label_data))\n",
    "]\n",
    "train_tensor_data = TensorDataset(*[torch.stack(tensors) for tensors in zip(*train_dataset)])\n",
    "g = torch.Generator()\n",
    "g.manual_seed(SEED)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_tensor_data,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,               # 顺序会被 g 固定\n",
    "    generator=g,                # 固定打乱\n",
    "   \n",
    ")\n",
    "\n",
    "if evaluation_mode == \"eigenmode\":\n",
    "    test_dataset = train_dataset\n",
    "    test_tensor_data = train_tensor_data\n",
    "    test_loader = DataLoader(test_tensor_data, batch_size=batch_size, shuffle=False)\n",
    "    eval_amplitudes = amplitudes\n",
    "    eval_amplitudes_phases = amplitudes_phases\n",
    "    eval_phases = phases\n",
    "    image_test_data = image_data\n",
    "elif evaluation_mode == \"superposition\":\n",
    "    if pred_case != 1:\n",
    "        raise ValueError(\"Superposition evaluation mode currently supports pred_case == 1 only.\")\n",
    "    super_ctx = build_superposition_eval_context(\n",
    "        num_superposition_eval_samples,\n",
    "        num_modes=num_modes,\n",
    "        field_size=field_size,\n",
    "        layer_size=layer_size,\n",
    "        mmf_modes=MMF_data_ts,\n",
    "        mmf_label_data=MMF_Label_data,\n",
    "        batch_size=batch_size,\n",
    "        second_mode_half_range=True,\n",
    "    )\n",
    "    test_dataset = super_ctx[\"dataset\"]\n",
    "    test_tensor_data = super_ctx[\"tensor_dataset\"]\n",
    "    test_loader = super_ctx[\"loader\"]\n",
    "    image_test_data = super_ctx[\"image_data\"]\n",
    "    eval_amplitudes = super_ctx[\"amplitudes\"]\n",
    "    eval_amplitudes_phases = super_ctx[\"amplitudes_phases\"]\n",
    "    eval_phases = super_ctx[\"phases\"]\n",
    "else:\n",
    "    raise ValueError(f\"Unknown evaluation_mode: {evaluation_mode}\")\n",
    "\n",
    "# Generate detection regions using existing function\n",
    "if pred_case ==1:\n",
    "    evaluation_regions = create_evaluation_regions(layer_size, layer_size, num_detector, focus_radius, detectsize)\n",
    "    print(\"Detection Regions:\", evaluation_regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647ac5ca-d9df-4572-bbc6-a15fb6b80145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training D2NN with 3 layers...\n",
      "\n",
      "D2NNModel(\n",
      "  (pre_propagation): Propagation()\n",
      "  (layers): ModuleList(\n",
      "    (0-2): 3 x DiffractionLayer()\n",
      "  )\n",
      "  (propagation): Propagation()\n",
      "  (regression): RegressionDetector()\n",
      ")\n",
      "Epoch [0/1000], Loss: 0.007294103503227234, Time: 8.50 seconds\n",
      "Epoch [100/1000], Loss: 0.003736137878149748, Time: 1.00 seconds\n",
      "Epoch [200/1000], Loss: 0.003663138253614306, Time: 1.01 seconds\n",
      "Epoch [300/1000], Loss: 0.003646797034889460, Time: 1.04 seconds\n",
      "Epoch [400/1000], Loss: 0.003643352771177888, Time: 1.06 seconds\n",
      "Epoch [500/1000], Loss: 0.003642342519015074, Time: 1.05 seconds\n",
      "Epoch [600/1000], Loss: 0.003641963470727205, Time: 1.08 seconds\n",
      "Epoch [700/1000], Loss: 0.003641816787421703, Time: 1.09 seconds\n",
      "Epoch [800/1000], Loss: 0.003641761373728514, Time: 1.02 seconds\n",
      "Epoch [900/1000], Loss: 0.003641740186139941, Time: 1.08 seconds\n",
      "✔ Saved model -> checkpoints/odnn_3layers.pth\n",
      "3 layers: modes=6, phase_opt=4, pred_case=1\n",
      "  amp_err=0.023463, amp_err_rel=0.057473\n",
      "  snr_full=0.993935, snr_crop=1.336885, throughput=0.733624\n",
      "  cc_amp=0.998357±0.000653, cc_phase=0.952105±0.025337, cc_real=0.999646±0.000171, cc_imag=1.000000±0.000000\n"
     ]
    }
   ],
   "source": [
    "for num_layer in num_layer_option:\n",
    "    print(f\"\\nTraining D2NN with {num_layer} layers...\\n\")\n",
    "\n",
    "    D2NN = D2NNModel(\n",
    "        num_layers=num_layer,\n",
    "        layer_size=layer_size,\n",
    "        z_layers=z_layers,\n",
    "        z_prop=z_prop,\n",
    "        pixel_size=pixel_size,\n",
    "        wavelength=wavelength,\n",
    "        device=device,\n",
    "        padding_ratio=0.5,\n",
    "        z_input_to_first=z_input_to_first,   # NEW\n",
    "    ).to(device)\n",
    "\n",
    "    print(D2NN)\n",
    "\n",
    "    # Training\n",
    "    criterion = nn.MSELoss()  # Define loss function (对比的是loss)\n",
    "    optimizer = optim.Adam(D2NN.parameters(), lr=1.99) \n",
    "    scheduler = ExponentialLR(optimizer, gamma=0.99)  \n",
    "    epochs = 1000\n",
    "    losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "        D2NN.train()\n",
    "        epoch_loss = 0\n",
    "        for images, labels in train_loader:\n",
    "            images = images.to(device, dtype=torch.complex64, non_blocking=True)\n",
    "            labels = labels.to(device, dtype=torch.float32,   non_blocking=True)\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            outputs = D2NN(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        scheduler.step()\n",
    "        avg_loss = epoch_loss / len(train_loader)  # Calculate average loss for the epoch\n",
    "        losses.append(avg_loss) # the loss for each model\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            print(f'Epoch [{epoch}/{epochs}], Loss: {avg_loss:.18f}, Time: {elapsed_time*100:.2f} seconds')\n",
    "    all_losses.append(losses) #save the loss for each model\n",
    "   \n",
    "    # === after training ===\n",
    "    ckpt_dir = \"checkpoints\"\n",
    "    os.makedirs(ckpt_dir, exist_ok=True)\n",
    "\n",
    "    ckpt = {\n",
    "        \"state_dict\": D2NN.state_dict(),\n",
    "        \"meta\": {\n",
    "            \"num_layers\":        len(D2NN.layers),\n",
    "            \"layer_size\":        layer_size,\n",
    "            \"z_layers\":          z_layers,\n",
    "            \"z_prop\":            z_prop,\n",
    "            \"pixel_size\":        pixel_size,\n",
    "            \"wavelength\":        wavelength,\n",
    "            \"padding_ratio\":     0.5,         \n",
    "            \"field_size\":        field_size,  \n",
    "            \"num_modes\":         num_modes, \n",
    "            \"z_input_to_first\":  z_input_to_first, \n",
    "        }\n",
    "    }\n",
    "    save_path = os.path.join(ckpt_dir, f\"odnn_{len(D2NN.layers)}layers.pth\")\n",
    "    torch.save(ckpt, save_path)\n",
    "    print(\"✔ Saved model ->\", save_path)\n",
    "    # Free GPU memory\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Cache phase masks for later visualization/export\n",
    "    phase_masks = []\n",
    "    for layer in D2NN.layers:\n",
    "        phase_np = layer.phase.detach().cpu().numpy()\n",
    "        phase_masks.append(np.remainder(phase_np, 2 * np.pi))\n",
    "    all_phase_masks.append(phase_masks)\n",
    "\n",
    "    # Collect evaluation metrics for this model\n",
    "    metrics = evaluate_spot_metrics(\n",
    "        D2NN,\n",
    "        test_loader,\n",
    "        evaluation_regions,\n",
    "        detect_radius=detectsize,\n",
    "        device=device,\n",
    "        pred_case=pred_case,\n",
    "        num_modes=num_modes,\n",
    "        phase_option=phase_option,\n",
    "        amplitudes=eval_amplitudes,\n",
    "        amplitudes_phases=eval_amplitudes_phases,\n",
    "        phases=eval_phases,\n",
    "        mmf_modes=MMF_data_ts,\n",
    "        field_size=field_size,\n",
    "        image_test_data=image_test_data,\n",
    "    )\n",
    "\n",
    "    model_metrics.append(metrics)\n",
    "    all_amplitudes_diff.append(metrics.get(\"amplitudes_diff\", np.array([])))\n",
    "    all_average_amplitudes_diff.append(float(metrics.get(\"avg_amplitudes_diff\", float(\"nan\"))))\n",
    "    all_amplitudes_relative_diff.append(float(metrics.get(\"avg_relative_amp_err\", float(\"nan\"))))\n",
    "    all_complex_weights_pred.append(metrics.get(\"complex_weights_pred\", np.array([])))\n",
    "    all_image_data_pred.append(metrics.get(\"image_data_pred\", np.array([])))\n",
    "    all_cc_recon_amp.append(metrics.get(\"cc_recon_amp\", np.array([])))\n",
    "    all_cc_recon_phase.append(metrics.get(\"cc_recon_phase\", np.array([])))\n",
    "    all_cc_real.append(metrics.get(\"cc_real\", np.array([])))\n",
    "    all_cc_imag.append(metrics.get(\"cc_imag\", np.array([])))\n",
    "\n",
    "    print(\n",
    "        format_metric_report(\n",
    "            num_modes=num_modes,\n",
    "            phase_option=phase_option,\n",
    "            pred_case=pred_case,\n",
    "            label=f\"{num_layer} layers\",\n",
    "            metrics=metrics,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abeeeb4-2f5a-4427-b263-f030537cbf3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device: cuda:5\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from scipy.io import savemat\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from ODNN_functions import (\n",
    "    create_evaluation_regions,\n",
    "    generate_complex_weights,\n",
    "    generate_fields_ts,\n",
    ")\n",
    "from odnn_generate_label import (\n",
    "    compute_label_centers,\n",
    "    compose_labels_from_patterns,\n",
    "    generate_detector_patterns,\n",
    ")\n",
    "from odnn_io import load_complex_modes_from_mat\n",
    "from odnn_model import D2NNModel\n",
    "from odnn_processing import prepare_sample\n",
    "from odnn_training_eval import (\n",
    "    build_superposition_eval_context,\n",
    "    compute_model_prediction_metrics,\n",
    "    evaluate_spot_metrics,\n",
    "    format_metric_report,\n",
    "    generate_superposition_sample,\n",
    "    infer_superposition_output,\n",
    ")\n",
    "from odnn_training_io import save_masks_one_file_per_layer, save_to_mat_light_plus\n",
    "from odnn_training_visualization import (\n",
    "    export_superposition_slices,\n",
    "    plot_amplitude_comparison_grid,\n",
    "    plot_reconstruction_vs_input,\n",
    "    plot_sys_vs_label_strict,\n",
    "    save_superposition_visuals,\n",
    "    visualize_model_slices,\n",
    ")\n",
    "\n",
    "SEED = 424242\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# 让 cuDNN/算子走确定性分支\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.use_deterministic_algorithms(True)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:5')           # 或者 'cuda:0'\n",
    "    print('Using Device:', device)\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('Using Device: CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81fa05f-85f8-4d7e-bb40-a659586f3899",
   "metadata": {},
   "outputs": [],
   "source": [
    "field_size = 25  #the field size in eigenmodes_OM4 is 50 pixels\n",
    "layer_size = 100 #400#300#100\n",
    "num_data = 1000 # options: 1. random datas 2.eigenmodes\n",
    "num_modes = 6 #the mode number of MMF 3 6 10\n",
    "circle_focus_radius = 13 # radius when using uniform circular detectors\n",
    "circle_detectsize = 26  # square window size for circular detectors\n",
    "eigenmode_focus_radius = 12.5  # radius when using eigenmode patterns\n",
    "eigenmode_detectsize = 27    # square window size for eigenmode patterns\n",
    "focus_radius = circle_focus_radius\n",
    "detectsize = circle_detectsize\n",
    "batch_size = 16\n",
    "\n",
    "# Evaluation selection: \"eigenmode\" uses the base modes, \"superposition\" samples random mixtures\n",
    "evaluation_mode = \"eigenmode\"  # options: \"eigenmode\", \"superposition\"\n",
    "num_superposition_eval_samples = 1000\n",
    "run_superposition_debug = True\n",
    "save_superposition_plots = True\n",
    "save_superposition_slices = True\n",
    "label_pattern_mode = \"eigenmode\"  # options: \"eigenmode\", \"circle\"\n",
    "# Define multiple D2NN models \n",
    "num_layer_option = [3]   #, 3]#, 4]  # Define the different layer-number ODNN\n",
    "all_losses = [] #the loss for each epoch of each ODNN model\n",
    "all_phase_masks = [] #the phase masks field of each ODNN model\n",
    "all_predictions = [] #the output light field of each ODNN model\n",
    "model_metrics: list[dict] = []\n",
    "all_amplitudes_diff: list[np.ndarray] = []\n",
    "all_average_amplitudes_diff: list[float] = []\n",
    "all_amplitudes_relative_diff: list[float] = []\n",
    "all_complex_weights_pred: list[np.ndarray] = []\n",
    "all_image_data_pred: list[np.ndarray] = []\n",
    "all_cc_real: list[np.ndarray] = []\n",
    "all_cc_imag: list[np.ndarray] = []\n",
    "all_cc_recon_amp: list[np.ndarray] = []\n",
    "all_cc_recon_phase: list[np.ndarray] = []\n",
    "# SLM\n",
    "z_layers   = 40e-6        # 原 47.571e-3  -> 40 μm\n",
    "pixel_size = 1e-6\n",
    "z_prop     = 120e-6        # 原 16.74e-2   -> 60 μm plus 40（最后一层到相机）\n",
    "wavelength = 1568e-9      # 原 1568     -> 1550 nm\n",
    "z_input_to_first = 40e-6  # 40 μm # 新增：输入面到第一层的传播距离"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34d9dcd-d196-4c4c-8b13-d0087d5ed71d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded modes shape: (25, 25, 6) dtype: complex64\n"
     ]
    }
   ],
   "source": [
    "eigenmodes_OM4 = load_complex_modes_from_mat(\n",
    "    'mmf_6modes_25_PD_1.15.mat',\n",
    "    key='modes_field'\n",
    ")\n",
    "# (H, W, M)\n",
    "print(\"Loaded modes shape:\", eigenmodes_OM4.shape, \"dtype:\", eigenmodes_OM4.dtype)\n",
    "\n",
    "# 取前 num_modes 个 → (H, W, M_sel) → (M_sel, H, W)\n",
    "MMF_data = eigenmodes_OM4[:, :, :num_modes].transpose(2, 0, 1)\n",
    "MMF_data_amp_norm = (np.abs(MMF_data) - np.min(np.abs(MMF_data))) / (np.max(np.abs(MMF_data)) - np.min(np.abs(MMF_data)))\n",
    "\n",
    "MMF_data = MMF_data_amp_norm * np.exp(1j * np.angle(MMF_data))\n",
    "\n",
    "#要是以后确定了用4我在想要不要去掉其他选项\n",
    "phase_option = 4\n",
    "#phase_option 1: (0,0,...,0)\n",
    "#phase_option 2: (0,2pi,...,2pi)\n",
    "#phase_option 3: (0,pi,...,2pi)\n",
    "#phase_option 4: eigenmodes\n",
    "#phase_option 5: (0,pi,...,pi)\n",
    "\n",
    "if phase_option in [1, 2, 3, 5]:\n",
    "    amplitudes,phases = generate_complex_weights(num_data,num_modes,phase_option)\n",
    "\n",
    "if phase_option == 4:\n",
    "    num_data = num_modes # use the eigenmodes to train ODNN\n",
    "    amplitudes = np.eye(num_modes)#[[1,0,0][0,1,0][0,0,1]]\n",
    "    phases = np.eye(num_modes)\n",
    "\n",
    "amplitudes_phases_ori = np.hstack((amplitudes[:, :], phases[:, 1:]))  # amplitudes (l2 norm) phases\n",
    "amplitudes_phases = np.hstack((amplitudes[:, :], phases[:, 1:]/(2*np.pi)))  # amplitudes (l2 norm) phases (0-1)\n",
    "\n",
    "# Generate complex weights vector with specified amplitudes and phases\n",
    "complex_weights = amplitudes * np.exp(1j * phases)\n",
    "MMF_data_ts = torch.from_numpy(MMF_data)\n",
    "complex_weights_ts = torch.from_numpy(complex_weights)\n",
    "image_data = generate_fields_ts(complex_weights_ts, MMF_data_ts, num_data, num_modes, field_size).to(torch.complex64)\n",
    "image_test_data = image_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905b39ac-2d8d-430e-9a72-28cd02679d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "相邻图案边缘间距： 行=16.00, 列=5.50\n",
      "相邻图案中心间距： 行=42.00, 列=31.50\n",
      "中心坐标： [(29, 18), (29, 50), (29, 82), (71, 18), (71, 50), (71, 82)]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "pred_case = 1: only amplitudes prediction\n",
    "pred_case = 2: only phases prediction\n",
    "pred_case = 3: amplitudes and phases prediction\n",
    "pred_case = 4: amplitudes and phases prediction (extra energy phase area)\n",
    "'''\n",
    "#\n",
    "pred_case = 1\n",
    "label_data = torch.zeros([num_data,1,layer_size,layer_size])\n",
    "label_size = layer_size\n",
    "\n",
    "if pred_case == 1: # 3\n",
    "    num_detector = num_modes\n",
    "    detector_focus_radius = focus_radius\n",
    "    detector_detectsize = detectsize\n",
    "    if label_pattern_mode == \"eigenmode\":\n",
    "        pattern_stack = np.transpose(np.abs(MMF_data), (1, 2, 0))\n",
    "        pattern_h, pattern_w, _ = pattern_stack.shape\n",
    "        if pattern_h > label_size or pattern_w > label_size:\n",
    "            raise ValueError(\n",
    "                f\"Eigenmode pattern size ({pattern_h}x{pattern_w}) exceeds label canvas {label_size}.\"\n",
    "            )\n",
    "        layout_radius = math.ceil(max(pattern_h, pattern_w) / 2)\n",
    "        detector_focus_radius = eigenmode_focus_radius\n",
    "        detector_detectsize = eigenmode_detectsize\n",
    "    elif label_pattern_mode == \"circle\":\n",
    "        circle_radius = circle_focus_radius\n",
    "        pattern_size = circle_radius * 2\n",
    "        if pattern_size % 2 == 0:\n",
    "            pattern_size += 1  \n",
    "        pattern_stack = generate_detector_patterns(pattern_size, pattern_size, num_detector, shape=\"circle\")\n",
    "        layout_radius = circle_radius\n",
    "        detector_focus_radius = circle_radius\n",
    "        detector_detectsize = circle_detectsize\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown label_pattern_mode: {label_pattern_mode}\")\n",
    "\n",
    "    centers, _, _ = compute_label_centers(label_size, label_size, num_detector, layout_radius)\n",
    "    mode_label_maps = [\n",
    "        compose_labels_from_patterns(\n",
    "            label_size,\n",
    "            label_size,\n",
    "            pattern_stack,\n",
    "            centers,\n",
    "            Index=i + 1,\n",
    "            visualize=False,\n",
    "        )\n",
    "        for i in range(num_detector)\n",
    "    ]\n",
    "    MMF_Label_data = torch.from_numpy(\n",
    "        np.stack(mode_label_maps, axis=2).astype(np.float32)\n",
    "    )\n",
    "    amplitude_weights = torch.from_numpy(amplitudes_phases[:num_data, 0:num_modes]).float()\n",
    "    combined_labels = (\n",
    "        amplitude_weights[:, None, None, :] * MMF_Label_data.unsqueeze(0)\n",
    "    ).sum(dim=3)\n",
    "    label_data[:, 0, :, :] = combined_labels\n",
    "    focus_radius = detector_focus_radius\n",
    "    detectsize = detector_detectsize\n",
    "\n",
    "label_test_data = label_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dff7008-f8b2-4947-8f75-4fe430cdf624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detection Regions: [(6, 32, 16, 42), (37, 63, 16, 42), (68, 94, 16, 42), (6, 32, 58, 84), (37, 63, 58, 84), (68, 94, 58, 84)]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = [\n",
    "    prepare_sample(image_data[i], label_data[i], layer_size) for i in range(len(label_data))\n",
    "]\n",
    "train_tensor_data = TensorDataset(*[torch.stack(tensors) for tensors in zip(*train_dataset)])\n",
    "g = torch.Generator()\n",
    "g.manual_seed(SEED)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_tensor_data,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,               # 顺序会被 g 固定\n",
    "    generator=g,                # 固定打乱\n",
    "   \n",
    ")\n",
    "\n",
    "if evaluation_mode == \"eigenmode\":\n",
    "    test_dataset = train_dataset\n",
    "    test_tensor_data = train_tensor_data\n",
    "    test_loader = DataLoader(test_tensor_data, batch_size=batch_size, shuffle=False)\n",
    "    eval_amplitudes = amplitudes\n",
    "    eval_amplitudes_phases = amplitudes_phases\n",
    "    eval_phases = phases\n",
    "    image_test_data = image_data\n",
    "elif evaluation_mode == \"superposition\":\n",
    "    if pred_case != 1:\n",
    "        raise ValueError(\"Superposition evaluation mode currently supports pred_case == 1 only.\")\n",
    "    super_ctx = build_superposition_eval_context(\n",
    "        num_superposition_eval_samples,\n",
    "        num_modes=num_modes,\n",
    "        field_size=field_size,\n",
    "        layer_size=layer_size,\n",
    "        mmf_modes=MMF_data_ts,\n",
    "        mmf_label_data=MMF_Label_data,\n",
    "        batch_size=batch_size,\n",
    "        second_mode_half_range=True,\n",
    "    )\n",
    "    test_dataset = super_ctx[\"dataset\"]\n",
    "    test_tensor_data = super_ctx[\"tensor_dataset\"]\n",
    "    test_loader = super_ctx[\"loader\"]\n",
    "    image_test_data = super_ctx[\"image_data\"]\n",
    "    eval_amplitudes = super_ctx[\"amplitudes\"]\n",
    "    eval_amplitudes_phases = super_ctx[\"amplitudes_phases\"]\n",
    "    eval_phases = super_ctx[\"phases\"]\n",
    "else:\n",
    "    raise ValueError(f\"Unknown evaluation_mode: {evaluation_mode}\")\n",
    "\n",
    "# Generate detection regions using existing function\n",
    "if pred_case ==1:\n",
    "    evaluation_regions = create_evaluation_regions(layer_size, layer_size, num_detector, focus_radius, detectsize)\n",
    "    print(\"Detection Regions:\", evaluation_regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010aa340-c07a-4ca5-b583-8af9c7bd2543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training D2NN with 3 layers...\n",
      "\n",
      "D2NNModel(\n",
      "  (pre_propagation): Propagation()\n",
      "  (layers): ModuleList(\n",
      "    (0-2): 3 x DiffractionLayer()\n",
      "  )\n",
      "  (propagation): Propagation()\n",
      "  (regression): RegressionDetector()\n",
      ")\n",
      "Epoch [0/1000], Loss: 0.007294103503227234, Time: 3.97 seconds\n",
      "Epoch [100/1000], Loss: 0.003736137878149748, Time: 1.03 seconds\n",
      "Epoch [200/1000], Loss: 0.003663138253614306, Time: 1.12 seconds\n",
      "Epoch [300/1000], Loss: 0.003646797034889460, Time: 1.11 seconds\n",
      "Epoch [400/1000], Loss: 0.003643352771177888, Time: 1.05 seconds\n",
      "Epoch [500/1000], Loss: 0.003642342519015074, Time: 1.10 seconds\n",
      "Epoch [600/1000], Loss: 0.003641963470727205, Time: 1.00 seconds\n",
      "Epoch [700/1000], Loss: 0.003641816787421703, Time: 1.01 seconds\n",
      "Epoch [800/1000], Loss: 0.003641761373728514, Time: 1.21 seconds\n",
      "Epoch [900/1000], Loss: 0.003641740186139941, Time: 1.12 seconds\n",
      "✔ Saved model -> checkpoints/odnn_3layers.pth\n",
      "3 layers: modes=6, phase_opt=4, pred_case=1\n",
      "  amp_err=0.023463, amp_err_rel=0.057473\n",
      "  snr_full=0.993935, snr_crop=1.336885, throughput=0.733624\n",
      "  cc_amp=0.998357±0.000653, cc_phase=0.952105±0.025337, cc_real=0.999646±0.000171, cc_imag=1.000000±0.000000\n"
     ]
    }
   ],
   "source": [
    "for num_layer in num_layer_option:\n",
    "    print(f\"\\nTraining D2NN with {num_layer} layers...\\n\")\n",
    "\n",
    "    D2NN = D2NNModel(\n",
    "        num_layers=num_layer,\n",
    "        layer_size=layer_size,\n",
    "        z_layers=z_layers,\n",
    "        z_prop=z_prop,\n",
    "        pixel_size=pixel_size,\n",
    "        wavelength=wavelength,\n",
    "        device=device,\n",
    "        padding_ratio=0.5,\n",
    "        z_input_to_first=z_input_to_first,   # NEW\n",
    "    ).to(device)\n",
    "\n",
    "    print(D2NN)\n",
    "\n",
    "    # Training\n",
    "    criterion = nn.MSELoss()  # Define loss function (对比的是loss)\n",
    "    optimizer = optim.Adam(D2NN.parameters(), lr=1.99) \n",
    "    scheduler = ExponentialLR(optimizer, gamma=0.99)  \n",
    "    epochs = 1000\n",
    "    losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "        D2NN.train()\n",
    "        epoch_loss = 0\n",
    "        for images, labels in train_loader:\n",
    "            images = images.to(device, dtype=torch.complex64, non_blocking=True)\n",
    "            labels = labels.to(device, dtype=torch.float32,   non_blocking=True)\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            outputs = D2NN(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        scheduler.step()\n",
    "        avg_loss = epoch_loss / len(train_loader)  # Calculate average loss for the epoch\n",
    "        losses.append(avg_loss) # the loss for each model\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            print(f'Epoch [{epoch}/{epochs}], Loss: {avg_loss:.18f}, Time: {elapsed_time*100:.2f} seconds')\n",
    "    all_losses.append(losses) #save the loss for each model\n",
    "   \n",
    "    # === after training ===\n",
    "    ckpt_dir = \"checkpoints\"\n",
    "    os.makedirs(ckpt_dir, exist_ok=True)\n",
    "\n",
    "    ckpt = {\n",
    "        \"state_dict\": D2NN.state_dict(),\n",
    "        \"meta\": {\n",
    "            \"num_layers\":        len(D2NN.layers),\n",
    "            \"layer_size\":        layer_size,\n",
    "            \"z_layers\":          z_layers,\n",
    "            \"z_prop\":            z_prop,\n",
    "            \"pixel_size\":        pixel_size,\n",
    "            \"wavelength\":        wavelength,\n",
    "            \"padding_ratio\":     0.5,         \n",
    "            \"field_size\":        field_size,  \n",
    "            \"num_modes\":         num_modes, \n",
    "            \"z_input_to_first\":  z_input_to_first, \n",
    "        }\n",
    "    }\n",
    "    save_path = os.path.join(ckpt_dir, f\"odnn_{len(D2NN.layers)}layers.pth\")\n",
    "    torch.save(ckpt, save_path)\n",
    "    print(\"✔ Saved model ->\", save_path)\n",
    "    # Free GPU memory\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Cache phase masks for later visualization/export\n",
    "    phase_masks = []\n",
    "    for layer in D2NN.layers:\n",
    "        phase_np = layer.phase.detach().cpu().numpy()\n",
    "        phase_masks.append(np.remainder(phase_np, 2 * np.pi))\n",
    "    all_phase_masks.append(phase_masks)\n",
    "\n",
    "    # Collect evaluation metrics for this model\n",
    "    metrics = evaluate_spot_metrics(\n",
    "        D2NN,\n",
    "        test_loader,\n",
    "        evaluation_regions,\n",
    "        detect_radius=detectsize,\n",
    "        device=device,\n",
    "        pred_case=pred_case,\n",
    "        num_modes=num_modes,\n",
    "        phase_option=phase_option,\n",
    "        amplitudes=eval_amplitudes,\n",
    "        amplitudes_phases=eval_amplitudes_phases,\n",
    "        phases=eval_phases,\n",
    "        mmf_modes=MMF_data_ts,\n",
    "        field_size=field_size,\n",
    "        image_test_data=image_test_data,\n",
    "    )\n",
    "\n",
    "    model_metrics.append(metrics)\n",
    "    all_amplitudes_diff.append(metrics.get(\"amplitudes_diff\", np.array([])))\n",
    "    all_average_amplitudes_diff.append(float(metrics.get(\"avg_amplitudes_diff\", float(\"nan\"))))\n",
    "    all_amplitudes_relative_diff.append(float(metrics.get(\"avg_relative_amp_err\", float(\"nan\"))))\n",
    "    all_complex_weights_pred.append(metrics.get(\"complex_weights_pred\", np.array([])))\n",
    "    all_image_data_pred.append(metrics.get(\"image_data_pred\", np.array([])))\n",
    "    all_cc_recon_amp.append(metrics.get(\"cc_recon_amp\", np.array([])))\n",
    "    all_cc_recon_phase.append(metrics.get(\"cc_recon_phase\", np.array([])))\n",
    "    all_cc_real.append(metrics.get(\"cc_real\", np.array([])))\n",
    "    all_cc_imag.append(metrics.get(\"cc_imag\", np.array([])))\n",
    "\n",
    "    print(\n",
    "        format_metric_report(\n",
    "            num_modes=num_modes,\n",
    "            phase_option=phase_option,\n",
    "            pred_case=pred_case,\n",
    "            label=f\"{num_layer} layers\",\n",
    "            metrics=metrics,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7dfb1db-847a-41ac-ba3d-486b7ae9bde4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training D2NN with 3 layers...\n",
      "\n",
      "D2NNModel(\n",
      "  (pre_propagation): Propagation()\n",
      "  (layers): ModuleList(\n",
      "    (0-2): 3 x DiffractionLayer()\n",
      "  )\n",
      "  (propagation): Propagation()\n",
      "  (regression): RegressionDetector()\n",
      ")\n",
      "Epoch [0/1000], Loss: 0.007294465322047472, Time: 1.13 seconds\n",
      "Epoch [100/1000], Loss: 0.003753445576876402, Time: 1.05 seconds\n",
      "Epoch [200/1000], Loss: 0.003712210571393371, Time: 1.02 seconds\n",
      "Epoch [300/1000], Loss: 0.003702156711369753, Time: 1.07 seconds\n",
      "Epoch [400/1000], Loss: 0.003698547370731831, Time: 1.19 seconds\n",
      "Epoch [500/1000], Loss: 0.003697032574564219, Time: 1.02 seconds\n",
      "Epoch [600/1000], Loss: 0.003696394385769963, Time: 1.04 seconds\n",
      "Epoch [700/1000], Loss: 0.003696147352457047, Time: 1.06 seconds\n",
      "Epoch [800/1000], Loss: 0.003696054453030229, Time: 1.18 seconds\n",
      "Epoch [900/1000], Loss: 0.003696019528433681, Time: 1.03 seconds\n",
      "✔ Saved model -> checkpoints/odnn_3layers.pth\n",
      "3 layers: modes=6, phase_opt=4, pred_case=1\n",
      "  amp_err=0.022938, amp_err_rel=0.056186\n",
      "  snr_full=0.963612, snr_crop=1.334333, throughput=0.710474\n",
      "  cc_amp=0.998386±0.000859, cc_phase=0.956021±0.025526, cc_real=0.999812±0.000164, cc_imag=1.000000±0.000000\n"
     ]
    }
   ],
   "source": [
    "for num_layer in num_layer_option:\n",
    "    print(f\"\\nTraining D2NN with {num_layer} layers...\\n\")\n",
    "\n",
    "    D2NN = D2NNModel(\n",
    "        num_layers=num_layer,\n",
    "        layer_size=layer_size,\n",
    "        z_layers=z_layers,\n",
    "        z_prop=z_prop,\n",
    "        pixel_size=pixel_size,\n",
    "        wavelength=wavelength,\n",
    "        device=device,\n",
    "        padding_ratio=0.5,\n",
    "        z_input_to_first=z_input_to_first,   # NEW\n",
    "    ).to(device)\n",
    "\n",
    "    print(D2NN)\n",
    "\n",
    "    # Training\n",
    "    criterion = nn.MSELoss()  # Define loss function (对比的是loss)\n",
    "    optimizer = optim.Adam(D2NN.parameters(), lr=1.99) \n",
    "    scheduler = ExponentialLR(optimizer, gamma=0.99)  \n",
    "    epochs = 1000\n",
    "    losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "        D2NN.train()\n",
    "        epoch_loss = 0\n",
    "        for images, labels in train_loader:\n",
    "            images = images.to(device, dtype=torch.complex64, non_blocking=True)\n",
    "            labels = labels.to(device, dtype=torch.float32,   non_blocking=True)\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            outputs = D2NN(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        scheduler.step()\n",
    "        avg_loss = epoch_loss / len(train_loader)  # Calculate average loss for the epoch\n",
    "        losses.append(avg_loss) # the loss for each model\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            print(f'Epoch [{epoch}/{epochs}], Loss: {avg_loss:.18f}, Time: {elapsed_time*100:.2f} seconds')\n",
    "    all_losses.append(losses) #save the loss for each model\n",
    "   \n",
    "    # === after training ===\n",
    "    ckpt_dir = \"checkpoints\"\n",
    "    os.makedirs(ckpt_dir, exist_ok=True)\n",
    "\n",
    "    ckpt = {\n",
    "        \"state_dict\": D2NN.state_dict(),\n",
    "        \"meta\": {\n",
    "            \"num_layers\":        len(D2NN.layers),\n",
    "            \"layer_size\":        layer_size,\n",
    "            \"z_layers\":          z_layers,\n",
    "            \"z_prop\":            z_prop,\n",
    "            \"pixel_size\":        pixel_size,\n",
    "            \"wavelength\":        wavelength,\n",
    "            \"padding_ratio\":     0.5,         \n",
    "            \"field_size\":        field_size,  \n",
    "            \"num_modes\":         num_modes, \n",
    "            \"z_input_to_first\":  z_input_to_first, \n",
    "        }\n",
    "    }\n",
    "    save_path = os.path.join(ckpt_dir, f\"odnn_{len(D2NN.layers)}layers.pth\")\n",
    "    torch.save(ckpt, save_path)\n",
    "    print(\"✔ Saved model ->\", save_path)\n",
    "    # Free GPU memory\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Cache phase masks for later visualization/export\n",
    "    phase_masks = []\n",
    "    for layer in D2NN.layers:\n",
    "        phase_np = layer.phase.detach().cpu().numpy()\n",
    "        phase_masks.append(np.remainder(phase_np, 2 * np.pi))\n",
    "    all_phase_masks.append(phase_masks)\n",
    "\n",
    "    # Collect evaluation metrics for this model\n",
    "    metrics = evaluate_spot_metrics(\n",
    "        D2NN,\n",
    "        test_loader,\n",
    "        evaluation_regions,\n",
    "        detect_radius=detectsize,\n",
    "        device=device,\n",
    "        pred_case=pred_case,\n",
    "        num_modes=num_modes,\n",
    "        phase_option=phase_option,\n",
    "        amplitudes=eval_amplitudes,\n",
    "        amplitudes_phases=eval_amplitudes_phases,\n",
    "        phases=eval_phases,\n",
    "        mmf_modes=MMF_data_ts,\n",
    "        field_size=field_size,\n",
    "        image_test_data=image_test_data,\n",
    "    )\n",
    "\n",
    "    model_metrics.append(metrics)\n",
    "    all_amplitudes_diff.append(metrics.get(\"amplitudes_diff\", np.array([])))\n",
    "    all_average_amplitudes_diff.append(float(metrics.get(\"avg_amplitudes_diff\", float(\"nan\"))))\n",
    "    all_amplitudes_relative_diff.append(float(metrics.get(\"avg_relative_amp_err\", float(\"nan\"))))\n",
    "    all_complex_weights_pred.append(metrics.get(\"complex_weights_pred\", np.array([])))\n",
    "    all_image_data_pred.append(metrics.get(\"image_data_pred\", np.array([])))\n",
    "    all_cc_recon_amp.append(metrics.get(\"cc_recon_amp\", np.array([])))\n",
    "    all_cc_recon_phase.append(metrics.get(\"cc_recon_phase\", np.array([])))\n",
    "    all_cc_real.append(metrics.get(\"cc_real\", np.array([])))\n",
    "    all_cc_imag.append(metrics.get(\"cc_imag\", np.array([])))\n",
    "\n",
    "    print(\n",
    "        format_metric_report(\n",
    "            num_modes=num_modes,\n",
    "            phase_option=phase_option,\n",
    "            pred_case=pred_case,\n",
    "            label=f\"{num_layer} layers\",\n",
    "            metrics=metrics,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4c6b6d-68ad-40c4-9ada-9a9b56757b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device: cuda:5\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from scipy.io import savemat\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from ODNN_functions import (\n",
    "    create_evaluation_regions,\n",
    "    generate_complex_weights,\n",
    "    generate_fields_ts,\n",
    ")\n",
    "from odnn_generate_label import (\n",
    "    compute_label_centers,\n",
    "    compose_labels_from_patterns,\n",
    "    generate_detector_patterns,\n",
    ")\n",
    "from odnn_io import load_complex_modes_from_mat\n",
    "from odnn_model import D2NNModel\n",
    "from odnn_processing import prepare_sample\n",
    "from odnn_training_eval import (\n",
    "    build_superposition_eval_context,\n",
    "    compute_model_prediction_metrics,\n",
    "    evaluate_spot_metrics,\n",
    "    format_metric_report,\n",
    "    generate_superposition_sample,\n",
    "    infer_superposition_output,\n",
    ")\n",
    "from odnn_training_io import save_masks_one_file_per_layer, save_to_mat_light_plus\n",
    "from odnn_training_visualization import (\n",
    "    export_superposition_slices,\n",
    "    plot_amplitude_comparison_grid,\n",
    "    plot_reconstruction_vs_input,\n",
    "    plot_sys_vs_label_strict,\n",
    "    save_superposition_visuals,\n",
    "    visualize_model_slices,\n",
    ")\n",
    "\n",
    "SEED = 424242\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# 让 cuDNN/算子走确定性分支\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.use_deterministic_algorithms(True)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:5')           # 或者 'cuda:0'\n",
    "    print('Using Device:', device)\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('Using Device: CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec36395-96af-4db6-9ba6-7d27ef08cc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "field_size = 25  #the field size in eigenmodes_OM4 is 50 pixels\n",
    "layer_size = 100 #400#300#100\n",
    "num_data = 1000 # options: 1. random datas 2.eigenmodes\n",
    "num_modes = 6 #the mode number of MMF 3 6 10\n",
    "circle_focus_radius = 13 # radius when using uniform circular detectors\n",
    "circle_detectsize = 26  # square window size for circular detectors\n",
    "eigenmode_focus_radius = 12.5  # radius when using eigenmode patterns\n",
    "eigenmode_detectsize = 27    # square window size for eigenmode patterns\n",
    "focus_radius = circle_focus_radius\n",
    "detectsize = circle_detectsize\n",
    "batch_size = 16\n",
    "\n",
    "# Evaluation selection: \"eigenmode\" uses the base modes, \"superposition\" samples random mixtures\n",
    "evaluation_mode = \"eigenmode\"  # options: \"eigenmode\", \"superposition\"\n",
    "num_superposition_eval_samples = 1000\n",
    "run_superposition_debug = True\n",
    "save_superposition_plots = True\n",
    "save_superposition_slices = True\n",
    "label_pattern_mode = \"eigenmode\"  # options: \"eigenmode\", \"circle\"\n",
    "# Define multiple D2NN models \n",
    "num_layer_option = [3]   #, 3]#, 4]  # Define the different layer-number ODNN\n",
    "all_losses = [] #the loss for each epoch of each ODNN model\n",
    "all_phase_masks = [] #the phase masks field of each ODNN model\n",
    "all_predictions = [] #the output light field of each ODNN model\n",
    "model_metrics: list[dict] = []\n",
    "all_amplitudes_diff: list[np.ndarray] = []\n",
    "all_average_amplitudes_diff: list[float] = []\n",
    "all_amplitudes_relative_diff: list[float] = []\n",
    "all_complex_weights_pred: list[np.ndarray] = []\n",
    "all_image_data_pred: list[np.ndarray] = []\n",
    "all_cc_real: list[np.ndarray] = []\n",
    "all_cc_imag: list[np.ndarray] = []\n",
    "all_cc_recon_amp: list[np.ndarray] = []\n",
    "all_cc_recon_phase: list[np.ndarray] = []\n",
    "all_training_summaries: list[dict] = []\n",
    "# SLM\n",
    "z_layers   = 40e-6        # 原 47.571e-3  -> 40 μm\n",
    "pixel_size = 1e-6\n",
    "z_prop     = 120e-6        # 原 16.74e-2   -> 60 μm plus 40（最后一层到相机）\n",
    "wavelength = 1568e-9      # 原 1568     -> 1550 nm\n",
    "z_input_to_first = 40e-6  # 40 μm # 新增：输入面到第一层的传播距离"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b153a7d9-a4f4-4705-8c7d-37ecc590143a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded modes shape: (25, 25, 6) dtype: complex64\n"
     ]
    }
   ],
   "source": [
    "eigenmodes_OM4 = load_complex_modes_from_mat(\n",
    "    'mmf_6modes_25_PD_1.15.mat',\n",
    "    key='modes_field'\n",
    ")\n",
    "# (H, W, M)\n",
    "print(\"Loaded modes shape:\", eigenmodes_OM4.shape, \"dtype:\", eigenmodes_OM4.dtype)\n",
    "\n",
    "# 取前 num_modes 个 → (H, W, M_sel) → (M_sel, H, W)\n",
    "MMF_data = eigenmodes_OM4[:, :, :num_modes].transpose(2, 0, 1)\n",
    "MMF_data_amp_norm = (np.abs(MMF_data) - np.min(np.abs(MMF_data))) / (np.max(np.abs(MMF_data)) - np.min(np.abs(MMF_data)))\n",
    "\n",
    "MMF_data = MMF_data_amp_norm * np.exp(1j * np.angle(MMF_data))\n",
    "\n",
    "#要是以后确定了用4我在想要不要去掉其他选项\n",
    "phase_option = 4\n",
    "#phase_option 1: (0,0,...,0)\n",
    "#phase_option 2: (0,2pi,...,2pi)\n",
    "#phase_option 3: (0,pi,...,2pi)\n",
    "#phase_option 4: eigenmodes\n",
    "#phase_option 5: (0,pi,...,pi)\n",
    "\n",
    "if phase_option in [1, 2, 3, 5]:\n",
    "    amplitudes,phases = generate_complex_weights(num_data,num_modes,phase_option)\n",
    "\n",
    "if phase_option == 4:\n",
    "    num_data = num_modes # use the eigenmodes to train ODNN\n",
    "    amplitudes = np.eye(num_modes)#[[1,0,0][0,1,0][0,0,1]]\n",
    "    phases = np.eye(num_modes)\n",
    "\n",
    "amplitudes_phases_ori = np.hstack((amplitudes[:, :], phases[:, 1:]))  # amplitudes (l2 norm) phases\n",
    "amplitudes_phases = np.hstack((amplitudes[:, :], phases[:, 1:]/(2*np.pi)))  # amplitudes (l2 norm) phases (0-1)\n",
    "\n",
    "# Generate complex weights vector with specified amplitudes and phases\n",
    "complex_weights = amplitudes * np.exp(1j * phases)\n",
    "MMF_data_ts = torch.from_numpy(MMF_data)\n",
    "complex_weights_ts = torch.from_numpy(complex_weights)\n",
    "image_data = generate_fields_ts(complex_weights_ts, MMF_data_ts, num_data, num_modes, field_size).to(torch.complex64)\n",
    "image_test_data = image_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc0e2b2-a3d8-4baf-ae65-4d73c2bbd8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "相邻图案边缘间距： 行=16.00, 列=5.50\n",
      "相邻图案中心间距： 行=42.00, 列=31.50\n",
      "中心坐标： [(29, 18), (29, 50), (29, 82), (71, 18), (71, 50), (71, 82)]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "pred_case = 1: only amplitudes prediction\n",
    "pred_case = 2: only phases prediction\n",
    "pred_case = 3: amplitudes and phases prediction\n",
    "pred_case = 4: amplitudes and phases prediction (extra energy phase area)\n",
    "'''\n",
    "#\n",
    "pred_case = 1\n",
    "label_data = torch.zeros([num_data,1,layer_size,layer_size])\n",
    "label_size = layer_size\n",
    "\n",
    "if pred_case == 1: # 3\n",
    "    num_detector = num_modes\n",
    "    detector_focus_radius = focus_radius\n",
    "    detector_detectsize = detectsize\n",
    "    if label_pattern_mode == \"eigenmode\":\n",
    "        pattern_stack = np.transpose(np.abs(MMF_data), (1, 2, 0))\n",
    "        pattern_h, pattern_w, _ = pattern_stack.shape\n",
    "        if pattern_h > label_size or pattern_w > label_size:\n",
    "            raise ValueError(\n",
    "                f\"Eigenmode pattern size ({pattern_h}x{pattern_w}) exceeds label canvas {label_size}.\"\n",
    "            )\n",
    "        layout_radius = math.ceil(max(pattern_h, pattern_w) / 2)\n",
    "        detector_focus_radius = eigenmode_focus_radius\n",
    "        detector_detectsize = eigenmode_detectsize\n",
    "    elif label_pattern_mode == \"circle\":\n",
    "        circle_radius = circle_focus_radius\n",
    "        pattern_size = circle_radius * 2\n",
    "        if pattern_size % 2 == 0:\n",
    "            pattern_size += 1  \n",
    "        pattern_stack = generate_detector_patterns(pattern_size, pattern_size, num_detector, shape=\"circle\")\n",
    "        layout_radius = circle_radius\n",
    "        detector_focus_radius = circle_radius\n",
    "        detector_detectsize = circle_detectsize\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown label_pattern_mode: {label_pattern_mode}\")\n",
    "\n",
    "    centers, _, _ = compute_label_centers(label_size, label_size, num_detector, layout_radius)\n",
    "    mode_label_maps = [\n",
    "        compose_labels_from_patterns(\n",
    "            label_size,\n",
    "            label_size,\n",
    "            pattern_stack,\n",
    "            centers,\n",
    "            Index=i + 1,\n",
    "            visualize=False,\n",
    "        )\n",
    "        for i in range(num_detector)\n",
    "    ]\n",
    "    MMF_Label_data = torch.from_numpy(\n",
    "        np.stack(mode_label_maps, axis=2).astype(np.float32)\n",
    "    )\n",
    "    amplitude_weights = torch.from_numpy(amplitudes_phases[:num_data, 0:num_modes]).float()\n",
    "    combined_labels = (\n",
    "        amplitude_weights[:, None, None, :] * MMF_Label_data.unsqueeze(0)\n",
    "    ).sum(dim=3)\n",
    "    label_data[:, 0, :, :] = combined_labels\n",
    "    focus_radius = detector_focus_radius\n",
    "    detectsize = detector_detectsize\n",
    "\n",
    "label_test_data = label_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45e70ea-e1af-4bad-9623-172948d22de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detection Regions: [(6, 32, 16, 42), (37, 63, 16, 42), (68, 94, 16, 42), (6, 32, 58, 84), (37, 63, 58, 84), (68, 94, 58, 84)]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = [\n",
    "    prepare_sample(image_data[i], label_data[i], layer_size) for i in range(len(label_data))\n",
    "]\n",
    "train_tensor_data = TensorDataset(*[torch.stack(tensors) for tensors in zip(*train_dataset)])\n",
    "g = torch.Generator()\n",
    "g.manual_seed(SEED)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_tensor_data,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,               # 顺序会被 g 固定\n",
    "    generator=g,                # 固定打乱\n",
    "   \n",
    ")\n",
    "\n",
    "if evaluation_mode == \"eigenmode\":\n",
    "    test_dataset = train_dataset\n",
    "    test_tensor_data = train_tensor_data\n",
    "    test_loader = DataLoader(test_tensor_data, batch_size=batch_size, shuffle=False)\n",
    "    eval_amplitudes = amplitudes\n",
    "    eval_amplitudes_phases = amplitudes_phases\n",
    "    eval_phases = phases\n",
    "    image_test_data = image_data\n",
    "elif evaluation_mode == \"superposition\":\n",
    "    if pred_case != 1:\n",
    "        raise ValueError(\"Superposition evaluation mode currently supports pred_case == 1 only.\")\n",
    "    super_ctx = build_superposition_eval_context(\n",
    "        num_superposition_eval_samples,\n",
    "        num_modes=num_modes,\n",
    "        field_size=field_size,\n",
    "        layer_size=layer_size,\n",
    "        mmf_modes=MMF_data_ts,\n",
    "        mmf_label_data=MMF_Label_data,\n",
    "        batch_size=batch_size,\n",
    "        second_mode_half_range=True,\n",
    "    )\n",
    "    test_dataset = super_ctx[\"dataset\"]\n",
    "    test_tensor_data = super_ctx[\"tensor_dataset\"]\n",
    "    test_loader = super_ctx[\"loader\"]\n",
    "    image_test_data = super_ctx[\"image_data\"]\n",
    "    eval_amplitudes = super_ctx[\"amplitudes\"]\n",
    "    eval_amplitudes_phases = super_ctx[\"amplitudes_phases\"]\n",
    "    eval_phases = super_ctx[\"phases\"]\n",
    "else:\n",
    "    raise ValueError(f\"Unknown evaluation_mode: {evaluation_mode}\")\n",
    "\n",
    "# Generate detection regions using existing function\n",
    "if pred_case ==1:\n",
    "    evaluation_regions = create_evaluation_regions(layer_size, layer_size, num_detector, focus_radius, detectsize)\n",
    "    print(\"Detection Regions:\", evaluation_regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d8eab3-fe6b-4043-9b20-d05a7ae2055c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training D2NN with 3 layers...\n",
      "\n",
      "D2NNModel(\n",
      "  (pre_propagation): Propagation()\n",
      "  (layers): ModuleList(\n",
      "    (0-2): 3 x DiffractionLayer()\n",
      "  )\n",
      "  (propagation): Propagation()\n",
      "  (regression): RegressionDetector()\n",
      ")\n",
      "Epoch [1/1000], Loss: 0.007294103503227234, Epoch Time: 0.01 seconds\n",
      "Epoch [100/1000], Loss: 0.003737372811883688, Epoch Time: 0.01 seconds\n",
      "Epoch [200/1000], Loss: 0.003663503332063556, Epoch Time: 0.01 seconds\n",
      "Epoch [300/1000], Loss: 0.003646859433501959, Epoch Time: 0.01 seconds\n",
      "Epoch [400/1000], Loss: 0.003643369767814875, Epoch Time: 0.01 seconds\n",
      "Epoch [500/1000], Loss: 0.003642348339781165, Epoch Time: 0.01 seconds\n",
      "Epoch [600/1000], Loss: 0.003641965799033642, Epoch Time: 0.01 seconds\n",
      "Epoch [700/1000], Loss: 0.003641817718744278, Epoch Time: 0.01 seconds\n",
      "Epoch [800/1000], Loss: 0.003641761373728514, Epoch Time: 0.01 seconds\n",
      "Epoch [900/1000], Loss: 0.003641740418970585, Epoch Time: 0.01 seconds\n",
      "Epoch [1000/1000], Loss: 0.003641732502728701, Epoch Time: 0.01 seconds\n",
      "Total training time for 3-layer model: 10.53 seconds (~0.18 minutes)\n",
      "✔ Saved training loss plot -> results/training_analysis/loss_curve_layers3_20251103_155057.png\n",
      "✔ Saved cumulative time plot -> results/training_analysis/epoch_time_layers3_20251103_155057.png\n",
      "✔ Saved training log data (.mat) -> results/training_analysis/training_curves_layers3_20251103_155057.mat\n",
      "✔ Saved model -> checkpoints/odnn_3layers.pth\n",
      "3 layers: modes=6, phase_opt=4, pred_case=1\n",
      "  amp_err=0.023463, amp_err_rel=0.057473\n",
      "  snr_full=0.993935, snr_crop=1.336885, throughput=0.733624\n",
      "  cc_amp=0.998357±0.000653, cc_phase=0.952105±0.025337, cc_real=0.999646±0.000171, cc_imag=1.000000±0.000000\n"
     ]
    }
   ],
   "source": [
    "for num_layer in num_layer_option:\n",
    "    print(f\"\\nTraining D2NN with {num_layer} layers...\\n\")\n",
    "\n",
    "    D2NN = D2NNModel(\n",
    "        num_layers=num_layer,\n",
    "        layer_size=layer_size,\n",
    "        z_layers=z_layers,\n",
    "        z_prop=z_prop,\n",
    "        pixel_size=pixel_size,\n",
    "        wavelength=wavelength,\n",
    "        device=device,\n",
    "        padding_ratio=0.5,\n",
    "        z_input_to_first=z_input_to_first,   # NEW\n",
    "    ).to(device)\n",
    "\n",
    "    print(D2NN)\n",
    "\n",
    "    # Training\n",
    "    criterion = nn.MSELoss()  # Define loss function (对比的是loss)\n",
    "    optimizer = optim.Adam(D2NN.parameters(), lr=1.99) \n",
    "    scheduler = ExponentialLR(optimizer, gamma=0.99)  \n",
    "    epochs = 1000\n",
    "    losses = []\n",
    "    epoch_durations: list[float] = []\n",
    "    training_start_time = time.time()\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        D2NN.train()\n",
    "        epoch_loss = 0\n",
    "        for images, labels in train_loader:\n",
    "            images = images.to(device, dtype=torch.complex64, non_blocking=True)\n",
    "            labels = labels.to(device, dtype=torch.float32,   non_blocking=True)\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            outputs = D2NN(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        scheduler.step()\n",
    "        avg_loss = epoch_loss / len(train_loader)  # Calculate average loss for the epoch\n",
    "        losses.append(avg_loss)  # the loss for each model\n",
    "        epoch_duration = time.time() - epoch_start_time\n",
    "        epoch_durations.append(epoch_duration)\n",
    "\n",
    "        if epoch % 100 == 0 or epoch == 1 or epoch == epochs:\n",
    "            print(\n",
    "                f'Epoch [{epoch}/{epochs}], Loss: {avg_loss:.18f}, '\n",
    "                f'Epoch Time: {epoch_duration:.2f} seconds'\n",
    "            )\n",
    "\n",
    "    total_training_time = time.time() - training_start_time\n",
    "    print(\n",
    "        f'Total training time for {num_layer}-layer model: {total_training_time:.2f} seconds '\n",
    "        f'(~{total_training_time / 60:.2f} minutes)'\n",
    "    )\n",
    "    all_losses.append(losses)  # save the loss for each model\n",
    "    training_output_dir = Path(\"results/training_analysis\")\n",
    "    training_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    epochs_array = np.arange(1, epochs + 1, dtype=np.int32)\n",
    "    cumulative_epoch_times = np.cumsum(epoch_durations)\n",
    "    timestamp_tag = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(epochs_array, losses, label=\"Training Loss\")\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.set_title(f\"D2NN Training Loss ({num_layer} layers)\")\n",
    "    ax.grid(True, which=\"both\", linestyle=\"--\", linewidth=0.5)\n",
    "    ax.legend()\n",
    "    loss_plot_path = training_output_dir / f\"loss_curve_layers{num_layer}_{timestamp_tag}.png\"\n",
    "    fig.savefig(loss_plot_path, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "    fig_time, ax_time = plt.subplots()\n",
    "    ax_time.plot(epochs_array, cumulative_epoch_times, label=\"Cumulative Time\")\n",
    "    ax_time.set_xlabel(\"Epoch\")\n",
    "    ax_time.set_ylabel(\"Time (seconds)\")\n",
    "    ax_time.set_title(f\"Cumulative Training Time ({num_layer} layers)\")\n",
    "    ax_time.grid(True, which=\"both\", linestyle=\"--\", linewidth=0.5)\n",
    "    ax_time.legend()\n",
    "    time_plot_path = training_output_dir / f\"epoch_time_layers{num_layer}_{timestamp_tag}.png\"\n",
    "    fig_time.savefig(time_plot_path, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close(fig_time)\n",
    "\n",
    "    mat_path = training_output_dir / f\"training_curves_layers{num_layer}_{timestamp_tag}.mat\"\n",
    "    savemat(\n",
    "        str(mat_path),\n",
    "        {\n",
    "            \"epochs\": epochs_array,\n",
    "            \"losses\": np.array(losses, dtype=np.float64),\n",
    "            \"epoch_durations\": np.array(epoch_durations, dtype=np.float64),\n",
    "            \"cumulative_epoch_times\": np.array(cumulative_epoch_times, dtype=np.float64),\n",
    "            \"total_training_time\": np.array([total_training_time], dtype=np.float64),\n",
    "            \"num_layers\": np.array([num_layer], dtype=np.int32),\n",
    "        },\n",
    "    )\n",
    "\n",
    "    print(f\"✔ Saved training loss plot -> {loss_plot_path}\")\n",
    "    print(f\"✔ Saved cumulative time plot -> {time_plot_path}\")\n",
    "    print(f\"✔ Saved training log data (.mat) -> {mat_path}\")\n",
    "    all_training_summaries.append(\n",
    "        {\n",
    "            \"num_layers\": num_layer,\n",
    "            \"total_time\": total_training_time,\n",
    "            \"loss_plot\": str(loss_plot_path),\n",
    "            \"time_plot\": str(time_plot_path),\n",
    "            \"mat_path\": str(mat_path),\n",
    "        }\n",
    "    )\n",
    "   \n",
    "    # === after training ===\n",
    "    ckpt_dir = \"checkpoints\"\n",
    "    os.makedirs(ckpt_dir, exist_ok=True)\n",
    "\n",
    "    ckpt = {\n",
    "        \"state_dict\": D2NN.state_dict(),\n",
    "        \"meta\": {\n",
    "            \"num_layers\":        len(D2NN.layers),\n",
    "            \"layer_size\":        layer_size,\n",
    "            \"z_layers\":          z_layers,\n",
    "            \"z_prop\":            z_prop,\n",
    "            \"pixel_size\":        pixel_size,\n",
    "            \"wavelength\":        wavelength,\n",
    "            \"padding_ratio\":     0.5,         \n",
    "            \"field_size\":        field_size,  \n",
    "            \"num_modes\":         num_modes, \n",
    "            \"z_input_to_first\":  z_input_to_first, \n",
    "        }\n",
    "    }\n",
    "    save_path = os.path.join(ckpt_dir, f\"odnn_{len(D2NN.layers)}layers.pth\")\n",
    "    torch.save(ckpt, save_path)\n",
    "    print(\"✔ Saved model ->\", save_path)\n",
    "    # Free GPU memory\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Cache phase masks for later visualization/export\n",
    "    phase_masks = []\n",
    "    for layer in D2NN.layers:\n",
    "        phase_np = layer.phase.detach().cpu().numpy()\n",
    "        phase_masks.append(np.remainder(phase_np, 2 * np.pi))\n",
    "    all_phase_masks.append(phase_masks)\n",
    "\n",
    "    # Collect evaluation metrics for this model\n",
    "    metrics = evaluate_spot_metrics(\n",
    "        D2NN,\n",
    "        test_loader,\n",
    "        evaluation_regions,\n",
    "        detect_radius=detectsize,\n",
    "        device=device,\n",
    "        pred_case=pred_case,\n",
    "        num_modes=num_modes,\n",
    "        phase_option=phase_option,\n",
    "        amplitudes=eval_amplitudes,\n",
    "        amplitudes_phases=eval_amplitudes_phases,\n",
    "        phases=eval_phases,\n",
    "        mmf_modes=MMF_data_ts,\n",
    "        field_size=field_size,\n",
    "        image_test_data=image_test_data,\n",
    "    )\n",
    "\n",
    "    model_metrics.append(metrics)\n",
    "    all_amplitudes_diff.append(metrics.get(\"amplitudes_diff\", np.array([])))\n",
    "    all_average_amplitudes_diff.append(float(metrics.get(\"avg_amplitudes_diff\", float(\"nan\"))))\n",
    "    all_amplitudes_relative_diff.append(float(metrics.get(\"avg_relative_amp_err\", float(\"nan\"))))\n",
    "    all_complex_weights_pred.append(metrics.get(\"complex_weights_pred\", np.array([])))\n",
    "    all_image_data_pred.append(metrics.get(\"image_data_pred\", np.array([])))\n",
    "    all_cc_recon_amp.append(metrics.get(\"cc_recon_amp\", np.array([])))\n",
    "    all_cc_recon_phase.append(metrics.get(\"cc_recon_phase\", np.array([])))\n",
    "    all_cc_real.append(metrics.get(\"cc_real\", np.array([])))\n",
    "    all_cc_imag.append(metrics.get(\"cc_imag\", np.array([])))\n",
    "\n",
    "    print(\n",
    "        format_metric_report(\n",
    "            num_modes=num_modes,\n",
    "            phase_option=phase_option,\n",
    "            pred_case=pred_case,\n",
    "            label=f\"{num_layer} layers\",\n",
    "            metrics=metrics,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2b81db-5f08-4429-a033-56ba6ef96060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training duration summary:\n",
      " - 3 layers: 10.53 s (~0.18 min)\n",
      "   Loss curve: results/training_analysis/loss_curve_layers3_20251103_155057.png\n",
      "   Time curve: results/training_analysis/epoch_time_layers3_20251103_155057.png\n",
      "   Data (.mat): results/training_analysis/training_curves_layers3_20251103_155057.mat\n"
     ]
    }
   ],
   "source": [
    "if all_training_summaries:\n",
    "    print(\"\\nTraining duration summary:\")\n",
    "    for summary in all_training_summaries:\n",
    "        minutes = summary[\"total_time\"] / 60\n",
    "        print(\n",
    "            f\" - {summary['num_layers']} layers: {summary['total_time']:.2f} s \"\n",
    "            f\"(~{minutes:.2f} min)\"\n",
    "        )\n",
    "        print(f\"   Loss curve: {summary['loss_plot']}\")\n",
    "        print(f\"   Time curve: {summary['time_plot']}\")\n",
    "        print(f\"   Data (.mat): {summary['mat_path']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c014ba-8a19-4fd7-a80d-6db2ceab85e9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'capture_eigenmode_propagation' from 'odnn_training_visualization' (/home/ydzhang/Desktop/odnn_code/odnn_training_visualization.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 43\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01modnn_training_eval\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     35\u001b[39m     build_superposition_eval_context,\n\u001b[32m     36\u001b[39m     compute_model_prediction_metrics,\n\u001b[32m   (...)\u001b[39m\u001b[32m     40\u001b[39m     infer_superposition_output,\n\u001b[32m     41\u001b[39m )\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01modnn_training_io\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m save_masks_one_file_per_layer, save_to_mat_light_plus\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01modnn_training_visualization\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     44\u001b[39m     capture_eigenmode_propagation,\n\u001b[32m     45\u001b[39m     export_superposition_slices,\n\u001b[32m     46\u001b[39m     plot_amplitude_comparison_grid,\n\u001b[32m     47\u001b[39m     plot_reconstruction_vs_input,\n\u001b[32m     48\u001b[39m     plot_sys_vs_label_strict,\n\u001b[32m     49\u001b[39m     save_superposition_visuals,\n\u001b[32m     50\u001b[39m     visualize_model_slices,\n\u001b[32m     51\u001b[39m )\n\u001b[32m     53\u001b[39m SEED = \u001b[32m424242\u001b[39m\n\u001b[32m     54\u001b[39m random.seed(SEED)\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'capture_eigenmode_propagation' from 'odnn_training_visualization' (/home/ydzhang/Desktop/odnn_code/odnn_training_visualization.py)"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from scipy.io import savemat\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from ODNN_functions import (\n",
    "    create_evaluation_regions,\n",
    "    generate_complex_weights,\n",
    "    generate_fields_ts,\n",
    ")\n",
    "from odnn_generate_label import (\n",
    "    compute_label_centers,\n",
    "    compose_labels_from_patterns,\n",
    "    generate_detector_patterns,\n",
    ")\n",
    "from odnn_io import load_complex_modes_from_mat\n",
    "from odnn_model import D2NNModel\n",
    "from odnn_processing import prepare_sample\n",
    "from odnn_training_eval import (\n",
    "    build_superposition_eval_context,\n",
    "    compute_model_prediction_metrics,\n",
    "    evaluate_spot_metrics,\n",
    "    format_metric_report,\n",
    "    generate_superposition_sample,\n",
    "    infer_superposition_output,\n",
    ")\n",
    "from odnn_training_io import save_masks_one_file_per_layer, save_to_mat_light_plus\n",
    "from odnn_training_visualization import (\n",
    "    capture_eigenmode_propagation,\n",
    "    export_superposition_slices,\n",
    "    plot_amplitude_comparison_grid,\n",
    "    plot_reconstruction_vs_input,\n",
    "    plot_sys_vs_label_strict,\n",
    "    save_superposition_visuals,\n",
    "    visualize_model_slices,\n",
    ")\n",
    "\n",
    "SEED = 424242\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# 让 cuDNN/算子走确定性分支\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.use_deterministic_algorithms(True)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:5')           # 或者 'cuda:0'\n",
    "    print('Using Device:', device)\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('Using Device: CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45750e4-7b1d-4c06-b7d0-1f63f7f4571d",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m由于上一个单元格中出现错误，单元格已取消。"
     ]
    }
   ],
   "source": [
    "field_size = 25  #the field size in eigenmodes_OM4 is 50 pixels\n",
    "layer_size = 100 #400#300#100\n",
    "num_data = 1000 # options: 1. random datas 2.eigenmodes\n",
    "num_modes = 6 #the mode number of MMF 3 6 10\n",
    "circle_focus_radius = 13 # radius when using uniform circular detectors\n",
    "circle_detectsize = 26  # square window size for circular detectors\n",
    "eigenmode_focus_radius = 12.5  # radius when using eigenmode patterns\n",
    "eigenmode_detectsize = 27    # square window size for eigenmode patterns\n",
    "focus_radius = circle_focus_radius\n",
    "detectsize = circle_detectsize\n",
    "batch_size = 16\n",
    "\n",
    "# Evaluation selection: \"eigenmode\" uses the base modes, \"superposition\" samples random mixtures\n",
    "evaluation_mode = \"eigenmode\"  # options: \"eigenmode\", \"superposition\"\n",
    "num_superposition_eval_samples = 1000\n",
    "run_superposition_debug = True\n",
    "save_superposition_plots = True\n",
    "save_superposition_slices = True\n",
    "label_pattern_mode = \"eigenmode\"  # options: \"eigenmode\", \"circle\"\n",
    "# Define multiple D2NN models \n",
    "num_layer_option = [3]   #, 3]#, 4]  # Define the different layer-number ODNN\n",
    "all_losses = [] #the loss for each epoch of each ODNN model\n",
    "all_phase_masks = [] #the phase masks field of each ODNN model\n",
    "all_predictions = [] #the output light field of each ODNN model\n",
    "model_metrics: list[dict] = []\n",
    "all_amplitudes_diff: list[np.ndarray] = []\n",
    "all_average_amplitudes_diff: list[float] = []\n",
    "all_amplitudes_relative_diff: list[float] = []\n",
    "all_complex_weights_pred: list[np.ndarray] = []\n",
    "all_image_data_pred: list[np.ndarray] = []\n",
    "all_cc_real: list[np.ndarray] = []\n",
    "all_cc_imag: list[np.ndarray] = []\n",
    "all_cc_recon_amp: list[np.ndarray] = []\n",
    "all_cc_recon_phase: list[np.ndarray] = []\n",
    "all_training_summaries: list[dict] = []\n",
    "# SLM\n",
    "z_layers   = 40e-6        # 原 47.571e-3  -> 40 μm\n",
    "pixel_size = 1e-6\n",
    "z_prop     = 120e-6        # 原 16.74e-2   -> 60 μm plus 40（最后一层到相机）\n",
    "wavelength = 1568e-9      # 原 1568     -> 1550 nm\n",
    "z_input_to_first = 40e-6  # 40 μm # 新增：输入面到第一层的传播距离"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce122a10-392a-4408-aa56-65d1eb890458",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m由于上一个单元格中出现错误，单元格已取消。"
     ]
    }
   ],
   "source": [
    "eigenmodes_OM4 = load_complex_modes_from_mat(\n",
    "    'mmf_6modes_25_PD_1.15.mat',\n",
    "    key='modes_field'\n",
    ")\n",
    "# (H, W, M)\n",
    "print(\"Loaded modes shape:\", eigenmodes_OM4.shape, \"dtype:\", eigenmodes_OM4.dtype)\n",
    "\n",
    "# 取前 num_modes 个 → (H, W, M_sel) → (M_sel, H, W)\n",
    "MMF_data = eigenmodes_OM4[:, :, :num_modes].transpose(2, 0, 1)\n",
    "MMF_data_amp_norm = (np.abs(MMF_data) - np.min(np.abs(MMF_data))) / (np.max(np.abs(MMF_data)) - np.min(np.abs(MMF_data)))\n",
    "\n",
    "MMF_data = MMF_data_amp_norm * np.exp(1j * np.angle(MMF_data))\n",
    "\n",
    "#要是以后确定了用4我在想要不要去掉其他选项\n",
    "phase_option = 4\n",
    "#phase_option 1: (0,0,...,0)\n",
    "#phase_option 2: (0,2pi,...,2pi)\n",
    "#phase_option 3: (0,pi,...,2pi)\n",
    "#phase_option 4: eigenmodes\n",
    "#phase_option 5: (0,pi,...,pi)\n",
    "\n",
    "if phase_option in [1, 2, 3, 5]:\n",
    "    amplitudes,phases = generate_complex_weights(num_data,num_modes,phase_option)\n",
    "\n",
    "if phase_option == 4:\n",
    "    num_data = num_modes # use the eigenmodes to train ODNN\n",
    "    amplitudes = np.eye(num_modes)#[[1,0,0][0,1,0][0,0,1]]\n",
    "    phases = np.eye(num_modes)\n",
    "\n",
    "amplitudes_phases_ori = np.hstack((amplitudes[:, :], phases[:, 1:]))  # amplitudes (l2 norm) phases\n",
    "amplitudes_phases = np.hstack((amplitudes[:, :], phases[:, 1:]/(2*np.pi)))  # amplitudes (l2 norm) phases (0-1)\n",
    "\n",
    "# Generate complex weights vector with specified amplitudes and phases\n",
    "complex_weights = amplitudes * np.exp(1j * phases)\n",
    "MMF_data_ts = torch.from_numpy(MMF_data)\n",
    "complex_weights_ts = torch.from_numpy(complex_weights)\n",
    "image_data = generate_fields_ts(complex_weights_ts, MMF_data_ts, num_data, num_modes, field_size).to(torch.complex64)\n",
    "image_test_data = image_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085f13ac-18da-4635-9180-ff813179d6d3",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m由于上一个单元格中出现错误，单元格已取消。"
     ]
    }
   ],
   "source": [
    "'''\n",
    "pred_case = 1: only amplitudes prediction\n",
    "pred_case = 2: only phases prediction\n",
    "pred_case = 3: amplitudes and phases prediction\n",
    "pred_case = 4: amplitudes and phases prediction (extra energy phase area)\n",
    "'''\n",
    "#\n",
    "pred_case = 1\n",
    "label_data = torch.zeros([num_data,1,layer_size,layer_size])\n",
    "label_size = layer_size\n",
    "\n",
    "if pred_case == 1: # 3\n",
    "    num_detector = num_modes\n",
    "    detector_focus_radius = focus_radius\n",
    "    detector_detectsize = detectsize\n",
    "    if label_pattern_mode == \"eigenmode\":\n",
    "        pattern_stack = np.transpose(np.abs(MMF_data), (1, 2, 0))\n",
    "        pattern_h, pattern_w, _ = pattern_stack.shape\n",
    "        if pattern_h > label_size or pattern_w > label_size:\n",
    "            raise ValueError(\n",
    "                f\"Eigenmode pattern size ({pattern_h}x{pattern_w}) exceeds label canvas {label_size}.\"\n",
    "            )\n",
    "        layout_radius = math.ceil(max(pattern_h, pattern_w) / 2)\n",
    "        detector_focus_radius = eigenmode_focus_radius\n",
    "        detector_detectsize = eigenmode_detectsize\n",
    "    elif label_pattern_mode == \"circle\":\n",
    "        circle_radius = circle_focus_radius\n",
    "        pattern_size = circle_radius * 2\n",
    "        if pattern_size % 2 == 0:\n",
    "            pattern_size += 1  \n",
    "        pattern_stack = generate_detector_patterns(pattern_size, pattern_size, num_detector, shape=\"circle\")\n",
    "        layout_radius = circle_radius\n",
    "        detector_focus_radius = circle_radius\n",
    "        detector_detectsize = circle_detectsize\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown label_pattern_mode: {label_pattern_mode}\")\n",
    "\n",
    "    centers, _, _ = compute_label_centers(label_size, label_size, num_detector, layout_radius)\n",
    "    mode_label_maps = [\n",
    "        compose_labels_from_patterns(\n",
    "            label_size,\n",
    "            label_size,\n",
    "            pattern_stack,\n",
    "            centers,\n",
    "            Index=i + 1,\n",
    "            visualize=False,\n",
    "        )\n",
    "        for i in range(num_detector)\n",
    "    ]\n",
    "    MMF_Label_data = torch.from_numpy(\n",
    "        np.stack(mode_label_maps, axis=2).astype(np.float32)\n",
    "    )\n",
    "    amplitude_weights = torch.from_numpy(amplitudes_phases[:num_data, 0:num_modes]).float()\n",
    "    combined_labels = (\n",
    "        amplitude_weights[:, None, None, :] * MMF_Label_data.unsqueeze(0)\n",
    "    ).sum(dim=3)\n",
    "    label_data[:, 0, :, :] = combined_labels\n",
    "    focus_radius = detector_focus_radius\n",
    "    detectsize = detector_detectsize\n",
    "\n",
    "label_test_data = label_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bd74a9-fac3-48ba-8b11-e6ee94241a1c",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m由于上一个单元格中出现错误，单元格已取消。"
     ]
    }
   ],
   "source": [
    "train_dataset = [\n",
    "    prepare_sample(image_data[i], label_data[i], layer_size) for i in range(len(label_data))\n",
    "]\n",
    "train_tensor_data = TensorDataset(*[torch.stack(tensors) for tensors in zip(*train_dataset)])\n",
    "g = torch.Generator()\n",
    "g.manual_seed(SEED)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_tensor_data,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,               # 顺序会被 g 固定\n",
    "    generator=g,                # 固定打乱\n",
    "   \n",
    ")\n",
    "\n",
    "if evaluation_mode == \"eigenmode\":\n",
    "    test_dataset = train_dataset\n",
    "    test_tensor_data = train_tensor_data\n",
    "    test_loader = DataLoader(test_tensor_data, batch_size=batch_size, shuffle=False)\n",
    "    eval_amplitudes = amplitudes\n",
    "    eval_amplitudes_phases = amplitudes_phases\n",
    "    eval_phases = phases\n",
    "    image_test_data = image_data\n",
    "elif evaluation_mode == \"superposition\":\n",
    "    if pred_case != 1:\n",
    "        raise ValueError(\"Superposition evaluation mode currently supports pred_case == 1 only.\")\n",
    "    super_ctx = build_superposition_eval_context(\n",
    "        num_superposition_eval_samples,\n",
    "        num_modes=num_modes,\n",
    "        field_size=field_size,\n",
    "        layer_size=layer_size,\n",
    "        mmf_modes=MMF_data_ts,\n",
    "        mmf_label_data=MMF_Label_data,\n",
    "        batch_size=batch_size,\n",
    "        second_mode_half_range=True,\n",
    "    )\n",
    "    test_dataset = super_ctx[\"dataset\"]\n",
    "    test_tensor_data = super_ctx[\"tensor_dataset\"]\n",
    "    test_loader = super_ctx[\"loader\"]\n",
    "    image_test_data = super_ctx[\"image_data\"]\n",
    "    eval_amplitudes = super_ctx[\"amplitudes\"]\n",
    "    eval_amplitudes_phases = super_ctx[\"amplitudes_phases\"]\n",
    "    eval_phases = super_ctx[\"phases\"]\n",
    "else:\n",
    "    raise ValueError(f\"Unknown evaluation_mode: {evaluation_mode}\")\n",
    "\n",
    "# Generate detection regions using existing function\n",
    "if pred_case ==1:\n",
    "    evaluation_regions = create_evaluation_regions(layer_size, layer_size, num_detector, focus_radius, detectsize)\n",
    "    print(\"Detection Regions:\", evaluation_regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d9b126-fb09-4609-bc6f-b2854bfac85f",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m由于上一个单元格中出现错误，单元格已取消。"
     ]
    }
   ],
   "source": [
    "for num_layer in num_layer_option:\n",
    "    print(f\"\\nTraining D2NN with {num_layer} layers...\\n\")\n",
    "\n",
    "    D2NN = D2NNModel(\n",
    "        num_layers=num_layer,\n",
    "        layer_size=layer_size,\n",
    "        z_layers=z_layers,\n",
    "        z_prop=z_prop,\n",
    "        pixel_size=pixel_size,\n",
    "        wavelength=wavelength,\n",
    "        device=device,\n",
    "        padding_ratio=0.5,\n",
    "        z_input_to_first=z_input_to_first,   # NEW\n",
    "    ).to(device)\n",
    "\n",
    "    print(D2NN)\n",
    "\n",
    "    # Training\n",
    "    criterion = nn.MSELoss()  # Define loss function (对比的是loss)\n",
    "    optimizer = optim.Adam(D2NN.parameters(), lr=1.99) \n",
    "    scheduler = ExponentialLR(optimizer, gamma=0.99)  \n",
    "    epochs = 1000\n",
    "    losses = []\n",
    "    epoch_durations: list[float] = []\n",
    "    training_start_time = time.time()\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        D2NN.train()\n",
    "        epoch_loss = 0\n",
    "        for images, labels in train_loader:\n",
    "            images = images.to(device, dtype=torch.complex64, non_blocking=True)\n",
    "            labels = labels.to(device, dtype=torch.float32,   non_blocking=True)\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            outputs = D2NN(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        scheduler.step()\n",
    "        avg_loss = epoch_loss / len(train_loader)  # Calculate average loss for the epoch\n",
    "        losses.append(avg_loss)  # the loss for each model\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.synchronize(device)\n",
    "        epoch_duration = time.time() - epoch_start_time\n",
    "        epoch_durations.append(epoch_duration)\n",
    "\n",
    "        if epoch % 100 == 0 or epoch == 1 or epoch == epochs:\n",
    "            print(\n",
    "                f'Epoch [{epoch}/{epochs}], Loss: {avg_loss:.18f}, '\n",
    "                f'Epoch Time: {epoch_duration:.2f} seconds'\n",
    "            )\n",
    "\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.synchronize(device)\n",
    "    total_training_time = time.time() - training_start_time\n",
    "    print(\n",
    "        f'Total training time for {num_layer}-layer model: {total_training_time:.2f} seconds '\n",
    "        f'(~{total_training_time / 60:.2f} minutes)'\n",
    "    )\n",
    "    all_losses.append(losses)  # save the loss for each model\n",
    "    training_output_dir = Path(\"results/training_analysis\")\n",
    "    training_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    epochs_array = np.arange(1, epochs + 1, dtype=np.int32)\n",
    "    cumulative_epoch_times = np.cumsum(epoch_durations)\n",
    "    timestamp_tag = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(epochs_array, losses, label=\"Training Loss\")\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.set_title(f\"D2NN Training Loss ({num_layer} layers)\")\n",
    "    ax.grid(True, which=\"both\", linestyle=\"--\", linewidth=0.5)\n",
    "    ax.legend()\n",
    "    loss_plot_path = training_output_dir / f\"loss_curve_layers{num_layer}_{timestamp_tag}.png\"\n",
    "    fig.savefig(loss_plot_path, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "    fig_time, ax_time = plt.subplots()\n",
    "    ax_time.plot(epochs_array, cumulative_epoch_times, label=\"Cumulative Time\")\n",
    "    ax_time.set_xlabel(\"Epoch\")\n",
    "    ax_time.set_ylabel(\"Time (seconds)\")\n",
    "    ax_time.set_title(f\"Cumulative Training Time ({num_layer} layers)\")\n",
    "    ax_time.grid(True, which=\"both\", linestyle=\"--\", linewidth=0.5)\n",
    "    ax_time.legend()\n",
    "    time_plot_path = training_output_dir / f\"epoch_time_layers{num_layer}_{timestamp_tag}.png\"\n",
    "    fig_time.savefig(time_plot_path, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close(fig_time)\n",
    "\n",
    "    mat_path = training_output_dir / f\"training_curves_layers{num_layer}_{timestamp_tag}.mat\"\n",
    "    savemat(\n",
    "        str(mat_path),\n",
    "        {\n",
    "            \"epochs\": epochs_array,\n",
    "            \"losses\": np.array(losses, dtype=np.float64),\n",
    "            \"epoch_durations\": np.array(epoch_durations, dtype=np.float64),\n",
    "            \"cumulative_epoch_times\": np.array(cumulative_epoch_times, dtype=np.float64),\n",
    "            \"total_training_time\": np.array([total_training_time], dtype=np.float64),\n",
    "            \"num_layers\": np.array([num_layer], dtype=np.int32),\n",
    "        },\n",
    "    )\n",
    "\n",
    "    print(f\"✔ Saved training loss plot -> {loss_plot_path}\")\n",
    "    print(f\"✔ Saved cumulative time plot -> {time_plot_path}\")\n",
    "    print(f\"✔ Saved training log data (.mat) -> {mat_path}\")\n",
    "\n",
    "    propagation_dir = Path(\"results/propagation_slices\")\n",
    "    eigenmode_index = min(2, MMF_data_ts.shape[0] - 1)\n",
    "    propagation_summary = capture_eigenmode_propagation(\n",
    "        model=D2NN,\n",
    "        eigenmode_field=MMF_data_ts[eigenmode_index],\n",
    "        mode_index=eigenmode_index,\n",
    "        layer_size=layer_size,\n",
    "        z_input_to_first=z_input_to_first,\n",
    "        z_layers=z_layers,\n",
    "        z_prop=z_prop,\n",
    "        pixel_size=pixel_size,\n",
    "        wavelength=wavelength,\n",
    "        output_dir=propagation_dir,\n",
    "        tag=f\"layers{num_layer}_{timestamp_tag}\",\n",
    "    )\n",
    "    print(f\"✔ Saved eigenmode-{eigenmode_index + 1} propagation plot -> {propagation_summary['fig_path']}\")\n",
    "    print(f\"✔ Saved eigenmode-{eigenmode_index + 1} propagation data (.mat) -> {propagation_summary['mat_path']}\")\n",
    "\n",
    "    all_training_summaries.append(\n",
    "        {\n",
    "            \"num_layers\": num_layer,\n",
    "            \"total_time\": total_training_time,\n",
    "            \"loss_plot\": str(loss_plot_path),\n",
    "            \"time_plot\": str(time_plot_path),\n",
    "            \"mat_path\": str(mat_path),\n",
    "            \"propagation_fig\": propagation_summary[\"fig_path\"],\n",
    "            \"propagation_mat\": propagation_summary[\"mat_path\"],\n",
    "        }\n",
    "    )\n",
    "   \n",
    "    # === after training ===\n",
    "    ckpt_dir = \"checkpoints\"\n",
    "    os.makedirs(ckpt_dir, exist_ok=True)\n",
    "\n",
    "    ckpt = {\n",
    "        \"state_dict\": D2NN.state_dict(),\n",
    "        \"meta\": {\n",
    "            \"num_layers\":        len(D2NN.layers),\n",
    "            \"layer_size\":        layer_size,\n",
    "            \"z_layers\":          z_layers,\n",
    "            \"z_prop\":            z_prop,\n",
    "            \"pixel_size\":        pixel_size,\n",
    "            \"wavelength\":        wavelength,\n",
    "            \"padding_ratio\":     0.5,         \n",
    "            \"field_size\":        field_size,  \n",
    "            \"num_modes\":         num_modes, \n",
    "            \"z_input_to_first\":  z_input_to_first, \n",
    "        }\n",
    "    }\n",
    "    save_path = os.path.join(ckpt_dir, f\"odnn_{len(D2NN.layers)}layers.pth\")\n",
    "    torch.save(ckpt, save_path)\n",
    "    print(\"✔ Saved model ->\", save_path)\n",
    "    # Free GPU memory\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Cache phase masks for later visualization/export\n",
    "    phase_masks = []\n",
    "    for layer in D2NN.layers:\n",
    "        phase_np = layer.phase.detach().cpu().numpy()\n",
    "        phase_masks.append(np.remainder(phase_np, 2 * np.pi))\n",
    "    all_phase_masks.append(phase_masks)\n",
    "\n",
    "    # Collect evaluation metrics for this model\n",
    "    metrics = evaluate_spot_metrics(\n",
    "        D2NN,\n",
    "        test_loader,\n",
    "        evaluation_regions,\n",
    "        detect_radius=detectsize,\n",
    "        device=device,\n",
    "        pred_case=pred_case,\n",
    "        num_modes=num_modes,\n",
    "        phase_option=phase_option,\n",
    "        amplitudes=eval_amplitudes,\n",
    "        amplitudes_phases=eval_amplitudes_phases,\n",
    "        phases=eval_phases,\n",
    "        mmf_modes=MMF_data_ts,\n",
    "        field_size=field_size,\n",
    "        image_test_data=image_test_data,\n",
    "    )\n",
    "\n",
    "    model_metrics.append(metrics)\n",
    "    all_amplitudes_diff.append(metrics.get(\"amplitudes_diff\", np.array([])))\n",
    "    all_average_amplitudes_diff.append(float(metrics.get(\"avg_amplitudes_diff\", float(\"nan\"))))\n",
    "    all_amplitudes_relative_diff.append(float(metrics.get(\"avg_relative_amp_err\", float(\"nan\"))))\n",
    "    all_complex_weights_pred.append(metrics.get(\"complex_weights_pred\", np.array([])))\n",
    "    all_image_data_pred.append(metrics.get(\"image_data_pred\", np.array([])))\n",
    "    all_cc_recon_amp.append(metrics.get(\"cc_recon_amp\", np.array([])))\n",
    "    all_cc_recon_phase.append(metrics.get(\"cc_recon_phase\", np.array([])))\n",
    "    all_cc_real.append(metrics.get(\"cc_real\", np.array([])))\n",
    "    all_cc_imag.append(metrics.get(\"cc_imag\", np.array([])))\n",
    "\n",
    "    print(\n",
    "        format_metric_report(\n",
    "            num_modes=num_modes,\n",
    "            phase_option=phase_option,\n",
    "            pred_case=pred_case,\n",
    "            label=f\"{num_layer} layers\",\n",
    "            metrics=metrics,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87d2e97-55f2-40ae-9dbc-ac37307bc86f",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m由于上一个单元格中出现错误，单元格已取消。"
     ]
    }
   ],
   "source": [
    "if all_training_summaries:\n",
    "    print(\"\\nTraining duration summary:\")\n",
    "    for summary in all_training_summaries:\n",
    "        minutes = summary[\"total_time\"] / 60\n",
    "        print(\n",
    "            f\" - {summary['num_layers']} layers: {summary['total_time']:.2f} s \"\n",
    "            f\"(~{minutes:.2f} min)\"\n",
    "        )\n",
    "        print(f\"   Loss curve: {summary['loss_plot']}\")\n",
    "        print(f\"   Time curve: {summary['time_plot']}\")\n",
    "        print(f\"   Data (.mat): {summary['mat_path']}\")\n",
    "        print(f\"   Propagation plot: {summary['propagation_fig']}\")\n",
    "        print(f\"   Propagation data (.mat): {summary['propagation_mat']}\")\n",
    "\n",
    "save_dir = \"results/plots\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "num_samples_to_display = 6\n",
    "for idx, num_layer in enumerate(num_layer_option):\n",
    "    plot_amplitude_comparison_grid(\n",
    "        image_test_data,\n",
    "        all_image_data_pred[idx],\n",
    "        all_cc_recon_amp[idx],\n",
    "        max_samples=num_samples_to_display,\n",
    "        save_path=os.path.join(save_dir, f\"Amp_{num_layer}layers.png\"),\n",
    "        title=f\"Amp. distribution of Real and Predicted Images({num_layer}_layer_ODNN)\",\n",
    "    )\n",
    "\n",
    "#直观的看看输出和label的差异\n",
    "for s in [0, 1, 2, 5]:\n",
    "    plot_sys_vs_label_strict(\n",
    "        D2NN,\n",
    "        test_dataset,\n",
    "        sample_idx=s,\n",
    "        evaluation_regions=evaluation_regions,\n",
    "        detect_radius=detectsize,\n",
    "        save_path=f\"results/plots/IO_Pred_Label_RAW_{s}.png\",\n",
    "        device=device,\n",
    "        use_big_canvas=False,\n",
    "        sys_scale=\"bg_pct\",\n",
    "        sys_pct=99.5,\n",
    "        clip_pct=99.5,\n",
    "        mask_roi_for_scale=True,\n",
    "        show_signed=True,\n",
    "    )\n",
    "    plot_reconstruction_vs_input(\n",
    "        image_test_data=image_test_data,\n",
    "        reconstructed_fields=all_image_data_pred,\n",
    "        sample_idx=s,\n",
    "        model_idx=0,\n",
    "        save_path=f\"results/plots/Reconstruction_vs_Input_{s}.png\",\n",
    "    )\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679e9a73-2e1d-4177-8f69-39b1f99c490f",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m由于上一个单元格中出现错误，单元格已取消。"
     ]
    }
   ],
   "source": [
    "temp_dataset = test_dataset\n",
    "FIXED_E_INDEX = 4\n",
    "\n",
    "def get_fixed_input(dataset, idx, device):\n",
    "    if isinstance(dataset, list):\n",
    "        sample = dataset[idx][0]\n",
    "    else:\n",
    "        sample = dataset.tensors[0][idx]\n",
    "    return sample.squeeze(0).to(device)\n",
    "\n",
    "\n",
    "assert len(temp_dataset) > 0, \"test_dataset 为空\"\n",
    "temp_E = get_fixed_input(temp_dataset, FIXED_E_INDEX % len(temp_dataset), device)\n",
    "\n",
    "z_start = 0.0\n",
    "z_step = 5e-6\n",
    "z_prop_plus = z_prop\n",
    "\n",
    "save_root = Path(\"results_MD\")\n",
    "save_root.mkdir(parents=True, exist_ok=True)\n",
    "run_stamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "filename_prefix = f\"ODNN_vis_{run_stamp}\"\n",
    "\n",
    "for i_model, phase_masks in enumerate(all_phase_masks, start=1):\n",
    "    model_dir = save_root / f\"m{i_model}\"\n",
    "    scans, camera_field = visualize_model_slices(\n",
    "        D2NN,\n",
    "        phase_masks,\n",
    "        temp_E,\n",
    "        output_dir=model_dir,\n",
    "        sample_tag=f\"m{i_model}\",\n",
    "        z_input_to_first=z_input_to_first,\n",
    "        z_layers=z_layers,\n",
    "        z_prop_plus=z_prop_plus,\n",
    "        z_step=z_step,\n",
    "        pixel_size=pixel_size,\n",
    "        wavelength=wavelength,\n",
    "    )\n",
    "\n",
    "    phase_stack = np.stack([np.asarray(mask, dtype=np.float32) for mask in phase_masks], axis=0)\n",
    "    meta = {\n",
    "        \"z_start\": float(z_start),\n",
    "        \"z_step\": float(z_step),\n",
    "        \"z_layers\": float(z_layers),\n",
    "        \"z_prop\": float(z_prop),\n",
    "        \"z_prop_plus\": float(z_prop_plus),\n",
    "        \"pixel_size\": float(pixel_size),\n",
    "        \"wavelength\": float(wavelength),\n",
    "        \"layer_size\": int(layer_size),\n",
    "        \"padding_ratio\": 0.5,\n",
    "    }\n",
    "\n",
    "    mat_path = model_dir / f\"{filename_prefix}_LIGHT_m{i_model}.mat\"\n",
    "    save_to_mat_light_plus(\n",
    "        mat_path,\n",
    "        phase_stack=phase_stack,\n",
    "        input_field=temp_E.detach().cpu().numpy(),\n",
    "        scans=scans,\n",
    "        camera_field=camera_field,\n",
    "        sample_stacks_kmax=20,\n",
    "        save_amplitude_only=False,\n",
    "        meta=meta,\n",
    "    )\n",
    "    print(\"Saved ->\", mat_path)\n",
    "\n",
    "    save_masks_one_file_per_layer(\n",
    "        phase_masks,\n",
    "        out_dir=model_dir,\n",
    "        base_name=f\"{filename_prefix}_MASK\",\n",
    "        save_degree=False,\n",
    "        use_xlsx=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ec0fbe-cc47-4752-9eaf-9e1c603ee50e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'capture_eigenmode_propagation' from 'odnn_training_visualization' (/home/ydzhang/Desktop/odnn_code/odnn_training_visualization.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 43\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01modnn_training_eval\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     35\u001b[39m     build_superposition_eval_context,\n\u001b[32m     36\u001b[39m     compute_model_prediction_metrics,\n\u001b[32m   (...)\u001b[39m\u001b[32m     40\u001b[39m     infer_superposition_output,\n\u001b[32m     41\u001b[39m )\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01modnn_training_io\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m save_masks_one_file_per_layer, save_to_mat_light_plus\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01modnn_training_visualization\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     44\u001b[39m     capture_eigenmode_propagation,\n\u001b[32m     45\u001b[39m     export_superposition_slices,\n\u001b[32m     46\u001b[39m     plot_amplitude_comparison_grid,\n\u001b[32m     47\u001b[39m     plot_reconstruction_vs_input,\n\u001b[32m     48\u001b[39m     plot_sys_vs_label_strict,\n\u001b[32m     49\u001b[39m     save_superposition_visuals,\n\u001b[32m     50\u001b[39m     save_mode_triptych,\n\u001b[32m     51\u001b[39m     visualize_model_slices,\n\u001b[32m     52\u001b[39m )\n\u001b[32m     54\u001b[39m SEED = \u001b[32m424242\u001b[39m\n\u001b[32m     55\u001b[39m random.seed(SEED)\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'capture_eigenmode_propagation' from 'odnn_training_visualization' (/home/ydzhang/Desktop/odnn_code/odnn_training_visualization.py)"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from scipy.io import savemat\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from ODNN_functions import (\n",
    "    create_evaluation_regions,\n",
    "    generate_complex_weights,\n",
    "    generate_fields_ts,\n",
    ")\n",
    "from odnn_generate_label import (\n",
    "    compute_label_centers,\n",
    "    compose_labels_from_patterns,\n",
    "    generate_detector_patterns,\n",
    ")\n",
    "from odnn_io import load_complex_modes_from_mat\n",
    "from odnn_model import D2NNModel\n",
    "from odnn_processing import prepare_sample\n",
    "from odnn_training_eval import (\n",
    "    build_superposition_eval_context,\n",
    "    compute_model_prediction_metrics,\n",
    "    evaluate_spot_metrics,\n",
    "    format_metric_report,\n",
    "    generate_superposition_sample,\n",
    "    infer_superposition_output,\n",
    ")\n",
    "from odnn_training_io import save_masks_one_file_per_layer, save_to_mat_light_plus\n",
    "from odnn_training_visualization import (\n",
    "    capture_eigenmode_propagation,\n",
    "    export_superposition_slices,\n",
    "    plot_amplitude_comparison_grid,\n",
    "    plot_reconstruction_vs_input,\n",
    "    plot_sys_vs_label_strict,\n",
    "    save_superposition_visuals,\n",
    "    save_mode_triptych,\n",
    "    visualize_model_slices,\n",
    ")\n",
    "\n",
    "SEED = 424242\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# 让 cuDNN/算子走确定性分支\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.use_deterministic_algorithms(True)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:5')           # 或者 'cuda:0'\n",
    "    print('Using Device:', device)\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('Using Device: CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ddb13e-126b-4db2-90fd-2c1af1c4d817",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m由于上一个单元格中出现错误，单元格已取消。"
     ]
    }
   ],
   "source": [
    "field_size = 25  #the field size in eigenmodes_OM4 is 50 pixels\n",
    "layer_size = 100 #400#300#100\n",
    "num_data = 1000 # options: 1. random datas 2.eigenmodes\n",
    "num_modes = 6 #the mode number of MMF 3 6 10\n",
    "circle_focus_radius = 5 # radius when using uniform circular detectors\n",
    "circle_detectsize = 10  # square window size for circular detectors\n",
    "eigenmode_focus_radius = 12.5  # radius when using eigenmode patterns\n",
    "eigenmode_detectsize = 27    # square window size for eigenmode patterns\n",
    "focus_radius = circle_focus_radius\n",
    "detectsize = circle_detectsize\n",
    "batch_size = 16\n",
    "\n",
    "# Evaluation selection: \"eigenmode\" uses the base modes, \"superposition\" samples random mixtures\n",
    "evaluation_mode = \"eigenmode\"  # options: \"eigenmode\", \"superposition\"\n",
    "num_superposition_eval_samples = 1000\n",
    "run_superposition_debug = True\n",
    "save_superposition_plots = True\n",
    "save_superposition_slices = True\n",
    "label_pattern_mode = \"circle\"  # options: \"eigenmode\", \"circle\"\n",
    "# Define multiple D2NN models \n",
    "num_layer_option = [3]   #, 3]#, 4]  # Define the different layer-number ODNN\n",
    "all_losses = [] #the loss for each epoch of each ODNN model\n",
    "all_phase_masks = [] #the phase masks field of each ODNN model\n",
    "all_predictions = [] #the output light field of each ODNN model\n",
    "model_metrics: list[dict] = []\n",
    "all_amplitudes_diff: list[np.ndarray] = []\n",
    "all_average_amplitudes_diff: list[float] = []\n",
    "all_amplitudes_relative_diff: list[float] = []\n",
    "all_complex_weights_pred: list[np.ndarray] = []\n",
    "all_image_data_pred: list[np.ndarray] = []\n",
    "all_cc_real: list[np.ndarray] = []\n",
    "all_cc_imag: list[np.ndarray] = []\n",
    "all_cc_recon_amp: list[np.ndarray] = []\n",
    "all_cc_recon_phase: list[np.ndarray] = []\n",
    "all_training_summaries: list[dict] = []\n",
    "# SLM\n",
    "z_layers   = 40e-6        # 原 47.571e-3  -> 40 μm\n",
    "pixel_size = 1e-6\n",
    "z_prop     = 120e-6        # 原 16.74e-2   -> 60 μm plus 40（最后一层到相机）\n",
    "wavelength = 1568e-9      # 原 1568     -> 1550 nm\n",
    "z_input_to_first = 40e-6  # 40 μm # 新增：输入面到第一层的传播距离"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98ea53c-9c1c-45ab-9f77-345673fc19ac",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m由于上一个单元格中出现错误，单元格已取消。"
     ]
    }
   ],
   "source": [
    "eigenmodes_OM4 = load_complex_modes_from_mat(\n",
    "    'mmf_6modes_25_PD_1.15.mat',\n",
    "    key='modes_field'\n",
    ")\n",
    "# (H, W, M)\n",
    "print(\"Loaded modes shape:\", eigenmodes_OM4.shape, \"dtype:\", eigenmodes_OM4.dtype)\n",
    "\n",
    "# 取前 num_modes 个 → (H, W, M_sel) → (M_sel, H, W)\n",
    "MMF_data = eigenmodes_OM4[:, :, :num_modes].transpose(2, 0, 1)\n",
    "MMF_data_amp_norm = (np.abs(MMF_data) - np.min(np.abs(MMF_data))) / (np.max(np.abs(MMF_data)) - np.min(np.abs(MMF_data)))\n",
    "\n",
    "MMF_data = MMF_data_amp_norm * np.exp(1j * np.angle(MMF_data))\n",
    "\n",
    "#要是以后确定了用4我在想要不要去掉其他选项\n",
    "phase_option = 4\n",
    "#phase_option 1: (0,0,...,0)\n",
    "#phase_option 2: (0,2pi,...,2pi)\n",
    "#phase_option 3: (0,pi,...,2pi)\n",
    "#phase_option 4: eigenmodes\n",
    "#phase_option 5: (0,pi,...,pi)\n",
    "\n",
    "if phase_option in [1, 2, 3, 5]:\n",
    "    amplitudes,phases = generate_complex_weights(num_data,num_modes,phase_option)\n",
    "\n",
    "if phase_option == 4:\n",
    "    num_data = num_modes # use the eigenmodes to train ODNN\n",
    "    amplitudes = np.eye(num_modes)#[[1,0,0][0,1,0][0,0,1]]\n",
    "    phases = np.eye(num_modes)\n",
    "\n",
    "amplitudes_phases_ori = np.hstack((amplitudes[:, :], phases[:, 1:]))  # amplitudes (l2 norm) phases\n",
    "amplitudes_phases = np.hstack((amplitudes[:, :], phases[:, 1:]/(2*np.pi)))  # amplitudes (l2 norm) phases (0-1)\n",
    "\n",
    "# Generate complex weights vector with specified amplitudes and phases\n",
    "complex_weights = amplitudes * np.exp(1j * phases)\n",
    "MMF_data_ts = torch.from_numpy(MMF_data)\n",
    "complex_weights_ts = torch.from_numpy(complex_weights)\n",
    "image_data = generate_fields_ts(complex_weights_ts, MMF_data_ts, num_data, num_modes, field_size).to(torch.complex64)\n",
    "image_test_data = image_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bdd345-7ab1-4fd5-8c0c-7e386a66b2dc",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m由于上一个单元格中出现错误，单元格已取消。"
     ]
    }
   ],
   "source": [
    "'''\n",
    "pred_case = 1: only amplitudes prediction\n",
    "pred_case = 2: only phases prediction\n",
    "pred_case = 3: amplitudes and phases prediction\n",
    "pred_case = 4: amplitudes and phases prediction (extra energy phase area)\n",
    "'''\n",
    "#\n",
    "pred_case = 1\n",
    "label_data = torch.zeros([num_data,1,layer_size,layer_size])\n",
    "label_size = layer_size\n",
    "\n",
    "if pred_case == 1: # 3\n",
    "    num_detector = num_modes\n",
    "    detector_focus_radius = focus_radius\n",
    "    detector_detectsize = detectsize\n",
    "    if label_pattern_mode == \"eigenmode\":\n",
    "        pattern_stack = np.transpose(np.abs(MMF_data), (1, 2, 0))\n",
    "        pattern_h, pattern_w, _ = pattern_stack.shape\n",
    "        if pattern_h > label_size or pattern_w > label_size:\n",
    "            raise ValueError(\n",
    "                f\"Eigenmode pattern size ({pattern_h}x{pattern_w}) exceeds label canvas {label_size}.\"\n",
    "            )\n",
    "        layout_radius = math.ceil(max(pattern_h, pattern_w) / 2)\n",
    "        detector_focus_radius = eigenmode_focus_radius\n",
    "        detector_detectsize = eigenmode_detectsize\n",
    "    elif label_pattern_mode == \"circle\":\n",
    "        circle_radius = circle_focus_radius\n",
    "        pattern_size = circle_radius * 2\n",
    "        if pattern_size % 2 == 0:\n",
    "            pattern_size += 1  \n",
    "        pattern_stack = generate_detector_patterns(pattern_size, pattern_size, num_detector, shape=\"circle\")\n",
    "        layout_radius = circle_radius\n",
    "        detector_focus_radius = circle_radius\n",
    "        detector_detectsize = circle_detectsize\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown label_pattern_mode: {label_pattern_mode}\")\n",
    "\n",
    "    centers, _, _ = compute_label_centers(label_size, label_size, num_detector, layout_radius)\n",
    "    mode_label_maps = [\n",
    "        compose_labels_from_patterns(\n",
    "            label_size,\n",
    "            label_size,\n",
    "            pattern_stack,\n",
    "            centers,\n",
    "            Index=i + 1,\n",
    "            visualize=False,\n",
    "        )\n",
    "        for i in range(num_detector)\n",
    "    ]\n",
    "    MMF_Label_data = torch.from_numpy(\n",
    "        np.stack(mode_label_maps, axis=2).astype(np.float32)\n",
    "    )\n",
    "    amplitude_weights = torch.from_numpy(amplitudes_phases[:num_data, 0:num_modes]).float()\n",
    "    combined_labels = (\n",
    "        amplitude_weights[:, None, None, :] * MMF_Label_data.unsqueeze(0)\n",
    "    ).sum(dim=3)\n",
    "    label_data[:, 0, :, :] = combined_labels\n",
    "    focus_radius = detector_focus_radius\n",
    "    detectsize = detector_detectsize\n",
    "\n",
    "label_test_data = label_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fef4ed-2cda-4ba2-a1d9-930ff9d7c82f",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m由于上一个单元格中出现错误，单元格已取消。"
     ]
    }
   ],
   "source": [
    "train_dataset = [\n",
    "    prepare_sample(image_data[i], label_data[i], layer_size) for i in range(len(label_data))\n",
    "]\n",
    "train_tensor_data = TensorDataset(*[torch.stack(tensors) for tensors in zip(*train_dataset)])\n",
    "g = torch.Generator()\n",
    "g.manual_seed(SEED)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_tensor_data,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,               # 顺序会被 g 固定\n",
    "    generator=g,                # 固定打乱\n",
    "   \n",
    ")\n",
    "\n",
    "if evaluation_mode == \"eigenmode\":\n",
    "    test_dataset = train_dataset\n",
    "    test_tensor_data = train_tensor_data\n",
    "    test_loader = DataLoader(test_tensor_data, batch_size=batch_size, shuffle=False)\n",
    "    eval_amplitudes = amplitudes\n",
    "    eval_amplitudes_phases = amplitudes_phases\n",
    "    eval_phases = phases\n",
    "    image_test_data = image_data\n",
    "elif evaluation_mode == \"superposition\":\n",
    "    if pred_case != 1:\n",
    "        raise ValueError(\"Superposition evaluation mode currently supports pred_case == 1 only.\")\n",
    "    super_ctx = build_superposition_eval_context(\n",
    "        num_superposition_eval_samples,\n",
    "        num_modes=num_modes,\n",
    "        field_size=field_size,\n",
    "        layer_size=layer_size,\n",
    "        mmf_modes=MMF_data_ts,\n",
    "        mmf_label_data=MMF_Label_data,\n",
    "        batch_size=batch_size,\n",
    "        second_mode_half_range=True,\n",
    "    )\n",
    "    test_dataset = super_ctx[\"dataset\"]\n",
    "    test_tensor_data = super_ctx[\"tensor_dataset\"]\n",
    "    test_loader = super_ctx[\"loader\"]\n",
    "    image_test_data = super_ctx[\"image_data\"]\n",
    "    eval_amplitudes = super_ctx[\"amplitudes\"]\n",
    "    eval_amplitudes_phases = super_ctx[\"amplitudes_phases\"]\n",
    "    eval_phases = super_ctx[\"phases\"]\n",
    "else:\n",
    "    raise ValueError(f\"Unknown evaluation_mode: {evaluation_mode}\")\n",
    "\n",
    "# Generate detection regions using existing function\n",
    "if pred_case ==1:\n",
    "    evaluation_regions = create_evaluation_regions(layer_size, layer_size, num_detector, focus_radius, detectsize)\n",
    "    print(\"Detection Regions:\", evaluation_regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25505b1-2744-4834-b756-a70a59975c97",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m由于上一个单元格中出现错误，单元格已取消。"
     ]
    }
   ],
   "source": [
    "for num_layer in num_layer_option:\n",
    "    print(f\"\\nTraining D2NN with {num_layer} layers...\\n\")\n",
    "\n",
    "    D2NN = D2NNModel(\n",
    "        num_layers=num_layer,\n",
    "        layer_size=layer_size,\n",
    "        z_layers=z_layers,\n",
    "        z_prop=z_prop,\n",
    "        pixel_size=pixel_size,\n",
    "        wavelength=wavelength,\n",
    "        device=device,\n",
    "        padding_ratio=0.5,\n",
    "        z_input_to_first=z_input_to_first,   # NEW\n",
    "    ).to(device)\n",
    "\n",
    "    print(D2NN)\n",
    "\n",
    "    # Training\n",
    "    criterion = nn.MSELoss()  # Define loss function (对比的是loss)\n",
    "    optimizer = optim.Adam(D2NN.parameters(), lr=1.99) \n",
    "    scheduler = ExponentialLR(optimizer, gamma=0.99)  \n",
    "    epochs = 1000\n",
    "    losses = []\n",
    "    epoch_durations: list[float] = []\n",
    "    training_start_time = time.time()\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        D2NN.train()\n",
    "        epoch_loss = 0\n",
    "        for images, labels in train_loader:\n",
    "            images = images.to(device, dtype=torch.complex64, non_blocking=True)\n",
    "            labels = labels.to(device, dtype=torch.float32,   non_blocking=True)\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            outputs = D2NN(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        scheduler.step()\n",
    "        avg_loss = epoch_loss / len(train_loader)  # Calculate average loss for the epoch\n",
    "        losses.append(avg_loss)  # the loss for each model\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.synchronize(device)\n",
    "        epoch_duration = time.time() - epoch_start_time\n",
    "        epoch_durations.append(epoch_duration)\n",
    "\n",
    "        if epoch % 100 == 0 or epoch == 1 or epoch == epochs:\n",
    "            print(\n",
    "                f'Epoch [{epoch}/{epochs}], Loss: {avg_loss:.18f}, '\n",
    "                f'Epoch Time: {epoch_duration:.2f} seconds'\n",
    "            )\n",
    "\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.synchronize(device)\n",
    "    total_training_time = time.time() - training_start_time\n",
    "    print(\n",
    "        f'Total training time for {num_layer}-layer model: {total_training_time:.2f} seconds '\n",
    "        f'(~{total_training_time / 60:.2f} minutes)'\n",
    "    )\n",
    "    all_losses.append(losses)  # save the loss for each model\n",
    "    training_output_dir = Path(\"results/training_analysis\")\n",
    "    training_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    epochs_array = np.arange(1, epochs + 1, dtype=np.int32)\n",
    "    cumulative_epoch_times = np.cumsum(epoch_durations)\n",
    "    timestamp_tag = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(epochs_array, losses, label=\"Training Loss\")\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.set_title(f\"D2NN Training Loss ({num_layer} layers)\")\n",
    "    ax.grid(True, which=\"both\", linestyle=\"--\", linewidth=0.5)\n",
    "    ax.legend()\n",
    "    loss_plot_path = training_output_dir / f\"loss_curve_layers{num_layer}_{timestamp_tag}.png\"\n",
    "    fig.savefig(loss_plot_path, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "    fig_time, ax_time = plt.subplots()\n",
    "    ax_time.plot(epochs_array, cumulative_epoch_times, label=\"Cumulative Time\")\n",
    "    ax_time.set_xlabel(\"Epoch\")\n",
    "    ax_time.set_ylabel(\"Time (seconds)\")\n",
    "    ax_time.set_title(f\"Cumulative Training Time ({num_layer} layers)\")\n",
    "    ax_time.grid(True, which=\"both\", linestyle=\"--\", linewidth=0.5)\n",
    "    ax_time.legend()\n",
    "    time_plot_path = training_output_dir / f\"epoch_time_layers{num_layer}_{timestamp_tag}.png\"\n",
    "    fig_time.savefig(time_plot_path, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close(fig_time)\n",
    "\n",
    "    mat_path = training_output_dir / f\"training_curves_layers{num_layer}_{timestamp_tag}.mat\"\n",
    "    savemat(\n",
    "        str(mat_path),\n",
    "        {\n",
    "            \"epochs\": epochs_array,\n",
    "            \"losses\": np.array(losses, dtype=np.float64),\n",
    "            \"epoch_durations\": np.array(epoch_durations, dtype=np.float64),\n",
    "            \"cumulative_epoch_times\": np.array(cumulative_epoch_times, dtype=np.float64),\n",
    "            \"total_training_time\": np.array([total_training_time], dtype=np.float64),\n",
    "            \"num_layers\": np.array([num_layer], dtype=np.int32),\n",
    "        },\n",
    "    )\n",
    "\n",
    "    print(f\"✔ Saved training loss plot -> {loss_plot_path}\")\n",
    "    print(f\"✔ Saved cumulative time plot -> {time_plot_path}\")\n",
    "    print(f\"✔ Saved training log data (.mat) -> {mat_path}\")\n",
    "\n",
    "    propagation_dir = Path(\"results/propagation_slices\")\n",
    "    eigenmode_index = min(2, MMF_data_ts.shape[0] - 1)\n",
    "    propagation_summary = capture_eigenmode_propagation(\n",
    "        model=D2NN,\n",
    "        eigenmode_field=MMF_data_ts[eigenmode_index],\n",
    "        mode_index=eigenmode_index,\n",
    "        layer_size=layer_size,\n",
    "        z_input_to_first=z_input_to_first,\n",
    "        z_layers=z_layers,\n",
    "        z_prop=z_prop,\n",
    "        pixel_size=pixel_size,\n",
    "        wavelength=wavelength,\n",
    "        output_dir=propagation_dir,\n",
    "        tag=f\"layers{num_layer}_{timestamp_tag}\",\n",
    "    )\n",
    "    print(f\"✔ Saved eigenmode-{eigenmode_index + 1} propagation plot -> {propagation_summary['fig_path']}\")\n",
    "    print(f\"✔ Saved eigenmode-{eigenmode_index + 1} propagation data (.mat) -> {propagation_summary['mat_path']}\")\n",
    "\n",
    "    mode_triptych_records: list[dict[str, str | int]] = []\n",
    "    if evaluation_mode == \"eigenmode\":\n",
    "        triptych_dir = Path(\"results/mode_triptychs\")\n",
    "        mode_tag = f\"layers{num_layer}_{timestamp_tag}\"\n",
    "        for mode_idx in range(min(num_modes, len(MMF_data_ts))):\n",
    "            label_tensor = label_data[mode_idx, 0]\n",
    "            record = save_mode_triptych(\n",
    "                model=D2NN,\n",
    "                mode_index=mode_idx,\n",
    "                eigenmode_field=MMF_data_ts[mode_idx],\n",
    "                label_field=label_tensor,\n",
    "                layer_size=layer_size,\n",
    "                output_dir=triptych_dir,\n",
    "                tag=mode_tag,\n",
    "            )\n",
    "            mode_triptych_records.append(\n",
    "                {\n",
    "                    \"mode\": mode_idx + 1,\n",
    "                    \"fig\": record[\"fig_path\"],\n",
    "                    \"mat\": record[\"mat_path\"],\n",
    "                }\n",
    "            )\n",
    "            print(\n",
    "                f\"✔ Saved mode {mode_idx + 1} triptych -> {record['fig_path']}\\n\"\n",
    "                f\"  MAT -> {record['mat_path']}\"\n",
    "            )\n",
    "\n",
    "    all_training_summaries.append(\n",
    "        {\n",
    "            \"num_layers\": num_layer,\n",
    "            \"total_time\": total_training_time,\n",
    "            \"loss_plot\": str(loss_plot_path),\n",
    "            \"time_plot\": str(time_plot_path),\n",
    "            \"mat_path\": str(mat_path),\n",
    "            \"propagation_fig\": propagation_summary[\"fig_path\"],\n",
    "            \"propagation_mat\": propagation_summary[\"mat_path\"],\n",
    "            \"mode_triptychs\": mode_triptych_records,\n",
    "        }\n",
    "    )\n",
    "   \n",
    "    # === after training ===\n",
    "    ckpt_dir = \"checkpoints\"\n",
    "    os.makedirs(ckpt_dir, exist_ok=True)\n",
    "\n",
    "    ckpt = {\n",
    "        \"state_dict\": D2NN.state_dict(),\n",
    "        \"meta\": {\n",
    "            \"num_layers\":        len(D2NN.layers),\n",
    "            \"layer_size\":        layer_size,\n",
    "            \"z_layers\":          z_layers,\n",
    "            \"z_prop\":            z_prop,\n",
    "            \"pixel_size\":        pixel_size,\n",
    "            \"wavelength\":        wavelength,\n",
    "            \"padding_ratio\":     0.5,         \n",
    "            \"field_size\":        field_size,  \n",
    "            \"num_modes\":         num_modes, \n",
    "            \"z_input_to_first\":  z_input_to_first, \n",
    "        }\n",
    "    }\n",
    "    save_path = os.path.join(ckpt_dir, f\"odnn_{len(D2NN.layers)}layers.pth\")\n",
    "    torch.save(ckpt, save_path)\n",
    "    print(\"✔ Saved model ->\", save_path)\n",
    "    # Free GPU memory\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Cache phase masks for later visualization/export\n",
    "    phase_masks = []\n",
    "    for layer in D2NN.layers:\n",
    "        phase_np = layer.phase.detach().cpu().numpy()\n",
    "        phase_masks.append(np.remainder(phase_np, 2 * np.pi))\n",
    "    all_phase_masks.append(phase_masks)\n",
    "\n",
    "    # Collect evaluation metrics for this model\n",
    "    metrics = evaluate_spot_metrics(\n",
    "        D2NN,\n",
    "        test_loader,\n",
    "        evaluation_regions,\n",
    "        detect_radius=detectsize,\n",
    "        device=device,\n",
    "        pred_case=pred_case,\n",
    "        num_modes=num_modes,\n",
    "        phase_option=phase_option,\n",
    "        amplitudes=eval_amplitudes,\n",
    "        amplitudes_phases=eval_amplitudes_phases,\n",
    "        phases=eval_phases,\n",
    "        mmf_modes=MMF_data_ts,\n",
    "        field_size=field_size,\n",
    "        image_test_data=image_test_data,\n",
    "    )\n",
    "\n",
    "    model_metrics.append(metrics)\n",
    "    all_amplitudes_diff.append(metrics.get(\"amplitudes_diff\", np.array([])))\n",
    "    all_average_amplitudes_diff.append(float(metrics.get(\"avg_amplitudes_diff\", float(\"nan\"))))\n",
    "    all_amplitudes_relative_diff.append(float(metrics.get(\"avg_relative_amp_err\", float(\"nan\"))))\n",
    "    all_complex_weights_pred.append(metrics.get(\"complex_weights_pred\", np.array([])))\n",
    "    all_image_data_pred.append(metrics.get(\"image_data_pred\", np.array([])))\n",
    "    all_cc_recon_amp.append(metrics.get(\"cc_recon_amp\", np.array([])))\n",
    "    all_cc_recon_phase.append(metrics.get(\"cc_recon_phase\", np.array([])))\n",
    "    all_cc_real.append(metrics.get(\"cc_real\", np.array([])))\n",
    "    all_cc_imag.append(metrics.get(\"cc_imag\", np.array([])))\n",
    "\n",
    "    print(\n",
    "        format_metric_report(\n",
    "            num_modes=num_modes,\n",
    "            phase_option=phase_option,\n",
    "            pred_case=pred_case,\n",
    "            label=f\"{num_layer} layers\",\n",
    "            metrics=metrics,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df7c721-4872-419e-a187-8f22444f4962",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m由于上一个单元格中出现错误，单元格已取消。"
     ]
    }
   ],
   "source": [
    "if all_training_summaries:\n",
    "    print(\"\\nTraining duration summary:\")\n",
    "    for summary in all_training_summaries:\n",
    "        minutes = summary[\"total_time\"] / 60\n",
    "        print(\n",
    "            f\" - {summary['num_layers']} layers: {summary['total_time']:.2f} s \"\n",
    "            f\"(~{minutes:.2f} min)\"\n",
    "        )\n",
    "        print(f\"   Loss curve: {summary['loss_plot']}\")\n",
    "        print(f\"   Time curve: {summary['time_plot']}\")\n",
    "        print(f\"   Data (.mat): {summary['mat_path']}\")\n",
    "        print(f\"   Propagation plot: {summary['propagation_fig']}\")\n",
    "        print(f\"   Propagation data (.mat): {summary['propagation_mat']}\")\n",
    "        mode_triptychs = summary.get(\"mode_triptychs\", [])\n",
    "        if mode_triptychs:\n",
    "            print(\"   Mode triptychs:\")\n",
    "            for trip in mode_triptychs:\n",
    "                print(\n",
    "                    f\"     Mode {trip['mode']}: fig={trip['fig']}, mat={trip['mat']}\"\n",
    "                )\n",
    "\n",
    "save_dir = \"results/plots\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "num_samples_to_display = 6\n",
    "for idx, num_layer in enumerate(num_layer_option):\n",
    "    plot_amplitude_comparison_grid(\n",
    "        image_test_data,\n",
    "        all_image_data_pred[idx],\n",
    "        all_cc_recon_amp[idx],\n",
    "        max_samples=num_samples_to_display,\n",
    "        save_path=os.path.join(save_dir, f\"Amp_{num_layer}layers.png\"),\n",
    "        title=f\"Amp. distribution of Real and Predicted Images({num_layer}_layer_ODNN)\",\n",
    "    )\n",
    "\n",
    "# #直观的看看输出和label的差异\n",
    "# for s in [0, 1, 2, 5]:\n",
    "#     plot_sys_vs_label_strict(\n",
    "#         D2NN,\n",
    "#         test_dataset,\n",
    "#         sample_idx=s,\n",
    "#         evaluation_regions=evaluation_regions,\n",
    "#         detect_radius=detectsize,\n",
    "#         save_path=f\"results/plots/IO_Pred_Label_RAW_{s}.png\",\n",
    "#         device=device,\n",
    "#         use_big_canvas=False,\n",
    "#         sys_scale=\"bg_pct\",\n",
    "#         sys_pct=99.5,\n",
    "#         clip_pct=99.5,\n",
    "#         mask_roi_for_scale=True,\n",
    "#         show_signed=True,\n",
    "#     )\n",
    "#     plot_reconstruction_vs_input(\n",
    "#         image_test_data=image_test_data,\n",
    "#         reconstructed_fields=all_image_data_pred,\n",
    "#         sample_idx=s,\n",
    "#         model_idx=0,\n",
    "#         save_path=f\"results/plots/Reconstruction_vs_Input_{s}.png\",\n",
    "#     )\n",
    "\n",
    "# #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "已重启 odnn_venv (Python 3.13.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a23844-d5ad-47ec-bef9-3d8bf6ee84cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device: cuda:5\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from scipy.io import savemat\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from ODNN_functions import (\n",
    "    create_evaluation_regions,\n",
    "    generate_complex_weights,\n",
    "    generate_fields_ts,\n",
    ")\n",
    "from odnn_generate_label import (\n",
    "    compute_label_centers,\n",
    "    compose_labels_from_patterns,\n",
    "    generate_detector_patterns,\n",
    ")\n",
    "from odnn_io import load_complex_modes_from_mat\n",
    "from odnn_model import D2NNModel\n",
    "from odnn_processing import prepare_sample\n",
    "from odnn_training_eval import (\n",
    "    build_superposition_eval_context,\n",
    "    compute_model_prediction_metrics,\n",
    "    evaluate_spot_metrics,\n",
    "    format_metric_report,\n",
    "    generate_superposition_sample,\n",
    "    infer_superposition_output,\n",
    ")\n",
    "from odnn_training_io import save_masks_one_file_per_layer, save_to_mat_light_plus\n",
    "from odnn_training_visualization import (\n",
    "    capture_eigenmode_propagation,\n",
    "    export_superposition_slices,\n",
    "    plot_amplitude_comparison_grid,\n",
    "    plot_reconstruction_vs_input,\n",
    "    plot_sys_vs_label_strict,\n",
    "    save_superposition_visuals,\n",
    "    save_mode_triptych,\n",
    "    visualize_model_slices,\n",
    ")\n",
    "\n",
    "SEED = 424242\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# 让 cuDNN/算子走确定性分支\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.use_deterministic_algorithms(True)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:5')           # 或者 'cuda:0'\n",
    "    print('Using Device:', device)\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('Using Device: CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f625c83-7f04-4bfd-aeba-e30f6a54ac37",
   "metadata": {},
   "outputs": [],
   "source": [
    "field_size = 25  #the field size in eigenmodes_OM4 is 50 pixels\n",
    "layer_size = 100 #400#300#100\n",
    "num_data = 1000 # options: 1. random datas 2.eigenmodes\n",
    "num_modes = 6 #the mode number of MMF 3 6 10\n",
    "circle_focus_radius = 5 # radius when using uniform circular detectors\n",
    "circle_detectsize = 10  # square window size for circular detectors\n",
    "eigenmode_focus_radius = 12.5  # radius when using eigenmode patterns\n",
    "eigenmode_detectsize = 27    # square window size for eigenmode patterns\n",
    "focus_radius = circle_focus_radius\n",
    "detectsize = circle_detectsize\n",
    "batch_size = 16\n",
    "\n",
    "# Evaluation selection: \"eigenmode\" uses the base modes, \"superposition\" samples random mixtures\n",
    "evaluation_mode = \"eigenmode\"  # options: \"eigenmode\", \"superposition\"\n",
    "num_superposition_eval_samples = 1000\n",
    "run_superposition_debug = True\n",
    "save_superposition_plots = True\n",
    "save_superposition_slices = True\n",
    "label_pattern_mode = \"circle\"  # options: \"eigenmode\", \"circle\"\n",
    "# Define multiple D2NN models \n",
    "num_layer_option = [3]   #, 3]#, 4]  # Define the different layer-number ODNN\n",
    "all_losses = [] #the loss for each epoch of each ODNN model\n",
    "all_phase_masks = [] #the phase masks field of each ODNN model\n",
    "all_predictions = [] #the output light field of each ODNN model\n",
    "model_metrics: list[dict] = []\n",
    "all_amplitudes_diff: list[np.ndarray] = []\n",
    "all_average_amplitudes_diff: list[float] = []\n",
    "all_amplitudes_relative_diff: list[float] = []\n",
    "all_complex_weights_pred: list[np.ndarray] = []\n",
    "all_image_data_pred: list[np.ndarray] = []\n",
    "all_cc_real: list[np.ndarray] = []\n",
    "all_cc_imag: list[np.ndarray] = []\n",
    "all_cc_recon_amp: list[np.ndarray] = []\n",
    "all_cc_recon_phase: list[np.ndarray] = []\n",
    "all_training_summaries: list[dict] = []\n",
    "# SLM\n",
    "z_layers   = 40e-6        # 原 47.571e-3  -> 40 μm\n",
    "pixel_size = 1e-6\n",
    "z_prop     = 120e-6        # 原 16.74e-2   -> 60 μm plus 40（最后一层到相机）\n",
    "wavelength = 1568e-9      # 原 1568     -> 1550 nm\n",
    "z_input_to_first = 40e-6  # 40 μm # 新增：输入面到第一层的传播距离"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e637c8-b5b3-4dd0-94c4-b622280378c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded modes shape: (25, 25, 6) dtype: complex64\n"
     ]
    }
   ],
   "source": [
    "eigenmodes_OM4 = load_complex_modes_from_mat(\n",
    "    'mmf_6modes_25_PD_1.15.mat',\n",
    "    key='modes_field'\n",
    ")\n",
    "# (H, W, M)\n",
    "print(\"Loaded modes shape:\", eigenmodes_OM4.shape, \"dtype:\", eigenmodes_OM4.dtype)\n",
    "\n",
    "# 取前 num_modes 个 → (H, W, M_sel) → (M_sel, H, W)\n",
    "MMF_data = eigenmodes_OM4[:, :, :num_modes].transpose(2, 0, 1)\n",
    "MMF_data_amp_norm = (np.abs(MMF_data) - np.min(np.abs(MMF_data))) / (np.max(np.abs(MMF_data)) - np.min(np.abs(MMF_data)))\n",
    "\n",
    "MMF_data = MMF_data_amp_norm * np.exp(1j * np.angle(MMF_data))\n",
    "\n",
    "#要是以后确定了用4我在想要不要去掉其他选项\n",
    "phase_option = 4\n",
    "#phase_option 1: (0,0,...,0)\n",
    "#phase_option 2: (0,2pi,...,2pi)\n",
    "#phase_option 3: (0,pi,...,2pi)\n",
    "#phase_option 4: eigenmodes\n",
    "#phase_option 5: (0,pi,...,pi)\n",
    "\n",
    "if phase_option in [1, 2, 3, 5]:\n",
    "    amplitudes,phases = generate_complex_weights(num_data,num_modes,phase_option)\n",
    "\n",
    "if phase_option == 4:\n",
    "    num_data = num_modes # use the eigenmodes to train ODNN\n",
    "    amplitudes = np.eye(num_modes)#[[1,0,0][0,1,0][0,0,1]]\n",
    "    phases = np.eye(num_modes)\n",
    "\n",
    "amplitudes_phases_ori = np.hstack((amplitudes[:, :], phases[:, 1:]))  # amplitudes (l2 norm) phases\n",
    "amplitudes_phases = np.hstack((amplitudes[:, :], phases[:, 1:]/(2*np.pi)))  # amplitudes (l2 norm) phases (0-1)\n",
    "\n",
    "# Generate complex weights vector with specified amplitudes and phases\n",
    "complex_weights = amplitudes * np.exp(1j * phases)\n",
    "MMF_data_ts = torch.from_numpy(MMF_data)\n",
    "complex_weights_ts = torch.from_numpy(complex_weights)\n",
    "image_data = generate_fields_ts(complex_weights_ts, MMF_data_ts, num_data, num_modes, field_size).to(torch.complex64)\n",
    "image_test_data = image_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ad47fb-e4db-4f45-88bc-c13ef6f9b774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "相邻图案边缘间距： 行=26.67, 列=17.50\n",
      "相邻图案中心间距： 行=36.67, 列=27.50\n",
      "中心坐标： [(32, 22), (32, 50), (32, 78), (68, 22), (68, 50), (68, 78)]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "pred_case = 1: only amplitudes prediction\n",
    "pred_case = 2: only phases prediction\n",
    "pred_case = 3: amplitudes and phases prediction\n",
    "pred_case = 4: amplitudes and phases prediction (extra energy phase area)\n",
    "'''\n",
    "#\n",
    "pred_case = 1\n",
    "label_data = torch.zeros([num_data,1,layer_size,layer_size])\n",
    "label_size = layer_size\n",
    "\n",
    "if pred_case == 1: # 3\n",
    "    num_detector = num_modes\n",
    "    detector_focus_radius = focus_radius\n",
    "    detector_detectsize = detectsize\n",
    "    if label_pattern_mode == \"eigenmode\":\n",
    "        pattern_stack = np.transpose(np.abs(MMF_data), (1, 2, 0))\n",
    "        pattern_h, pattern_w, _ = pattern_stack.shape\n",
    "        if pattern_h > label_size or pattern_w > label_size:\n",
    "            raise ValueError(\n",
    "                f\"Eigenmode pattern size ({pattern_h}x{pattern_w}) exceeds label canvas {label_size}.\"\n",
    "            )\n",
    "        layout_radius = math.ceil(max(pattern_h, pattern_w) / 2)\n",
    "        detector_focus_radius = eigenmode_focus_radius\n",
    "        detector_detectsize = eigenmode_detectsize\n",
    "    elif label_pattern_mode == \"circle\":\n",
    "        circle_radius = circle_focus_radius\n",
    "        pattern_size = circle_radius * 2\n",
    "        if pattern_size % 2 == 0:\n",
    "            pattern_size += 1  \n",
    "        pattern_stack = generate_detector_patterns(pattern_size, pattern_size, num_detector, shape=\"circle\")\n",
    "        layout_radius = circle_radius\n",
    "        detector_focus_radius = circle_radius\n",
    "        detector_detectsize = circle_detectsize\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown label_pattern_mode: {label_pattern_mode}\")\n",
    "\n",
    "    centers, _, _ = compute_label_centers(label_size, label_size, num_detector, layout_radius)\n",
    "    mode_label_maps = [\n",
    "        compose_labels_from_patterns(\n",
    "            label_size,\n",
    "            label_size,\n",
    "            pattern_stack,\n",
    "            centers,\n",
    "            Index=i + 1,\n",
    "            visualize=False,\n",
    "        )\n",
    "        for i in range(num_detector)\n",
    "    ]\n",
    "    MMF_Label_data = torch.from_numpy(\n",
    "        np.stack(mode_label_maps, axis=2).astype(np.float32)\n",
    "    )\n",
    "    amplitude_weights = torch.from_numpy(amplitudes_phases[:num_data, 0:num_modes]).float()\n",
    "    combined_labels = (\n",
    "        amplitude_weights[:, None, None, :] * MMF_Label_data.unsqueeze(0)\n",
    "    ).sum(dim=3)\n",
    "    label_data[:, 0, :, :] = combined_labels\n",
    "    focus_radius = detector_focus_radius\n",
    "    detectsize = detector_detectsize\n",
    "\n",
    "label_test_data = label_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa05a78-ed7c-4e37-88f0-7475d1c4131d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detection Regions: [(17, 27, 27, 37), (45, 55, 27, 37), (73, 83, 27, 37), (17, 27, 63, 73), (45, 55, 63, 73), (73, 83, 63, 73)]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = [\n",
    "    prepare_sample(image_data[i], label_data[i], layer_size) for i in range(len(label_data))\n",
    "]\n",
    "train_tensor_data = TensorDataset(*[torch.stack(tensors) for tensors in zip(*train_dataset)])\n",
    "g = torch.Generator()\n",
    "g.manual_seed(SEED)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_tensor_data,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,               # 顺序会被 g 固定\n",
    "    generator=g,                # 固定打乱\n",
    "   \n",
    ")\n",
    "\n",
    "if evaluation_mode == \"eigenmode\":\n",
    "    test_dataset = train_dataset\n",
    "    test_tensor_data = train_tensor_data\n",
    "    test_loader = DataLoader(test_tensor_data, batch_size=batch_size, shuffle=False)\n",
    "    eval_amplitudes = amplitudes\n",
    "    eval_amplitudes_phases = amplitudes_phases\n",
    "    eval_phases = phases\n",
    "    image_test_data = image_data\n",
    "elif evaluation_mode == \"superposition\":\n",
    "    if pred_case != 1:\n",
    "        raise ValueError(\"Superposition evaluation mode currently supports pred_case == 1 only.\")\n",
    "    super_ctx = build_superposition_eval_context(\n",
    "        num_superposition_eval_samples,\n",
    "        num_modes=num_modes,\n",
    "        field_size=field_size,\n",
    "        layer_size=layer_size,\n",
    "        mmf_modes=MMF_data_ts,\n",
    "        mmf_label_data=MMF_Label_data,\n",
    "        batch_size=batch_size,\n",
    "        second_mode_half_range=True,\n",
    "    )\n",
    "    test_dataset = super_ctx[\"dataset\"]\n",
    "    test_tensor_data = super_ctx[\"tensor_dataset\"]\n",
    "    test_loader = super_ctx[\"loader\"]\n",
    "    image_test_data = super_ctx[\"image_data\"]\n",
    "    eval_amplitudes = super_ctx[\"amplitudes\"]\n",
    "    eval_amplitudes_phases = super_ctx[\"amplitudes_phases\"]\n",
    "    eval_phases = super_ctx[\"phases\"]\n",
    "else:\n",
    "    raise ValueError(f\"Unknown evaluation_mode: {evaluation_mode}\")\n",
    "\n",
    "# Generate detection regions using existing function\n",
    "if pred_case ==1:\n",
    "    evaluation_regions = create_evaluation_regions(layer_size, layer_size, num_detector, focus_radius, detectsize)\n",
    "    print(\"Detection Regions:\", evaluation_regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f432192d-ed9d-4181-be01-c338b9a71593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training D2NN with 3 layers...\n",
      "\n",
      "D2NNModel(\n",
      "  (pre_propagation): Propagation()\n",
      "  (layers): ModuleList(\n",
      "    (0-2): 3 x DiffractionLayer()\n",
      "  )\n",
      "  (propagation): Propagation()\n",
      "  (regression): RegressionDetector()\n",
      ")\n",
      "Epoch [1/1000], Loss: 0.008090289309620857, Epoch Time: 0.10 seconds\n",
      "Epoch [100/1000], Loss: 0.002760213334113359, Epoch Time: 0.01 seconds\n",
      "Epoch [200/1000], Loss: 0.002684592502191663, Epoch Time: 0.01 seconds\n",
      "Epoch [300/1000], Loss: 0.002658451907336712, Epoch Time: 0.01 seconds\n",
      "Epoch [400/1000], Loss: 0.002648203633725643, Epoch Time: 0.01 seconds\n",
      "Epoch [500/1000], Loss: 0.002644307911396027, Epoch Time: 0.01 seconds\n",
      "Epoch [600/1000], Loss: 0.002642895793542266, Epoch Time: 0.01 seconds\n",
      "Epoch [700/1000], Loss: 0.002642374718561769, Epoch Time: 0.01 seconds\n",
      "Epoch [800/1000], Loss: 0.002642178907990456, Epoch Time: 0.01 seconds\n",
      "Epoch [900/1000], Loss: 0.002642104867845774, Epoch Time: 0.01 seconds\n",
      "Epoch [1000/1000], Loss: 0.002642077626660466, Epoch Time: 0.01 seconds\n",
      "Total training time for 3-layer model: 10.79 seconds (~0.18 minutes)\n",
      "✔ Saved training loss plot -> results/training_analysis/loss_curve_layers3_20251103_164848.png\n",
      "✔ Saved cumulative time plot -> results/training_analysis/epoch_time_layers3_20251103_164848.png\n",
      "✔ Saved training log data (.mat) -> results/training_analysis/training_curves_layers3_20251103_164848.mat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ydzhang/Desktop/odnn_code/odnn_training_visualization.py:916: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "  plt.tight_layout(rect=[0, 0, 1, 0.97])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Saved eigenmode-3 propagation plot -> results/propagation_slices/propagation_mode3_layers3_20251103_164848.png\n",
      "✔ Saved eigenmode-3 propagation data (.mat) -> results/propagation_slices/propagation_mode3_layers3_20251103_164848.mat\n",
      "✔ Saved mode 1 triptych -> results/mode_triptychs/mode1_layers3_20251103_164848.png\n",
      "  MAT -> results/mode_triptychs/mode1_layers3_20251103_164848.mat\n",
      "✔ Saved mode 2 triptych -> results/mode_triptychs/mode2_layers3_20251103_164848.png\n",
      "  MAT -> results/mode_triptychs/mode2_layers3_20251103_164848.mat\n",
      "✔ Saved mode 3 triptych -> results/mode_triptychs/mode3_layers3_20251103_164848.png\n",
      "  MAT -> results/mode_triptychs/mode3_layers3_20251103_164848.mat\n",
      "✔ Saved mode 4 triptych -> results/mode_triptychs/mode4_layers3_20251103_164848.png\n",
      "  MAT -> results/mode_triptychs/mode4_layers3_20251103_164848.mat\n",
      "✔ Saved mode 5 triptych -> results/mode_triptychs/mode5_layers3_20251103_164848.png\n",
      "  MAT -> results/mode_triptychs/mode5_layers3_20251103_164848.mat\n",
      "✔ Saved mode 6 triptych -> results/mode_triptychs/mode6_layers3_20251103_164848.png\n",
      "  MAT -> results/mode_triptychs/mode6_layers3_20251103_164848.mat\n",
      "✔ Saved model -> checkpoints/odnn_3layers.pth\n",
      "3 layers: modes=6, phase_opt=4, pred_case=1\n",
      "  amp_err=0.006320, amp_err_rel=0.015481\n",
      "  snr_full=0.590649, snr_crop=0.831407, throughput=0.710266\n",
      "  cc_amp=0.999873±0.000046, cc_phase=0.966133±0.038934, cc_real=0.999879±0.000089, cc_imag=1.000000±0.000000\n"
     ]
    }
   ],
   "source": [
    "for num_layer in num_layer_option:\n",
    "    print(f\"\\nTraining D2NN with {num_layer} layers...\\n\")\n",
    "\n",
    "    D2NN = D2NNModel(\n",
    "        num_layers=num_layer,\n",
    "        layer_size=layer_size,\n",
    "        z_layers=z_layers,\n",
    "        z_prop=z_prop,\n",
    "        pixel_size=pixel_size,\n",
    "        wavelength=wavelength,\n",
    "        device=device,\n",
    "        padding_ratio=0.5,\n",
    "        z_input_to_first=z_input_to_first,   # NEW\n",
    "    ).to(device)\n",
    "\n",
    "    print(D2NN)\n",
    "\n",
    "    # Training\n",
    "    criterion = nn.MSELoss()  # Define loss function (对比的是loss)\n",
    "    optimizer = optim.Adam(D2NN.parameters(), lr=1.99) \n",
    "    scheduler = ExponentialLR(optimizer, gamma=0.99)  \n",
    "    epochs = 1000\n",
    "    losses = []\n",
    "    epoch_durations: list[float] = []\n",
    "    training_start_time = time.time()\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        D2NN.train()\n",
    "        epoch_loss = 0\n",
    "        for images, labels in train_loader:\n",
    "            images = images.to(device, dtype=torch.complex64, non_blocking=True)\n",
    "            labels = labels.to(device, dtype=torch.float32,   non_blocking=True)\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            outputs = D2NN(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        scheduler.step()\n",
    "        avg_loss = epoch_loss / len(train_loader)  # Calculate average loss for the epoch\n",
    "        losses.append(avg_loss)  # the loss for each model\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.synchronize(device)\n",
    "        epoch_duration = time.time() - epoch_start_time\n",
    "        epoch_durations.append(epoch_duration)\n",
    "\n",
    "        if epoch % 100 == 0 or epoch == 1 or epoch == epochs:\n",
    "            print(\n",
    "                f'Epoch [{epoch}/{epochs}], Loss: {avg_loss:.18f}, '\n",
    "                f'Epoch Time: {epoch_duration:.2f} seconds'\n",
    "            )\n",
    "\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.synchronize(device)\n",
    "    total_training_time = time.time() - training_start_time\n",
    "    print(\n",
    "        f'Total training time for {num_layer}-layer model: {total_training_time:.2f} seconds '\n",
    "        f'(~{total_training_time / 60:.2f} minutes)'\n",
    "    )\n",
    "    all_losses.append(losses)  # save the loss for each model\n",
    "    training_output_dir = Path(\"results/training_analysis\")\n",
    "    training_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    epochs_array = np.arange(1, epochs + 1, dtype=np.int32)\n",
    "    cumulative_epoch_times = np.cumsum(epoch_durations)\n",
    "    timestamp_tag = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(epochs_array, losses, label=\"Training Loss\")\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.set_title(f\"D2NN Training Loss ({num_layer} layers)\")\n",
    "    ax.grid(True, which=\"both\", linestyle=\"--\", linewidth=0.5)\n",
    "    ax.legend()\n",
    "    loss_plot_path = training_output_dir / f\"loss_curve_layers{num_layer}_{timestamp_tag}.png\"\n",
    "    fig.savefig(loss_plot_path, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "    fig_time, ax_time = plt.subplots()\n",
    "    ax_time.plot(epochs_array, cumulative_epoch_times, label=\"Cumulative Time\")\n",
    "    ax_time.set_xlabel(\"Epoch\")\n",
    "    ax_time.set_ylabel(\"Time (seconds)\")\n",
    "    ax_time.set_title(f\"Cumulative Training Time ({num_layer} layers)\")\n",
    "    ax_time.grid(True, which=\"both\", linestyle=\"--\", linewidth=0.5)\n",
    "    ax_time.legend()\n",
    "    time_plot_path = training_output_dir / f\"epoch_time_layers{num_layer}_{timestamp_tag}.png\"\n",
    "    fig_time.savefig(time_plot_path, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close(fig_time)\n",
    "\n",
    "    mat_path = training_output_dir / f\"training_curves_layers{num_layer}_{timestamp_tag}.mat\"\n",
    "    savemat(\n",
    "        str(mat_path),\n",
    "        {\n",
    "            \"epochs\": epochs_array,\n",
    "            \"losses\": np.array(losses, dtype=np.float64),\n",
    "            \"epoch_durations\": np.array(epoch_durations, dtype=np.float64),\n",
    "            \"cumulative_epoch_times\": np.array(cumulative_epoch_times, dtype=np.float64),\n",
    "            \"total_training_time\": np.array([total_training_time], dtype=np.float64),\n",
    "            \"num_layers\": np.array([num_layer], dtype=np.int32),\n",
    "        },\n",
    "    )\n",
    "\n",
    "    print(f\"✔ Saved training loss plot -> {loss_plot_path}\")\n",
    "    print(f\"✔ Saved cumulative time plot -> {time_plot_path}\")\n",
    "    print(f\"✔ Saved training log data (.mat) -> {mat_path}\")\n",
    "\n",
    "    propagation_dir = Path(\"results/propagation_slices\")\n",
    "    eigenmode_index = min(2, MMF_data_ts.shape[0] - 1)\n",
    "    propagation_summary = capture_eigenmode_propagation(\n",
    "        model=D2NN,\n",
    "        eigenmode_field=MMF_data_ts[eigenmode_index],\n",
    "        mode_index=eigenmode_index,\n",
    "        layer_size=layer_size,\n",
    "        z_input_to_first=z_input_to_first,\n",
    "        z_layers=z_layers,\n",
    "        z_prop=z_prop,\n",
    "        pixel_size=pixel_size,\n",
    "        wavelength=wavelength,\n",
    "        output_dir=propagation_dir,\n",
    "        tag=f\"layers{num_layer}_{timestamp_tag}\",\n",
    "    )\n",
    "    print(f\"✔ Saved eigenmode-{eigenmode_index + 1} propagation plot -> {propagation_summary['fig_path']}\")\n",
    "    print(f\"✔ Saved eigenmode-{eigenmode_index + 1} propagation data (.mat) -> {propagation_summary['mat_path']}\")\n",
    "\n",
    "    mode_triptych_records: list[dict[str, str | int]] = []\n",
    "    if evaluation_mode == \"eigenmode\":\n",
    "        triptych_dir = Path(\"results/mode_triptychs\")\n",
    "        mode_tag = f\"layers{num_layer}_{timestamp_tag}\"\n",
    "        for mode_idx in range(min(num_modes, len(MMF_data_ts))):\n",
    "            label_tensor = label_data[mode_idx, 0]\n",
    "            record = save_mode_triptych(\n",
    "                model=D2NN,\n",
    "                mode_index=mode_idx,\n",
    "                eigenmode_field=MMF_data_ts[mode_idx],\n",
    "                label_field=label_tensor,\n",
    "                layer_size=layer_size,\n",
    "                output_dir=triptych_dir,\n",
    "                tag=mode_tag,\n",
    "            )\n",
    "            mode_triptych_records.append(\n",
    "                {\n",
    "                    \"mode\": mode_idx + 1,\n",
    "                    \"fig\": record[\"fig_path\"],\n",
    "                    \"mat\": record[\"mat_path\"],\n",
    "                }\n",
    "            )\n",
    "            print(\n",
    "                f\"✔ Saved mode {mode_idx + 1} triptych -> {record['fig_path']}\\n\"\n",
    "                f\"  MAT -> {record['mat_path']}\"\n",
    "            )\n",
    "\n",
    "    all_training_summaries.append(\n",
    "        {\n",
    "            \"num_layers\": num_layer,\n",
    "            \"total_time\": total_training_time,\n",
    "            \"loss_plot\": str(loss_plot_path),\n",
    "            \"time_plot\": str(time_plot_path),\n",
    "            \"mat_path\": str(mat_path),\n",
    "            \"propagation_fig\": propagation_summary[\"fig_path\"],\n",
    "            \"propagation_mat\": propagation_summary[\"mat_path\"],\n",
    "            \"mode_triptychs\": mode_triptych_records,\n",
    "        }\n",
    "    )\n",
    "   \n",
    "    # === after training ===\n",
    "    ckpt_dir = \"checkpoints\"\n",
    "    os.makedirs(ckpt_dir, exist_ok=True)\n",
    "\n",
    "    ckpt = {\n",
    "        \"state_dict\": D2NN.state_dict(),\n",
    "        \"meta\": {\n",
    "            \"num_layers\":        len(D2NN.layers),\n",
    "            \"layer_size\":        layer_size,\n",
    "            \"z_layers\":          z_layers,\n",
    "            \"z_prop\":            z_prop,\n",
    "            \"pixel_size\":        pixel_size,\n",
    "            \"wavelength\":        wavelength,\n",
    "            \"padding_ratio\":     0.5,         \n",
    "            \"field_size\":        field_size,  \n",
    "            \"num_modes\":         num_modes, \n",
    "            \"z_input_to_first\":  z_input_to_first, \n",
    "        }\n",
    "    }\n",
    "    save_path = os.path.join(ckpt_dir, f\"odnn_{len(D2NN.layers)}layers.pth\")\n",
    "    torch.save(ckpt, save_path)\n",
    "    print(\"✔ Saved model ->\", save_path)\n",
    "    # Free GPU memory\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Cache phase masks for later visualization/export\n",
    "    phase_masks = []\n",
    "    for layer in D2NN.layers:\n",
    "        phase_np = layer.phase.detach().cpu().numpy()\n",
    "        phase_masks.append(np.remainder(phase_np, 2 * np.pi))\n",
    "    all_phase_masks.append(phase_masks)\n",
    "\n",
    "    # Collect evaluation metrics for this model\n",
    "    metrics = evaluate_spot_metrics(\n",
    "        D2NN,\n",
    "        test_loader,\n",
    "        evaluation_regions,\n",
    "        detect_radius=detectsize,\n",
    "        device=device,\n",
    "        pred_case=pred_case,\n",
    "        num_modes=num_modes,\n",
    "        phase_option=phase_option,\n",
    "        amplitudes=eval_amplitudes,\n",
    "        amplitudes_phases=eval_amplitudes_phases,\n",
    "        phases=eval_phases,\n",
    "        mmf_modes=MMF_data_ts,\n",
    "        field_size=field_size,\n",
    "        image_test_data=image_test_data,\n",
    "    )\n",
    "\n",
    "    model_metrics.append(metrics)\n",
    "    all_amplitudes_diff.append(metrics.get(\"amplitudes_diff\", np.array([])))\n",
    "    all_average_amplitudes_diff.append(float(metrics.get(\"avg_amplitudes_diff\", float(\"nan\"))))\n",
    "    all_amplitudes_relative_diff.append(float(metrics.get(\"avg_relative_amp_err\", float(\"nan\"))))\n",
    "    all_complex_weights_pred.append(metrics.get(\"complex_weights_pred\", np.array([])))\n",
    "    all_image_data_pred.append(metrics.get(\"image_data_pred\", np.array([])))\n",
    "    all_cc_recon_amp.append(metrics.get(\"cc_recon_amp\", np.array([])))\n",
    "    all_cc_recon_phase.append(metrics.get(\"cc_recon_phase\", np.array([])))\n",
    "    all_cc_real.append(metrics.get(\"cc_real\", np.array([])))\n",
    "    all_cc_imag.append(metrics.get(\"cc_imag\", np.array([])))\n",
    "\n",
    "    print(\n",
    "        format_metric_report(\n",
    "            num_modes=num_modes,\n",
    "            phase_option=phase_option,\n",
    "            pred_case=pred_case,\n",
    "            label=f\"{num_layer} layers\",\n",
    "            metrics=metrics,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7526effb-7543-4a42-b9fc-3ea3d6ea048e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training duration summary:\n",
      " - 3 layers: 10.79 s (~0.18 min)\n",
      "   Loss curve: results/training_analysis/loss_curve_layers3_20251103_164848.png\n",
      "   Time curve: results/training_analysis/epoch_time_layers3_20251103_164848.png\n",
      "   Data (.mat): results/training_analysis/training_curves_layers3_20251103_164848.mat\n",
      "   Propagation plot: results/propagation_slices/propagation_mode3_layers3_20251103_164848.png\n",
      "   Propagation data (.mat): results/propagation_slices/propagation_mode3_layers3_20251103_164848.mat\n",
      "   Mode triptychs:\n",
      "     Mode 1: fig=results/mode_triptychs/mode1_layers3_20251103_164848.png, mat=results/mode_triptychs/mode1_layers3_20251103_164848.mat\n",
      "     Mode 2: fig=results/mode_triptychs/mode2_layers3_20251103_164848.png, mat=results/mode_triptychs/mode2_layers3_20251103_164848.mat\n",
      "     Mode 3: fig=results/mode_triptychs/mode3_layers3_20251103_164848.png, mat=results/mode_triptychs/mode3_layers3_20251103_164848.mat\n",
      "     Mode 4: fig=results/mode_triptychs/mode4_layers3_20251103_164848.png, mat=results/mode_triptychs/mode4_layers3_20251103_164848.mat\n",
      "     Mode 5: fig=results/mode_triptychs/mode5_layers3_20251103_164848.png, mat=results/mode_triptychs/mode5_layers3_20251103_164848.mat\n",
      "     Mode 6: fig=results/mode_triptychs/mode6_layers3_20251103_164848.png, mat=results/mode_triptychs/mode6_layers3_20251103_164848.mat\n",
      "✔ Saved: /home/ydzhang/Desktop/odnn_code/results/plots/Amp_3layers.png\n"
     ]
    }
   ],
   "source": [
    "if all_training_summaries:\n",
    "    print(\"\\nTraining duration summary:\")\n",
    "    for summary in all_training_summaries:\n",
    "        minutes = summary[\"total_time\"] / 60\n",
    "        print(\n",
    "            f\" - {summary['num_layers']} layers: {summary['total_time']:.2f} s \"\n",
    "            f\"(~{minutes:.2f} min)\"\n",
    "        )\n",
    "        print(f\"   Loss curve: {summary['loss_plot']}\")\n",
    "        print(f\"   Time curve: {summary['time_plot']}\")\n",
    "        print(f\"   Data (.mat): {summary['mat_path']}\")\n",
    "        print(f\"   Propagation plot: {summary['propagation_fig']}\")\n",
    "        print(f\"   Propagation data (.mat): {summary['propagation_mat']}\")\n",
    "        mode_triptychs = summary.get(\"mode_triptychs\", [])\n",
    "        if mode_triptychs:\n",
    "            print(\"   Mode triptychs:\")\n",
    "            for trip in mode_triptychs:\n",
    "                print(\n",
    "                    f\"     Mode {trip['mode']}: fig={trip['fig']}, mat={trip['mat']}\"\n",
    "                )\n",
    "\n",
    "save_dir = \"results/plots\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "num_samples_to_display = 6\n",
    "for idx, num_layer in enumerate(num_layer_option):\n",
    "    plot_amplitude_comparison_grid(\n",
    "        image_test_data,\n",
    "        all_image_data_pred[idx],\n",
    "        all_cc_recon_amp[idx],\n",
    "        max_samples=num_samples_to_display,\n",
    "        save_path=os.path.join(save_dir, f\"Amp_{num_layer}layers.png\"),\n",
    "        title=f\"Amp. distribution of Real and Predicted Images({num_layer}_layer_ODNN)\",\n",
    "    )\n",
    "\n",
    "# #直观的看看输出和label的差异\n",
    "# for s in [0, 1, 2, 5]:\n",
    "#     plot_sys_vs_label_strict(\n",
    "#         D2NN,\n",
    "#         test_dataset,\n",
    "#         sample_idx=s,\n",
    "#         evaluation_regions=evaluation_regions,\n",
    "#         detect_radius=detectsize,\n",
    "#         save_path=f\"results/plots/IO_Pred_Label_RAW_{s}.png\",\n",
    "#         device=device,\n",
    "#         use_big_canvas=False,\n",
    "#         sys_scale=\"bg_pct\",\n",
    "#         sys_pct=99.5,\n",
    "#         clip_pct=99.5,\n",
    "#         mask_roi_for_scale=True,\n",
    "#         show_signed=True,\n",
    "#     )\n",
    "#     plot_reconstruction_vs_input(\n",
    "#         image_test_data=image_test_data,\n",
    "#         reconstructed_fields=all_image_data_pred,\n",
    "#         sample_idx=s,\n",
    "#         model_idx=0,\n",
    "#         save_path=f\"results/plots/Reconstruction_vs_Input_{s}.png\",\n",
    "#     )\n",
    "\n",
    "# #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef73e573-19fe-4c91-b4cb-3703d624d156",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'save_superposition_triptych' from 'odnn_training_visualization' (/home/ydzhang/Desktop/odnn_code/odnn_training_visualization.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 44\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01modnn_training_eval\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     36\u001b[39m     build_superposition_eval_context,\n\u001b[32m     37\u001b[39m     compute_model_prediction_metrics,\n\u001b[32m   (...)\u001b[39m\u001b[32m     41\u001b[39m     infer_superposition_output,\n\u001b[32m     42\u001b[39m )\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01modnn_training_io\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m save_masks_one_file_per_layer, save_to_mat_light_plus\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01modnn_training_visualization\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     45\u001b[39m     capture_eigenmode_propagation,\n\u001b[32m     46\u001b[39m     export_superposition_slices,\n\u001b[32m     47\u001b[39m     plot_amplitude_comparison_grid,\n\u001b[32m     48\u001b[39m     plot_reconstruction_vs_input,\n\u001b[32m     49\u001b[39m     plot_sys_vs_label_strict,\n\u001b[32m     50\u001b[39m     save_superposition_triptych,\n\u001b[32m     51\u001b[39m     save_mode_triptych,\n\u001b[32m     52\u001b[39m     visualize_model_slices,\n\u001b[32m     53\u001b[39m )\n\u001b[32m     55\u001b[39m SEED = \u001b[32m424242\u001b[39m\n\u001b[32m     56\u001b[39m random.seed(SEED)\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'save_superposition_triptych' from 'odnn_training_visualization' (/home/ydzhang/Desktop/odnn_code/odnn_training_visualization.py)"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from scipy.io import savemat\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from ODNN_functions import (\n",
    "    create_evaluation_regions,\n",
    "    generate_complex_weights,\n",
    "    generate_fields_ts,\n",
    ")\n",
    "from odnn_generate_label import (\n",
    "    compute_label_centers,\n",
    "    compose_labels_from_patterns,\n",
    "    generate_detector_patterns,\n",
    ")\n",
    "from odnn_io import load_complex_modes_from_mat\n",
    "from odnn_model import D2NNModel\n",
    "from odnn_processing import prepare_sample\n",
    "from odnn_training_eval import (\n",
    "    build_superposition_eval_context,\n",
    "    compute_model_prediction_metrics,\n",
    "    evaluate_spot_metrics,\n",
    "    format_metric_report,\n",
    "    generate_superposition_sample,\n",
    "    infer_superposition_output,\n",
    ")\n",
    "from odnn_training_io import save_masks_one_file_per_layer, save_to_mat_light_plus\n",
    "from odnn_training_visualization import (\n",
    "    capture_eigenmode_propagation,\n",
    "    export_superposition_slices,\n",
    "    plot_amplitude_comparison_grid,\n",
    "    plot_reconstruction_vs_input,\n",
    "    plot_sys_vs_label_strict,\n",
    "    save_superposition_triptych,\n",
    "    save_mode_triptych,\n",
    "    visualize_model_slices,\n",
    ")\n",
    "\n",
    "SEED = 424242\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# 让 cuDNN/算子走确定性分支\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.use_deterministic_algorithms(True)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:5')           # 或者 'cuda:0'\n",
    "    print('Using Device:', device)\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('Using Device: CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2152840f-95d7-4e29-a212-ece1f00dc5c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m由于上一个单元格中出现错误，单元格已取消。"
     ]
    }
   ],
   "source": [
    "field_size = 25  #the field size in eigenmodes_OM4 is 50 pixels\n",
    "layer_size = 100 #400#300#100\n",
    "num_data = 1000 # options: 1. random datas 2.eigenmodes\n",
    "num_modes = 6 #the mode number of MMF 3 6 10\n",
    "circle_focus_radius = 5 # radius when using uniform circular detectors\n",
    "circle_detectsize = 10  # square window size for circular detectors\n",
    "eigenmode_focus_radius = 12.5  # radius when using eigenmode patterns\n",
    "eigenmode_detectsize = 27    # square window size for eigenmode patterns\n",
    "focus_radius = circle_focus_radius\n",
    "detectsize = circle_detectsize\n",
    "batch_size = 16\n",
    "\n",
    "# Evaluation selection: \"eigenmode\" uses the base modes, \"superposition\" samples random mixtures\n",
    "evaluation_mode = \"superposition\"  # options: \"eigenmode\", \"superposition\"\n",
    "num_superposition_eval_samples = 1000\n",
    "num_superposition_visual_samples = 20\n",
    "run_superposition_debug = True\n",
    "save_superposition_plots = True\n",
    "save_superposition_slices = True\n",
    "run_misalignment_robustness = True\n",
    "label_pattern_mode = \"circle\"  # options: \"eigenmode\", \"circle\"\n",
    "# Define multiple D2NN models \n",
    "num_layer_option = [3]   #, 3]#, 4]  # Define the different layer-number ODNN\n",
    "all_losses = [] #the loss for each epoch of each ODNN model\n",
    "all_phase_masks = [] #the phase masks field of each ODNN model\n",
    "all_predictions = [] #the output light field of each ODNN model\n",
    "model_metrics: list[dict] = []\n",
    "all_amplitudes_diff: list[np.ndarray] = []\n",
    "all_average_amplitudes_diff: list[float] = []\n",
    "all_amplitudes_relative_diff: list[float] = []\n",
    "all_complex_weights_pred: list[np.ndarray] = []\n",
    "all_image_data_pred: list[np.ndarray] = []\n",
    "all_cc_real: list[np.ndarray] = []\n",
    "all_cc_imag: list[np.ndarray] = []\n",
    "all_cc_recon_amp: list[np.ndarray] = []\n",
    "all_cc_recon_phase: list[np.ndarray] = []\n",
    "all_training_summaries: list[dict] = []\n",
    "# SLM\n",
    "z_layers   = 40e-6        # 原 47.571e-3  -> 40 μm\n",
    "pixel_size = 1e-6\n",
    "z_prop     = 120e-6        # 原 16.74e-2   -> 60 μm plus 40（最后一层到相机）\n",
    "wavelength = 1568e-9      # 原 1568     -> 1550 nm\n",
    "z_input_to_first = 40e-6  # 40 μm # 新增：输入面到第一层的传播距离"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1946efc0-e168-451c-9073-dae45267fbc1",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m由于上一个单元格中出现错误，单元格已取消。"
     ]
    }
   ],
   "source": [
    "eigenmodes_OM4 = load_complex_modes_from_mat(\n",
    "    'mmf_6modes_25_PD_1.15.mat',\n",
    "    key='modes_field'\n",
    ")\n",
    "# (H, W, M)\n",
    "print(\"Loaded modes shape:\", eigenmodes_OM4.shape, \"dtype:\", eigenmodes_OM4.dtype)\n",
    "\n",
    "# 取前 num_modes 个 → (H, W, M_sel) → (M_sel, H, W)\n",
    "MMF_data = eigenmodes_OM4[:, :, :num_modes].transpose(2, 0, 1)\n",
    "MMF_data_amp_norm = (np.abs(MMF_data) - np.min(np.abs(MMF_data))) / (np.max(np.abs(MMF_data)) - np.min(np.abs(MMF_data)))\n",
    "\n",
    "MMF_data = MMF_data_amp_norm * np.exp(1j * np.angle(MMF_data))\n",
    "\n",
    "#要是以后确定了用4我在想要不要去掉其他选项\n",
    "phase_option = 4\n",
    "#phase_option 1: (0,0,...,0)\n",
    "#phase_option 2: (0,2pi,...,2pi)\n",
    "#phase_option 3: (0,pi,...,2pi)\n",
    "#phase_option 4: eigenmodes\n",
    "#phase_option 5: (0,pi,...,pi)\n",
    "\n",
    "if phase_option in [1, 2, 3, 5]:\n",
    "    amplitudes,phases = generate_complex_weights(num_data,num_modes,phase_option)\n",
    "\n",
    "if phase_option == 4:\n",
    "    num_data = num_modes # use the eigenmodes to train ODNN\n",
    "    amplitudes = np.eye(num_modes)#[[1,0,0][0,1,0][0,0,1]]\n",
    "    phases = np.eye(num_modes)\n",
    "\n",
    "amplitudes_phases_ori = np.hstack((amplitudes[:, :], phases[:, 1:]))  # amplitudes (l2 norm) phases\n",
    "amplitudes_phases = np.hstack((amplitudes[:, :], phases[:, 1:]/(2*np.pi)))  # amplitudes (l2 norm) phases (0-1)\n",
    "\n",
    "# Generate complex weights vector with specified amplitudes and phases\n",
    "complex_weights = amplitudes * np.exp(1j * phases)\n",
    "MMF_data_ts = torch.from_numpy(MMF_data)\n",
    "complex_weights_ts = torch.from_numpy(complex_weights)\n",
    "image_data = generate_fields_ts(complex_weights_ts, MMF_data_ts, num_data, num_modes, field_size).to(torch.complex64)\n",
    "image_test_data = image_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6836eb-e41e-40c6-a9f8-9aecbfdccb2d",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m由于上一个单元格中出现错误，单元格已取消。"
     ]
    }
   ],
   "source": [
    "'''\n",
    "pred_case = 1: only amplitudes prediction\n",
    "pred_case = 2: only phases prediction\n",
    "pred_case = 3: amplitudes and phases prediction\n",
    "pred_case = 4: amplitudes and phases prediction (extra energy phase area)\n",
    "'''\n",
    "#\n",
    "pred_case = 1\n",
    "label_data = torch.zeros([num_data,1,layer_size,layer_size])\n",
    "label_size = layer_size\n",
    "\n",
    "if pred_case == 1: # 3\n",
    "    num_detector = num_modes\n",
    "    detector_focus_radius = focus_radius\n",
    "    detector_detectsize = detectsize\n",
    "    if label_pattern_mode == \"eigenmode\":\n",
    "        pattern_stack = np.transpose(np.abs(MMF_data), (1, 2, 0))\n",
    "        pattern_h, pattern_w, _ = pattern_stack.shape\n",
    "        if pattern_h > label_size or pattern_w > label_size:\n",
    "            raise ValueError(\n",
    "                f\"Eigenmode pattern size ({pattern_h}x{pattern_w}) exceeds label canvas {label_size}.\"\n",
    "            )\n",
    "        layout_radius = math.ceil(max(pattern_h, pattern_w) / 2)\n",
    "        detector_focus_radius = eigenmode_focus_radius\n",
    "        detector_detectsize = eigenmode_detectsize\n",
    "    elif label_pattern_mode == \"circle\":\n",
    "        circle_radius = circle_focus_radius\n",
    "        pattern_size = circle_radius * 2\n",
    "        if pattern_size % 2 == 0:\n",
    "            pattern_size += 1  \n",
    "        pattern_stack = generate_detector_patterns(pattern_size, pattern_size, num_detector, shape=\"circle\")\n",
    "        layout_radius = circle_radius\n",
    "        detector_focus_radius = circle_radius\n",
    "        detector_detectsize = circle_detectsize\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown label_pattern_mode: {label_pattern_mode}\")\n",
    "\n",
    "    centers, _, _ = compute_label_centers(label_size, label_size, num_detector, layout_radius)\n",
    "    mode_label_maps = [\n",
    "        compose_labels_from_patterns(\n",
    "            label_size,\n",
    "            label_size,\n",
    "            pattern_stack,\n",
    "            centers,\n",
    "            Index=i + 1,\n",
    "            visualize=False,\n",
    "        )\n",
    "        for i in range(num_detector)\n",
    "    ]\n",
    "    MMF_Label_data = torch.from_numpy(\n",
    "        np.stack(mode_label_maps, axis=2).astype(np.float32)\n",
    "    )\n",
    "    amplitude_weights = torch.from_numpy(amplitudes_phases[:num_data, 0:num_modes]).float()\n",
    "    combined_labels = (\n",
    "        amplitude_weights[:, None, None, :] * MMF_Label_data.unsqueeze(0)\n",
    "    ).sum(dim=3)\n",
    "    label_data[:, 0, :, :] = combined_labels\n",
    "    focus_radius = detector_focus_radius\n",
    "    detectsize = detector_detectsize\n",
    "\n",
    "label_test_data = label_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88833858-2233-4d61-9d5a-2e71af80ec51",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m由于上一个单元格中出现错误，单元格已取消。"
     ]
    }
   ],
   "source": [
    "train_dataset = [\n",
    "    prepare_sample(image_data[i], label_data[i], layer_size) for i in range(len(label_data))\n",
    "]\n",
    "train_tensor_data = TensorDataset(*[torch.stack(tensors) for tensors in zip(*train_dataset)])\n",
    "g = torch.Generator()\n",
    "g.manual_seed(SEED)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_tensor_data,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,               # 顺序会被 g 固定\n",
    "    generator=g,                # 固定打乱\n",
    "   \n",
    ")\n",
    "\n",
    "superposition_eval_ctx: dict | None = None\n",
    "if evaluation_mode == \"eigenmode\":\n",
    "    test_dataset = train_dataset\n",
    "    test_tensor_data = train_tensor_data\n",
    "    test_loader = DataLoader(test_tensor_data, batch_size=batch_size, shuffle=False)\n",
    "    eval_amplitudes = amplitudes\n",
    "    eval_amplitudes_phases = amplitudes_phases\n",
    "    eval_phases = phases\n",
    "    image_test_data = image_data\n",
    "elif evaluation_mode == \"superposition\":\n",
    "    if pred_case != 1:\n",
    "        raise ValueError(\"Superposition evaluation mode currently supports pred_case == 1 only.\")\n",
    "    super_ctx = build_superposition_eval_context(\n",
    "        num_superposition_eval_samples,\n",
    "        num_modes=num_modes,\n",
    "        field_size=field_size,\n",
    "        layer_size=layer_size,\n",
    "        mmf_modes=MMF_data_ts,\n",
    "        mmf_label_data=MMF_Label_data,\n",
    "        batch_size=batch_size,\n",
    "        second_mode_half_range=True,\n",
    "    )\n",
    "    test_dataset = super_ctx[\"dataset\"]\n",
    "    test_tensor_data = super_ctx[\"tensor_dataset\"]\n",
    "    test_loader = super_ctx[\"loader\"]\n",
    "    image_test_data = super_ctx[\"image_data\"]\n",
    "    eval_amplitudes = super_ctx[\"amplitudes\"]\n",
    "    eval_amplitudes_phases = super_ctx[\"amplitudes_phases\"]\n",
    "    eval_phases = super_ctx[\"phases\"]\n",
    "    superposition_eval_ctx = super_ctx\n",
    "else:\n",
    "    raise ValueError(f\"Unknown evaluation_mode: {evaluation_mode}\")\n",
    "\n",
    "# Generate detection regions using existing function\n",
    "if pred_case ==1:\n",
    "    evaluation_regions = create_evaluation_regions(layer_size, layer_size, num_detector, focus_radius, detectsize)\n",
    "    print(\"Detection Regions:\", evaluation_regions)\n",
    "\n",
    "\n",
    "def shift_complex_batch(batch: torch.Tensor, shift_y: int, shift_x: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Translate a batch of complex fields by (shift_y, shift_x) pixels with zero padding.\n",
    "    Positive shift_y moves downward; positive shift_x moves right.\n",
    "    \"\"\"\n",
    "    if shift_y == 0 and shift_x == 0:\n",
    "        return batch\n",
    "\n",
    "    _, _, height, width = batch.shape\n",
    "    if abs(shift_y) >= height or abs(shift_x) >= width:\n",
    "        return torch.zeros_like(batch)\n",
    "\n",
    "    real_imag = torch.view_as_real(batch)\n",
    "    shifted = torch.zeros_like(real_imag)\n",
    "\n",
    "    if shift_y >= 0:\n",
    "        src_y = slice(0, height - shift_y)\n",
    "        dst_y = slice(shift_y, height)\n",
    "    else:\n",
    "        src_y = slice(-shift_y, height)\n",
    "        dst_y = slice(0, height + shift_y)\n",
    "\n",
    "    if shift_x >= 0:\n",
    "        src_x = slice(0, width - shift_x)\n",
    "        dst_x = slice(shift_x, width)\n",
    "    else:\n",
    "        src_x = slice(-shift_x, width)\n",
    "        dst_x = slice(0, width + shift_x)\n",
    "\n",
    "    shifted[:, :, dst_y, dst_x, :] = real_imag[:, :, src_y, src_x, :]\n",
    "    return torch.view_as_complex(shifted)\n",
    "\n",
    "\n",
    "def compute_amp_relative_error_with_shift(\n",
    "    model: torch.nn.Module,\n",
    "    loader,\n",
    "    *,\n",
    "    shift_y_px: int,\n",
    "    shift_x_px: int,\n",
    "    evaluation_regions,\n",
    "    pred_case: int,\n",
    "    num_modes: int,\n",
    "    eval_amplitudes: np.ndarray,\n",
    "    eval_amplitudes_phases: np.ndarray,\n",
    "    eval_phases: np.ndarray,\n",
    "    phase_option: int,\n",
    "    mmf_modes: torch.Tensor,\n",
    "    field_size: int,\n",
    "    image_test_data: torch.Tensor,\n",
    "    device: torch.device,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Evaluate amplitude-related metrics when the input field is shifted by (shift_y_px, shift_x_px).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_weights_pred: list[np.ndarray] = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, _ in loader:\n",
    "            images = images.to(device, dtype=torch.complex64, non_blocking=True)\n",
    "            shifted_images = shift_complex_batch(images, shift_y_px, shift_x_px)\n",
    "            preds = model(shifted_images)\n",
    "            preds_np = preds.detach().cpu().numpy()\n",
    "\n",
    "            for sample_idx in range(preds_np.shape[0]):\n",
    "                intensity_map = preds_np[sample_idx, 0]\n",
    "                weights = []\n",
    "                for (x0, x1, y0, y1) in evaluation_regions:\n",
    "                    weights.append(float(intensity_map[y0:y1, x0:x1].mean()))\n",
    "                weights = np.asarray(weights, dtype=np.float64)\n",
    "\n",
    "                if pred_case == 3 and num_modes <= len(weights):\n",
    "                    norm_val = np.linalg.norm(weights[:num_modes])\n",
    "                    if norm_val > 0:\n",
    "                        weights[:num_modes] /= norm_val\n",
    "                else:\n",
    "                    norm_val = np.linalg.norm(weights)\n",
    "                    if norm_val > 0:\n",
    "                        weights /= norm_val\n",
    "\n",
    "                all_weights_pred.append(weights)\n",
    "\n",
    "    metrics = compute_model_prediction_metrics(\n",
    "        all_weights_pred,\n",
    "        eval_amplitudes,\n",
    "        eval_amplitudes_phases,\n",
    "        eval_phases,\n",
    "        phase_option,\n",
    "        pred_case,\n",
    "        num_modes,\n",
    "        mmf_modes,\n",
    "        field_size,\n",
    "        image_test_data,\n",
    "    )\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ef283f-5d87-4400-830d-2ef312c06915",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m由于上一个单元格中出现错误，单元格已取消。"
     ]
    }
   ],
   "source": [
    "for num_layer in num_layer_option:\n",
    "    print(f\"\\nTraining D2NN with {num_layer} layers...\\n\")\n",
    "\n",
    "    D2NN = D2NNModel(\n",
    "        num_layers=num_layer,\n",
    "        layer_size=layer_size,\n",
    "        z_layers=z_layers,\n",
    "        z_prop=z_prop,\n",
    "        pixel_size=pixel_size,\n",
    "        wavelength=wavelength,\n",
    "        device=device,\n",
    "        padding_ratio=0.5,\n",
    "        z_input_to_first=z_input_to_first,   # NEW\n",
    "    ).to(device)\n",
    "\n",
    "    print(D2NN)\n",
    "\n",
    "    # Training\n",
    "    criterion = nn.MSELoss()  # Define loss function (对比的是loss)\n",
    "    optimizer = optim.Adam(D2NN.parameters(), lr=1.99) \n",
    "    scheduler = ExponentialLR(optimizer, gamma=0.99)  \n",
    "    epochs = 1000\n",
    "    losses = []\n",
    "    epoch_durations: list[float] = []\n",
    "    training_start_time = time.time()\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        D2NN.train()\n",
    "        epoch_loss = 0\n",
    "        for images, labels in train_loader:\n",
    "            images = images.to(device, dtype=torch.complex64, non_blocking=True)\n",
    "            labels = labels.to(device, dtype=torch.float32,   non_blocking=True)\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            outputs = D2NN(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        scheduler.step()\n",
    "        avg_loss = epoch_loss / len(train_loader)  # Calculate average loss for the epoch\n",
    "        losses.append(avg_loss)  # the loss for each model\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.synchronize(device)\n",
    "        epoch_duration = time.time() - epoch_start_time\n",
    "        epoch_durations.append(epoch_duration)\n",
    "\n",
    "        if epoch % 100 == 0 or epoch == 1 or epoch == epochs:\n",
    "            print(\n",
    "                f'Epoch [{epoch}/{epochs}], Loss: {avg_loss:.18f}, '\n",
    "                f'Epoch Time: {epoch_duration:.2f} seconds'\n",
    "            )\n",
    "\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.synchronize(device)\n",
    "    total_training_time = time.time() - training_start_time\n",
    "    print(\n",
    "        f'Total training time for {num_layer}-layer model: {total_training_time:.2f} seconds '\n",
    "        f'(~{total_training_time / 60:.2f} minutes)'\n",
    "    )\n",
    "    all_losses.append(losses)  # save the loss for each model\n",
    "    training_output_dir = Path(\"results/training_analysis\")\n",
    "    training_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    epochs_array = np.arange(1, epochs + 1, dtype=np.int32)\n",
    "    cumulative_epoch_times = np.cumsum(epoch_durations)\n",
    "    timestamp_tag = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(epochs_array, losses, label=\"Training Loss\")\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.set_title(f\"D2NN Training Loss ({num_layer} layers)\")\n",
    "    ax.grid(True, which=\"both\", linestyle=\"--\", linewidth=0.5)\n",
    "    ax.legend()\n",
    "    loss_plot_path = training_output_dir / f\"loss_curve_layers{num_layer}_{timestamp_tag}.png\"\n",
    "    fig.savefig(loss_plot_path, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "    fig_time, ax_time = plt.subplots()\n",
    "    ax_time.plot(epochs_array, cumulative_epoch_times, label=\"Cumulative Time\")\n",
    "    ax_time.set_xlabel(\"Epoch\")\n",
    "    ax_time.set_ylabel(\"Time (seconds)\")\n",
    "    ax_time.set_title(f\"Cumulative Training Time ({num_layer} layers)\")\n",
    "    ax_time.grid(True, which=\"both\", linestyle=\"--\", linewidth=0.5)\n",
    "    ax_time.legend()\n",
    "    time_plot_path = training_output_dir / f\"epoch_time_layers{num_layer}_{timestamp_tag}.png\"\n",
    "    fig_time.savefig(time_plot_path, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close(fig_time)\n",
    "\n",
    "    mat_path = training_output_dir / f\"training_curves_layers{num_layer}_{timestamp_tag}.mat\"\n",
    "    savemat(\n",
    "        str(mat_path),\n",
    "        {\n",
    "            \"epochs\": epochs_array,\n",
    "            \"losses\": np.array(losses, dtype=np.float64),\n",
    "            \"epoch_durations\": np.array(epoch_durations, dtype=np.float64),\n",
    "            \"cumulative_epoch_times\": np.array(cumulative_epoch_times, dtype=np.float64),\n",
    "            \"total_training_time\": np.array([total_training_time], dtype=np.float64),\n",
    "            \"num_layers\": np.array([num_layer], dtype=np.int32),\n",
    "        },\n",
    "    )\n",
    "\n",
    "    print(f\"✔ Saved training loss plot -> {loss_plot_path}\")\n",
    "    print(f\"✔ Saved cumulative time plot -> {time_plot_path}\")\n",
    "    print(f\"✔ Saved training log data (.mat) -> {mat_path}\")\n",
    "\n",
    "    propagation_dir = Path(\"results/propagation_slices\")\n",
    "    eigenmode_index = min(2, MMF_data_ts.shape[0] - 1)\n",
    "    propagation_summary = capture_eigenmode_propagation(\n",
    "        model=D2NN,\n",
    "        eigenmode_field=MMF_data_ts[eigenmode_index],\n",
    "        mode_index=eigenmode_index,\n",
    "        layer_size=layer_size,\n",
    "        z_input_to_first=z_input_to_first,\n",
    "        z_layers=z_layers,\n",
    "        z_prop=z_prop,\n",
    "        pixel_size=pixel_size,\n",
    "        wavelength=wavelength,\n",
    "        output_dir=propagation_dir,\n",
    "        tag=f\"layers{num_layer}_{timestamp_tag}\",\n",
    "    )\n",
    "    print(f\"✔ Saved eigenmode-{eigenmode_index + 1} propagation plot -> {propagation_summary['fig_path']}\")\n",
    "    print(f\"✔ Saved eigenmode-{eigenmode_index + 1} propagation data (.mat) -> {propagation_summary['mat_path']}\")\n",
    "\n",
    "    mode_triptych_records: list[dict[str, str | int]] = []\n",
    "    if evaluation_mode == \"eigenmode\":\n",
    "        triptych_dir = Path(\"results/mode_triptychs\")\n",
    "        mode_tag = f\"layers{num_layer}_{timestamp_tag}\"\n",
    "        for mode_idx in range(min(num_modes, len(MMF_data_ts))):\n",
    "            label_tensor = label_data[mode_idx, 0]\n",
    "            record = save_mode_triptych(\n",
    "                model=D2NN,\n",
    "                mode_index=mode_idx,\n",
    "                eigenmode_field=MMF_data_ts[mode_idx],\n",
    "                label_field=label_tensor,\n",
    "                layer_size=layer_size,\n",
    "                output_dir=triptych_dir,\n",
    "                tag=mode_tag,\n",
    "            )\n",
    "            mode_triptych_records.append(\n",
    "                {\n",
    "                    \"mode\": mode_idx + 1,\n",
    "                    \"fig\": record[\"fig_path\"],\n",
    "                    \"mat\": record[\"mat_path\"],\n",
    "                }\n",
    "            )\n",
    "            print(\n",
    "                f\"✔ Saved mode {mode_idx + 1} triptych -> {record['fig_path']}\\n\"\n",
    "                f\"  MAT -> {record['mat_path']}\"\n",
    "            )\n",
    "\n",
    "    all_training_summaries.append(\n",
    "        {\n",
    "            \"num_layers\": num_layer,\n",
    "            \"total_time\": total_training_time,\n",
    "            \"loss_plot\": str(loss_plot_path),\n",
    "            \"time_plot\": str(time_plot_path),\n",
    "            \"mat_path\": str(mat_path),\n",
    "            \"propagation_fig\": propagation_summary[\"fig_path\"],\n",
    "            \"propagation_mat\": propagation_summary[\"mat_path\"],\n",
    "            \"mode_triptychs\": mode_triptych_records,\n",
    "        }\n",
    "    )\n",
    "   \n",
    "    # === after training ===\n",
    "    ckpt_dir = \"checkpoints\"\n",
    "    os.makedirs(ckpt_dir, exist_ok=True)\n",
    "\n",
    "    ckpt = {\n",
    "        \"state_dict\": D2NN.state_dict(),\n",
    "        \"meta\": {\n",
    "            \"num_layers\":        len(D2NN.layers),\n",
    "            \"layer_size\":        layer_size,\n",
    "            \"z_layers\":          z_layers,\n",
    "            \"z_prop\":            z_prop,\n",
    "            \"pixel_size\":        pixel_size,\n",
    "            \"wavelength\":        wavelength,\n",
    "            \"padding_ratio\":     0.5,         \n",
    "            \"field_size\":        field_size,  \n",
    "            \"num_modes\":         num_modes, \n",
    "            \"z_input_to_first\":  z_input_to_first, \n",
    "        }\n",
    "    }\n",
    "    save_path = os.path.join(ckpt_dir, f\"odnn_{len(D2NN.layers)}layers.pth\")\n",
    "    torch.save(ckpt, save_path)\n",
    "    print(\"✔ Saved model ->\", save_path)\n",
    "    # Free GPU memory\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Cache phase masks for later visualization/export\n",
    "    phase_masks = []\n",
    "    for layer in D2NN.layers:\n",
    "        phase_np = layer.phase.detach().cpu().numpy()\n",
    "        phase_masks.append(np.remainder(phase_np, 2 * np.pi))\n",
    "    all_phase_masks.append(phase_masks)\n",
    "\n",
    "    # Collect evaluation metrics for this model\n",
    "    metrics = evaluate_spot_metrics(\n",
    "        D2NN,\n",
    "        test_loader,\n",
    "        evaluation_regions,\n",
    "        detect_radius=detectsize,\n",
    "        device=device,\n",
    "        pred_case=pred_case,\n",
    "        num_modes=num_modes,\n",
    "        phase_option=phase_option,\n",
    "        amplitudes=eval_amplitudes,\n",
    "        amplitudes_phases=eval_amplitudes_phases,\n",
    "        phases=eval_phases,\n",
    "        mmf_modes=MMF_data_ts,\n",
    "        field_size=field_size,\n",
    "        image_test_data=image_test_data,\n",
    "    )\n",
    "\n",
    "    model_metrics.append(metrics)\n",
    "    all_amplitudes_diff.append(metrics.get(\"amplitudes_diff\", np.array([])))\n",
    "    all_average_amplitudes_diff.append(float(metrics.get(\"avg_amplitudes_diff\", float(\"nan\"))))\n",
    "    all_amplitudes_relative_diff.append(float(metrics.get(\"avg_relative_amp_err\", float(\"nan\"))))\n",
    "    all_complex_weights_pred.append(metrics.get(\"complex_weights_pred\", np.array([])))\n",
    "    all_image_data_pred.append(metrics.get(\"image_data_pred\", np.array([])))\n",
    "    all_cc_recon_amp.append(metrics.get(\"cc_recon_amp\", np.array([])))\n",
    "    all_cc_recon_phase.append(metrics.get(\"cc_recon_phase\", np.array([])))\n",
    "    all_cc_real.append(metrics.get(\"cc_real\", np.array([])))\n",
    "    all_cc_imag.append(metrics.get(\"cc_imag\", np.array([])))\n",
    "\n",
    "    print(\n",
    "        format_metric_report(\n",
    "            num_modes=num_modes,\n",
    "            phase_option=phase_option,\n",
    "            pred_case=pred_case,\n",
    "            label=f\"{num_layer} layers\",\n",
    "            metrics=metrics,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f3d43d-9621-4020-95b6-268025e2f4fb",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m由于上一个单元格中出现错误，单元格已取消。"
     ]
    }
   ],
   "source": [
    "if all_training_summaries:\n",
    "    print(\"\\nTraining duration summary:\")\n",
    "    for summary in all_training_summaries:\n",
    "        minutes = summary[\"total_time\"] / 60\n",
    "        print(\n",
    "            f\" - {summary['num_layers']} layers: {summary['total_time']:.2f} s \"\n",
    "            f\"(~{minutes:.2f} min)\"\n",
    "        )\n",
    "        print(f\"   Loss curve: {summary['loss_plot']}\")\n",
    "        print(f\"   Time curve: {summary['time_plot']}\")\n",
    "        print(f\"   Data (.mat): {summary['mat_path']}\")\n",
    "        print(f\"   Propagation plot: {summary['propagation_fig']}\")\n",
    "        print(f\"   Propagation data (.mat): {summary['propagation_mat']}\")\n",
    "        mode_triptychs = summary.get(\"mode_triptychs\", [])\n",
    "        if mode_triptychs:\n",
    "            print(\"   Mode triptychs:\")\n",
    "            for trip in mode_triptychs:\n",
    "                print(\n",
    "                    f\"     Mode {trip['mode']}: fig={trip['fig']}, mat={trip['mat']}\"\n",
    "                )\n",
    "\n",
    "save_dir = \"results/plots\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "num_samples_to_display = 6\n",
    "for idx, num_layer in enumerate(num_layer_option):\n",
    "    plot_amplitude_comparison_grid(\n",
    "        image_test_data,\n",
    "        all_image_data_pred[idx],\n",
    "        all_cc_recon_amp[idx],\n",
    "        max_samples=num_samples_to_display,\n",
    "        save_path=os.path.join(save_dir, f\"Amp_{num_layer}layers.png\"),\n",
    "        title=f\"Amp. distribution of Real and Predicted Images({num_layer}_layer_ODNN)\",\n",
    "    )\n",
    "\n",
    "# #直观的看看输出和label的差异\n",
    "# for s in [0, 1, 2, 5]:\n",
    "#     plot_sys_vs_label_strict(\n",
    "#         D2NN,\n",
    "#         test_dataset,\n",
    "#         sample_idx=s,\n",
    "#         evaluation_regions=evaluation_regions,\n",
    "#         detect_radius=detectsize,\n",
    "#         save_path=f\"results/plots/IO_Pred_Label_RAW_{s}.png\",\n",
    "#         device=device,\n",
    "#         use_big_canvas=False,\n",
    "#         sys_scale=\"bg_pct\",\n",
    "#         sys_pct=99.5,\n",
    "#         clip_pct=99.5,\n",
    "#         mask_roi_for_scale=True,\n",
    "#         show_signed=True,\n",
    "#     )\n",
    "#     plot_reconstruction_vs_input(\n",
    "#         image_test_data=image_test_data,\n",
    "#         reconstructed_fields=all_image_data_pred,\n",
    "#         sample_idx=s,\n",
    "#         model_idx=0,\n",
    "#         save_path=f\"results/plots/Reconstruction_vs_Input_{s}.png\",\n",
    "#     )\n",
    "\n",
    "# #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3990f5d1-6737-499a-8204-ffd3b30da1a1",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m由于上一个单元格中出现错误，单元格已取消。"
     ]
    }
   ],
   "source": [
    "temp_dataset = test_dataset\n",
    "FIXED_E_INDEX = 4\n",
    "\n",
    "def get_fixed_input(dataset, idx, device):\n",
    "    if isinstance(dataset, list):\n",
    "        sample = dataset[idx][0]\n",
    "    else:\n",
    "        sample = dataset.tensors[0][idx]\n",
    "    return sample.squeeze(0).to(device)\n",
    "\n",
    "\n",
    "assert len(temp_dataset) > 0, \"test_dataset 为空\"\n",
    "temp_E = get_fixed_input(temp_dataset, FIXED_E_INDEX % len(temp_dataset), device)\n",
    "\n",
    "z_start = 0.0\n",
    "z_step = 5e-6\n",
    "z_prop_plus = z_prop\n",
    "\n",
    "save_root = Path(\"results_MD\")\n",
    "save_root.mkdir(parents=True, exist_ok=True)\n",
    "run_stamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "filename_prefix = f\"ODNN_vis_{run_stamp}\"\n",
    "\n",
    "for i_model, phase_masks in enumerate(all_phase_masks, start=1):\n",
    "    model_dir = save_root / f\"m{i_model}\"\n",
    "    scans, camera_field = visualize_model_slices(\n",
    "        D2NN,\n",
    "        phase_masks,\n",
    "        temp_E,\n",
    "        output_dir=model_dir,\n",
    "        sample_tag=f\"m{i_model}\",\n",
    "        z_input_to_first=z_input_to_first,\n",
    "        z_layers=z_layers,\n",
    "        z_prop_plus=z_prop_plus,\n",
    "        z_step=z_step,\n",
    "        pixel_size=pixel_size,\n",
    "        wavelength=wavelength,\n",
    "    )\n",
    "\n",
    "    phase_stack = np.stack([np.asarray(mask, dtype=np.float32) for mask in phase_masks], axis=0)\n",
    "    meta = {\n",
    "        \"z_start\": float(z_start),\n",
    "        \"z_step\": float(z_step),\n",
    "        \"z_layers\": float(z_layers),\n",
    "        \"z_prop\": float(z_prop),\n",
    "        \"z_prop_plus\": float(z_prop_plus),\n",
    "        \"pixel_size\": float(pixel_size),\n",
    "        \"wavelength\": float(wavelength),\n",
    "        \"layer_size\": int(layer_size),\n",
    "        \"padding_ratio\": 0.5,\n",
    "    }\n",
    "\n",
    "    mat_path = model_dir / f\"{filename_prefix}_LIGHT_m{i_model}.mat\"\n",
    "    save_to_mat_light_plus(\n",
    "        mat_path,\n",
    "        phase_stack=phase_stack,\n",
    "        input_field=temp_E.detach().cpu().numpy(),\n",
    "        scans=scans,\n",
    "        camera_field=camera_field,\n",
    "        sample_stacks_kmax=20,\n",
    "        save_amplitude_only=False,\n",
    "        meta=meta,\n",
    "    )\n",
    "    print(\"Saved ->\", mat_path)\n",
    "\n",
    "    save_masks_one_file_per_layer(\n",
    "        phase_masks,\n",
    "        out_dir=model_dir,\n",
    "        base_name=f\"{filename_prefix}_MASK\",\n",
    "        save_degree=False,\n",
    "        use_xlsx=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18301d14-1dbf-4e9b-bbb6-e105061c9a5d",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m由于上一个单元格中出现错误，单元格已取消。"
     ]
    }
   ],
   "source": [
    "if pred_case == 1 and run_superposition_debug:\n",
    "    super_dir = Path(\"results_superposition\")\n",
    "    super_dir.mkdir(parents=True, exist_ok=True)\n",
    "    super_tag = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    super_records: list[dict[str, str | int]] = []\n",
    "    slice_reference_input: torch.Tensor | None = None\n",
    "\n",
    "    for sample_idx in range(num_superposition_visual_samples):\n",
    "        super_sample = generate_superposition_sample(\n",
    "            num_modes=num_modes,\n",
    "            field_size=field_size,\n",
    "            layer_size=layer_size,\n",
    "            mmf_modes=MMF_data_ts,\n",
    "            mmf_label_data=MMF_Label_data,\n",
    "        )\n",
    "        super_output_map = infer_superposition_output(\n",
    "            D2NN,\n",
    "            super_sample[\"padded_image\"],\n",
    "            device,\n",
    "        )\n",
    "\n",
    "        sample_tag = f\"{super_tag}_s{sample_idx:02d}\"\n",
    "        triptych_paths = save_superposition_triptych(\n",
    "            input_field=super_sample[\"padded_image\"][0],\n",
    "            output_intensity_map=super_output_map,\n",
    "            amplitudes=super_sample[\"amplitudes\"],\n",
    "            phases=super_sample[\"phases\"],\n",
    "            complex_weights=super_sample[\"complex_weights\"],\n",
    "            label_map=super_sample[\"padded_label\"][0],\n",
    "            evaluation_regions=evaluation_regions,\n",
    "            detect_radius=detectsize,\n",
    "            output_dir=super_dir,\n",
    "            tag=sample_tag,\n",
    "            save_plot=save_superposition_plots,\n",
    "        )\n",
    "        if triptych_paths[\"fig_path\"]:\n",
    "            print(\n",
    "                f\"Superposition sample {sample_idx + 1}/{num_superposition_visual_samples} -> \"\n",
    "                f\"{triptych_paths['fig_path']}\"\n",
    "            )\n",
    "        print(f\"  MAT saved -> {triptych_paths['mat_path']}\")\n",
    "\n",
    "        super_records.append(\n",
    "            {\n",
    "                \"index\": sample_idx,\n",
    "                \"tag\": sample_tag,\n",
    "                \"fig\": triptych_paths[\"fig_path\"] if triptych_paths else \"\",\n",
    "                \"mat\": triptych_paths[\"mat_path\"] if triptych_paths else \"\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "        if slice_reference_input is None:\n",
    "            slice_reference_input = (\n",
    "                super_sample[\"padded_image\"].squeeze(0).to(device, dtype=torch.complex64)\n",
    "            )\n",
    "\n",
    "    if save_superposition_slices and all_phase_masks and slice_reference_input is not None:\n",
    "        slices_root = super_dir / f\"slices_{super_tag}\"\n",
    "        export_superposition_slices(\n",
    "            D2NN,\n",
    "            all_phase_masks,\n",
    "            slice_reference_input,\n",
    "            slices_root,\n",
    "            sample_tag=\"superposition\",\n",
    "            z_input_to_first=z_input_to_first,\n",
    "            z_layers=z_layers,\n",
    "            z_prop=z_prop,\n",
    "            z_step=z_step,\n",
    "            pixel_size=pixel_size,\n",
    "            wavelength=wavelength,\n",
    "        )\n",
    "\n",
    "    if super_records:\n",
    "        print(\"\\nSuperposition sample outputs:\")\n",
    "        for record in super_records:\n",
    "            print(\n",
    "                f\" - Sample {record['index'] + 1:02d} ({record['tag']}): \"\n",
    "                f\"fig={record['fig']}, mat={record['mat']}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "已重启 odnn_venv (Python 3.13.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d541884-b3e0-4936-bca1-56c636d987a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device: cuda:5\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from scipy.io import savemat\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from ODNN_functions import (\n",
    "    create_evaluation_regions,\n",
    "    generate_complex_weights,\n",
    "    generate_fields_ts,\n",
    ")\n",
    "from odnn_generate_label import (\n",
    "    compute_label_centers,\n",
    "    compose_labels_from_patterns,\n",
    "    generate_detector_patterns,\n",
    ")\n",
    "from odnn_io import load_complex_modes_from_mat\n",
    "from odnn_model import D2NNModel\n",
    "from odnn_processing import prepare_sample\n",
    "from odnn_training_eval import (\n",
    "    build_superposition_eval_context,\n",
    "    compute_model_prediction_metrics,\n",
    "    evaluate_spot_metrics,\n",
    "    format_metric_report,\n",
    "    generate_superposition_sample,\n",
    "    infer_superposition_output,\n",
    ")\n",
    "from odnn_training_io import save_masks_one_file_per_layer, save_to_mat_light_plus\n",
    "from odnn_training_visualization import (\n",
    "    capture_eigenmode_propagation,\n",
    "    export_superposition_slices,\n",
    "    plot_amplitude_comparison_grid,\n",
    "    plot_reconstruction_vs_input,\n",
    "    plot_sys_vs_label_strict,\n",
    "    save_superposition_triptych,\n",
    "    save_mode_triptych,\n",
    "    visualize_model_slices,\n",
    ")\n",
    "\n",
    "SEED = 424242\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# 让 cuDNN/算子走确定性分支\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.use_deterministic_algorithms(True)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:5')           # 或者 'cuda:0'\n",
    "    print('Using Device:', device)\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('Using Device: CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fb3fbf-c39c-4150-a93b-4c9b5f8e10ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "field_size = 25  #the field size in eigenmodes_OM4 is 50 pixels\n",
    "layer_size = 100 #400#300#100\n",
    "num_data = 1000 # options: 1. random datas 2.eigenmodes\n",
    "num_modes = 6 #the mode number of MMF 3 6 10\n",
    "circle_focus_radius = 5 # radius when using uniform circular detectors\n",
    "circle_detectsize = 10  # square window size for circular detectors\n",
    "eigenmode_focus_radius = 12.5  # radius when using eigenmode patterns\n",
    "eigenmode_detectsize = 27    # square window size for eigenmode patterns\n",
    "focus_radius = circle_focus_radius\n",
    "detectsize = circle_detectsize\n",
    "batch_size = 16\n",
    "\n",
    "# Evaluation selection: \"eigenmode\" uses the base modes, \"superposition\" samples random mixtures\n",
    "evaluation_mode = \"superposition\"  # options: \"eigenmode\", \"superposition\"\n",
    "num_superposition_eval_samples = 1000\n",
    "num_superposition_visual_samples = 20\n",
    "run_superposition_debug = True\n",
    "save_superposition_plots = True\n",
    "save_superposition_slices = True\n",
    "run_misalignment_robustness = True\n",
    "label_pattern_mode = \"circle\"  # options: \"eigenmode\", \"circle\"\n",
    "# Define multiple D2NN models \n",
    "num_layer_option = [3]   #, 3]#, 4]  # Define the different layer-number ODNN\n",
    "all_losses = [] #the loss for each epoch of each ODNN model\n",
    "all_phase_masks = [] #the phase masks field of each ODNN model\n",
    "all_predictions = [] #the output light field of each ODNN model\n",
    "model_metrics: list[dict] = []\n",
    "all_amplitudes_diff: list[np.ndarray] = []\n",
    "all_average_amplitudes_diff: list[float] = []\n",
    "all_amplitudes_relative_diff: list[float] = []\n",
    "all_complex_weights_pred: list[np.ndarray] = []\n",
    "all_image_data_pred: list[np.ndarray] = []\n",
    "all_cc_real: list[np.ndarray] = []\n",
    "all_cc_imag: list[np.ndarray] = []\n",
    "all_cc_recon_amp: list[np.ndarray] = []\n",
    "all_cc_recon_phase: list[np.ndarray] = []\n",
    "all_training_summaries: list[dict] = []\n",
    "# SLM\n",
    "z_layers   = 40e-6        # 原 47.571e-3  -> 40 μm\n",
    "pixel_size = 1e-6\n",
    "z_prop     = 120e-6        # 原 16.74e-2   -> 60 μm plus 40（最后一层到相机）\n",
    "wavelength = 1568e-9      # 原 1568     -> 1550 nm\n",
    "z_input_to_first = 40e-6  # 40 μm # 新增：输入面到第一层的传播距离"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f15a56-9a41-4b4b-b739-b47db93df620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded modes shape: (25, 25, 6) dtype: complex64\n"
     ]
    }
   ],
   "source": [
    "eigenmodes_OM4 = load_complex_modes_from_mat(\n",
    "    'mmf_6modes_25_PD_1.15.mat',\n",
    "    key='modes_field'\n",
    ")\n",
    "# (H, W, M)\n",
    "print(\"Loaded modes shape:\", eigenmodes_OM4.shape, \"dtype:\", eigenmodes_OM4.dtype)\n",
    "\n",
    "# 取前 num_modes 个 → (H, W, M_sel) → (M_sel, H, W)\n",
    "MMF_data = eigenmodes_OM4[:, :, :num_modes].transpose(2, 0, 1)\n",
    "MMF_data_amp_norm = (np.abs(MMF_data) - np.min(np.abs(MMF_data))) / (np.max(np.abs(MMF_data)) - np.min(np.abs(MMF_data)))\n",
    "\n",
    "MMF_data = MMF_data_amp_norm * np.exp(1j * np.angle(MMF_data))\n",
    "\n",
    "#要是以后确定了用4我在想要不要去掉其他选项\n",
    "phase_option = 4\n",
    "#phase_option 1: (0,0,...,0)\n",
    "#phase_option 2: (0,2pi,...,2pi)\n",
    "#phase_option 3: (0,pi,...,2pi)\n",
    "#phase_option 4: eigenmodes\n",
    "#phase_option 5: (0,pi,...,pi)\n",
    "\n",
    "if phase_option in [1, 2, 3, 5]:\n",
    "    amplitudes,phases = generate_complex_weights(num_data,num_modes,phase_option)\n",
    "\n",
    "if phase_option == 4:\n",
    "    num_data = num_modes # use the eigenmodes to train ODNN\n",
    "    amplitudes = np.eye(num_modes)#[[1,0,0][0,1,0][0,0,1]]\n",
    "    phases = np.eye(num_modes)\n",
    "\n",
    "amplitudes_phases_ori = np.hstack((amplitudes[:, :], phases[:, 1:]))  # amplitudes (l2 norm) phases\n",
    "amplitudes_phases = np.hstack((amplitudes[:, :], phases[:, 1:]/(2*np.pi)))  # amplitudes (l2 norm) phases (0-1)\n",
    "\n",
    "# Generate complex weights vector with specified amplitudes and phases\n",
    "complex_weights = amplitudes * np.exp(1j * phases)\n",
    "MMF_data_ts = torch.from_numpy(MMF_data)\n",
    "complex_weights_ts = torch.from_numpy(complex_weights)\n",
    "image_data = generate_fields_ts(complex_weights_ts, MMF_data_ts, num_data, num_modes, field_size).to(torch.complex64)\n",
    "image_test_data = image_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f53e33d-6b5b-400d-b654-e6b850b5e60b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "相邻图案边缘间距： 行=26.67, 列=17.50\n",
      "相邻图案中心间距： 行=36.67, 列=27.50\n",
      "中心坐标： [(32, 22), (32, 50), (32, 78), (68, 22), (68, 50), (68, 78)]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "pred_case = 1: only amplitudes prediction\n",
    "pred_case = 2: only phases prediction\n",
    "pred_case = 3: amplitudes and phases prediction\n",
    "pred_case = 4: amplitudes and phases prediction (extra energy phase area)\n",
    "'''\n",
    "#\n",
    "pred_case = 1\n",
    "label_data = torch.zeros([num_data,1,layer_size,layer_size])\n",
    "label_size = layer_size\n",
    "\n",
    "if pred_case == 1: # 3\n",
    "    num_detector = num_modes\n",
    "    detector_focus_radius = focus_radius\n",
    "    detector_detectsize = detectsize\n",
    "    if label_pattern_mode == \"eigenmode\":\n",
    "        pattern_stack = np.transpose(np.abs(MMF_data), (1, 2, 0))\n",
    "        pattern_h, pattern_w, _ = pattern_stack.shape\n",
    "        if pattern_h > label_size or pattern_w > label_size:\n",
    "            raise ValueError(\n",
    "                f\"Eigenmode pattern size ({pattern_h}x{pattern_w}) exceeds label canvas {label_size}.\"\n",
    "            )\n",
    "        layout_radius = math.ceil(max(pattern_h, pattern_w) / 2)\n",
    "        detector_focus_radius = eigenmode_focus_radius\n",
    "        detector_detectsize = eigenmode_detectsize\n",
    "    elif label_pattern_mode == \"circle\":\n",
    "        circle_radius = circle_focus_radius\n",
    "        pattern_size = circle_radius * 2\n",
    "        if pattern_size % 2 == 0:\n",
    "            pattern_size += 1  \n",
    "        pattern_stack = generate_detector_patterns(pattern_size, pattern_size, num_detector, shape=\"circle\")\n",
    "        layout_radius = circle_radius\n",
    "        detector_focus_radius = circle_radius\n",
    "        detector_detectsize = circle_detectsize\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown label_pattern_mode: {label_pattern_mode}\")\n",
    "\n",
    "    centers, _, _ = compute_label_centers(label_size, label_size, num_detector, layout_radius)\n",
    "    mode_label_maps = [\n",
    "        compose_labels_from_patterns(\n",
    "            label_size,\n",
    "            label_size,\n",
    "            pattern_stack,\n",
    "            centers,\n",
    "            Index=i + 1,\n",
    "            visualize=False,\n",
    "        )\n",
    "        for i in range(num_detector)\n",
    "    ]\n",
    "    MMF_Label_data = torch.from_numpy(\n",
    "        np.stack(mode_label_maps, axis=2).astype(np.float32)\n",
    "    )\n",
    "    amplitude_weights = torch.from_numpy(amplitudes_phases[:num_data, 0:num_modes]).float()\n",
    "    combined_labels = (\n",
    "        amplitude_weights[:, None, None, :] * MMF_Label_data.unsqueeze(0)\n",
    "    ).sum(dim=3)\n",
    "    label_data[:, 0, :, :] = combined_labels\n",
    "    focus_radius = detector_focus_radius\n",
    "    detectsize = detector_detectsize\n",
    "\n",
    "label_test_data = label_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf5154c-c402-4ce0-aac4-18a9c5d33539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detection Regions: [(17, 27, 27, 37), (45, 55, 27, 37), (73, 83, 27, 37), (17, 27, 63, 73), (45, 55, 63, 73), (73, 83, 63, 73)]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = [\n",
    "    prepare_sample(image_data[i], label_data[i], layer_size) for i in range(len(label_data))\n",
    "]\n",
    "train_tensor_data = TensorDataset(*[torch.stack(tensors) for tensors in zip(*train_dataset)])\n",
    "g = torch.Generator()\n",
    "g.manual_seed(SEED)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_tensor_data,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,               # 顺序会被 g 固定\n",
    "    generator=g,                # 固定打乱\n",
    "   \n",
    ")\n",
    "\n",
    "superposition_eval_ctx: dict | None = None\n",
    "if evaluation_mode == \"eigenmode\":\n",
    "    test_dataset = train_dataset\n",
    "    test_tensor_data = train_tensor_data\n",
    "    test_loader = DataLoader(test_tensor_data, batch_size=batch_size, shuffle=False)\n",
    "    eval_amplitudes = amplitudes\n",
    "    eval_amplitudes_phases = amplitudes_phases\n",
    "    eval_phases = phases\n",
    "    image_test_data = image_data\n",
    "elif evaluation_mode == \"superposition\":\n",
    "    if pred_case != 1:\n",
    "        raise ValueError(\"Superposition evaluation mode currently supports pred_case == 1 only.\")\n",
    "    super_ctx = build_superposition_eval_context(\n",
    "        num_superposition_eval_samples,\n",
    "        num_modes=num_modes,\n",
    "        field_size=field_size,\n",
    "        layer_size=layer_size,\n",
    "        mmf_modes=MMF_data_ts,\n",
    "        mmf_label_data=MMF_Label_data,\n",
    "        batch_size=batch_size,\n",
    "        second_mode_half_range=True,\n",
    "    )\n",
    "    test_dataset = super_ctx[\"dataset\"]\n",
    "    test_tensor_data = super_ctx[\"tensor_dataset\"]\n",
    "    test_loader = super_ctx[\"loader\"]\n",
    "    image_test_data = super_ctx[\"image_data\"]\n",
    "    eval_amplitudes = super_ctx[\"amplitudes\"]\n",
    "    eval_amplitudes_phases = super_ctx[\"amplitudes_phases\"]\n",
    "    eval_phases = super_ctx[\"phases\"]\n",
    "    superposition_eval_ctx = super_ctx\n",
    "else:\n",
    "    raise ValueError(f\"Unknown evaluation_mode: {evaluation_mode}\")\n",
    "\n",
    "# Generate detection regions using existing function\n",
    "if pred_case ==1:\n",
    "    evaluation_regions = create_evaluation_regions(layer_size, layer_size, num_detector, focus_radius, detectsize)\n",
    "    print(\"Detection Regions:\", evaluation_regions)\n",
    "\n",
    "\n",
    "def shift_complex_batch(batch: torch.Tensor, shift_y: int, shift_x: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Translate a batch of complex fields by (shift_y, shift_x) pixels with zero padding.\n",
    "    Positive shift_y moves downward; positive shift_x moves right.\n",
    "    \"\"\"\n",
    "    if shift_y == 0 and shift_x == 0:\n",
    "        return batch\n",
    "\n",
    "    _, _, height, width = batch.shape\n",
    "    if abs(shift_y) >= height or abs(shift_x) >= width:\n",
    "        return torch.zeros_like(batch)\n",
    "\n",
    "    real_imag = torch.view_as_real(batch)\n",
    "    shifted = torch.zeros_like(real_imag)\n",
    "\n",
    "    if shift_y >= 0:\n",
    "        src_y = slice(0, height - shift_y)\n",
    "        dst_y = slice(shift_y, height)\n",
    "    else:\n",
    "        src_y = slice(-shift_y, height)\n",
    "        dst_y = slice(0, height + shift_y)\n",
    "\n",
    "    if shift_x >= 0:\n",
    "        src_x = slice(0, width - shift_x)\n",
    "        dst_x = slice(shift_x, width)\n",
    "    else:\n",
    "        src_x = slice(-shift_x, width)\n",
    "        dst_x = slice(0, width + shift_x)\n",
    "\n",
    "    shifted[:, :, dst_y, dst_x, :] = real_imag[:, :, src_y, src_x, :]\n",
    "    return torch.view_as_complex(shifted)\n",
    "\n",
    "\n",
    "def compute_amp_relative_error_with_shift(\n",
    "    model: torch.nn.Module,\n",
    "    loader,\n",
    "    *,\n",
    "    shift_y_px: int,\n",
    "    shift_x_px: int,\n",
    "    evaluation_regions,\n",
    "    pred_case: int,\n",
    "    num_modes: int,\n",
    "    eval_amplitudes: np.ndarray,\n",
    "    eval_amplitudes_phases: np.ndarray,\n",
    "    eval_phases: np.ndarray,\n",
    "    phase_option: int,\n",
    "    mmf_modes: torch.Tensor,\n",
    "    field_size: int,\n",
    "    image_test_data: torch.Tensor,\n",
    "    device: torch.device,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Evaluate amplitude-related metrics when the input field is shifted by (shift_y_px, shift_x_px).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_weights_pred: list[np.ndarray] = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, _ in loader:\n",
    "            images = images.to(device, dtype=torch.complex64, non_blocking=True)\n",
    "            shifted_images = shift_complex_batch(images, shift_y_px, shift_x_px)\n",
    "            preds = model(shifted_images)\n",
    "            preds_np = preds.detach().cpu().numpy()\n",
    "\n",
    "            for sample_idx in range(preds_np.shape[0]):\n",
    "                intensity_map = preds_np[sample_idx, 0]\n",
    "                weights = []\n",
    "                for (x0, x1, y0, y1) in evaluation_regions:\n",
    "                    weights.append(float(intensity_map[y0:y1, x0:x1].mean()))\n",
    "                weights = np.asarray(weights, dtype=np.float64)\n",
    "\n",
    "                if pred_case == 3 and num_modes <= len(weights):\n",
    "                    norm_val = np.linalg.norm(weights[:num_modes])\n",
    "                    if norm_val > 0:\n",
    "                        weights[:num_modes] /= norm_val\n",
    "                else:\n",
    "                    norm_val = np.linalg.norm(weights)\n",
    "                    if norm_val > 0:\n",
    "                        weights /= norm_val\n",
    "\n",
    "                all_weights_pred.append(weights)\n",
    "\n",
    "    metrics = compute_model_prediction_metrics(\n",
    "        all_weights_pred,\n",
    "        eval_amplitudes,\n",
    "        eval_amplitudes_phases,\n",
    "        eval_phases,\n",
    "        phase_option,\n",
    "        pred_case,\n",
    "        num_modes,\n",
    "        mmf_modes,\n",
    "        field_size,\n",
    "        image_test_data,\n",
    "    )\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80985deb-92dc-4629-9094-f04f9e73ddab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training D2NN with 3 layers...\n",
      "\n",
      "D2NNModel(\n",
      "  (pre_propagation): Propagation()\n",
      "  (layers): ModuleList(\n",
      "    (0-2): 3 x DiffractionLayer()\n",
      "  )\n",
      "  (propagation): Propagation()\n",
      "  (regression): RegressionDetector()\n",
      ")\n",
      "Epoch [1/1000], Loss: 0.008090289309620857, Epoch Time: 0.10 seconds\n",
      "Epoch [100/1000], Loss: 0.002760213334113359, Epoch Time: 0.01 seconds\n",
      "Epoch [200/1000], Loss: 0.002684592502191663, Epoch Time: 0.01 seconds\n",
      "Epoch [300/1000], Loss: 0.002658451907336712, Epoch Time: 0.01 seconds\n",
      "Epoch [400/1000], Loss: 0.002648203633725643, Epoch Time: 0.01 seconds\n",
      "Epoch [500/1000], Loss: 0.002644307911396027, Epoch Time: 0.01 seconds\n",
      "Epoch [600/1000], Loss: 0.002642895793542266, Epoch Time: 0.01 seconds\n",
      "Epoch [700/1000], Loss: 0.002642374718561769, Epoch Time: 0.01 seconds\n",
      "Epoch [800/1000], Loss: 0.002642178907990456, Epoch Time: 0.01 seconds\n",
      "Epoch [900/1000], Loss: 0.002642104867845774, Epoch Time: 0.01 seconds\n",
      "Epoch [1000/1000], Loss: 0.002642077626660466, Epoch Time: 0.01 seconds\n",
      "Total training time for 3-layer model: 10.28 seconds (~0.17 minutes)\n",
      "✔ Saved training loss plot -> results/training_analysis/loss_curve_layers3_20251103_181746.png\n",
      "✔ Saved cumulative time plot -> results/training_analysis/epoch_time_layers3_20251103_181746.png\n",
      "✔ Saved training log data (.mat) -> results/training_analysis/training_curves_layers3_20251103_181746.mat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ydzhang/Desktop/odnn_code/odnn_training_visualization.py:916: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "  plt.tight_layout(rect=[0, 0, 1, 0.97])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Saved eigenmode-3 propagation plot -> results/propagation_slices/propagation_mode3_layers3_20251103_181746.png\n",
      "✔ Saved eigenmode-3 propagation data (.mat) -> results/propagation_slices/propagation_mode3_layers3_20251103_181746.mat\n",
      "✔ Saved model -> checkpoints/odnn_3layers.pth\n",
      "3 layers: modes=6, phase_opt=4, pred_case=1\n",
      "  amp_err=0.085485, amp_err_rel=0.209394\n",
      "  snr_full=0.590977, snr_crop=0.831166, throughput=0.710960\n",
      "  cc_amp=0.963570±0.021786, cc_phase=0.793870±0.137667, cc_real=0.969919±0.025222, cc_imag=0.970586±0.030134\n"
     ]
    }
   ],
   "source": [
    "for num_layer in num_layer_option:\n",
    "    print(f\"\\nTraining D2NN with {num_layer} layers...\\n\")\n",
    "\n",
    "    D2NN = D2NNModel(\n",
    "        num_layers=num_layer,\n",
    "        layer_size=layer_size,\n",
    "        z_layers=z_layers,\n",
    "        z_prop=z_prop,\n",
    "        pixel_size=pixel_size,\n",
    "        wavelength=wavelength,\n",
    "        device=device,\n",
    "        padding_ratio=0.5,\n",
    "        z_input_to_first=z_input_to_first,   # NEW\n",
    "    ).to(device)\n",
    "\n",
    "    print(D2NN)\n",
    "\n",
    "    # Training\n",
    "    criterion = nn.MSELoss()  # Define loss function (对比的是loss)\n",
    "    optimizer = optim.Adam(D2NN.parameters(), lr=1.99) \n",
    "    scheduler = ExponentialLR(optimizer, gamma=0.99)  \n",
    "    epochs = 1000\n",
    "    losses = []\n",
    "    epoch_durations: list[float] = []\n",
    "    training_start_time = time.time()\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        D2NN.train()\n",
    "        epoch_loss = 0\n",
    "        for images, labels in train_loader:\n",
    "            images = images.to(device, dtype=torch.complex64, non_blocking=True)\n",
    "            labels = labels.to(device, dtype=torch.float32,   non_blocking=True)\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            outputs = D2NN(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        scheduler.step()\n",
    "        avg_loss = epoch_loss / len(train_loader)  # Calculate average loss for the epoch\n",
    "        losses.append(avg_loss)  # the loss for each model\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.synchronize(device)\n",
    "        epoch_duration = time.time() - epoch_start_time\n",
    "        epoch_durations.append(epoch_duration)\n",
    "\n",
    "        if epoch % 100 == 0 or epoch == 1 or epoch == epochs:\n",
    "            print(\n",
    "                f'Epoch [{epoch}/{epochs}], Loss: {avg_loss:.18f}, '\n",
    "                f'Epoch Time: {epoch_duration:.2f} seconds'\n",
    "            )\n",
    "\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.synchronize(device)\n",
    "    total_training_time = time.time() - training_start_time\n",
    "    print(\n",
    "        f'Total training time for {num_layer}-layer model: {total_training_time:.2f} seconds '\n",
    "        f'(~{total_training_time / 60:.2f} minutes)'\n",
    "    )\n",
    "    all_losses.append(losses)  # save the loss for each model\n",
    "    training_output_dir = Path(\"results/training_analysis\")\n",
    "    training_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    epochs_array = np.arange(1, epochs + 1, dtype=np.int32)\n",
    "    cumulative_epoch_times = np.cumsum(epoch_durations)\n",
    "    timestamp_tag = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(epochs_array, losses, label=\"Training Loss\")\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.set_title(f\"D2NN Training Loss ({num_layer} layers)\")\n",
    "    ax.grid(True, which=\"both\", linestyle=\"--\", linewidth=0.5)\n",
    "    ax.legend()\n",
    "    loss_plot_path = training_output_dir / f\"loss_curve_layers{num_layer}_{timestamp_tag}.png\"\n",
    "    fig.savefig(loss_plot_path, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "    fig_time, ax_time = plt.subplots()\n",
    "    ax_time.plot(epochs_array, cumulative_epoch_times, label=\"Cumulative Time\")\n",
    "    ax_time.set_xlabel(\"Epoch\")\n",
    "    ax_time.set_ylabel(\"Time (seconds)\")\n",
    "    ax_time.set_title(f\"Cumulative Training Time ({num_layer} layers)\")\n",
    "    ax_time.grid(True, which=\"both\", linestyle=\"--\", linewidth=0.5)\n",
    "    ax_time.legend()\n",
    "    time_plot_path = training_output_dir / f\"epoch_time_layers{num_layer}_{timestamp_tag}.png\"\n",
    "    fig_time.savefig(time_plot_path, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close(fig_time)\n",
    "\n",
    "    mat_path = training_output_dir / f\"training_curves_layers{num_layer}_{timestamp_tag}.mat\"\n",
    "    savemat(\n",
    "        str(mat_path),\n",
    "        {\n",
    "            \"epochs\": epochs_array,\n",
    "            \"losses\": np.array(losses, dtype=np.float64),\n",
    "            \"epoch_durations\": np.array(epoch_durations, dtype=np.float64),\n",
    "            \"cumulative_epoch_times\": np.array(cumulative_epoch_times, dtype=np.float64),\n",
    "            \"total_training_time\": np.array([total_training_time], dtype=np.float64),\n",
    "            \"num_layers\": np.array([num_layer], dtype=np.int32),\n",
    "        },\n",
    "    )\n",
    "\n",
    "    print(f\"✔ Saved training loss plot -> {loss_plot_path}\")\n",
    "    print(f\"✔ Saved cumulative time plot -> {time_plot_path}\")\n",
    "    print(f\"✔ Saved training log data (.mat) -> {mat_path}\")\n",
    "\n",
    "    propagation_dir = Path(\"results/propagation_slices\")\n",
    "    eigenmode_index = min(2, MMF_data_ts.shape[0] - 1)\n",
    "    propagation_summary = capture_eigenmode_propagation(\n",
    "        model=D2NN,\n",
    "        eigenmode_field=MMF_data_ts[eigenmode_index],\n",
    "        mode_index=eigenmode_index,\n",
    "        layer_size=layer_size,\n",
    "        z_input_to_first=z_input_to_first,\n",
    "        z_layers=z_layers,\n",
    "        z_prop=z_prop,\n",
    "        pixel_size=pixel_size,\n",
    "        wavelength=wavelength,\n",
    "        output_dir=propagation_dir,\n",
    "        tag=f\"layers{num_layer}_{timestamp_tag}\",\n",
    "    )\n",
    "    print(f\"✔ Saved eigenmode-{eigenmode_index + 1} propagation plot -> {propagation_summary['fig_path']}\")\n",
    "    print(f\"✔ Saved eigenmode-{eigenmode_index + 1} propagation data (.mat) -> {propagation_summary['mat_path']}\")\n",
    "\n",
    "    mode_triptych_records: list[dict[str, str | int]] = []\n",
    "    if evaluation_mode == \"eigenmode\":\n",
    "        triptych_dir = Path(\"results/mode_triptychs\")\n",
    "        mode_tag = f\"layers{num_layer}_{timestamp_tag}\"\n",
    "        for mode_idx in range(min(num_modes, len(MMF_data_ts))):\n",
    "            label_tensor = label_data[mode_idx, 0]\n",
    "            record = save_mode_triptych(\n",
    "                model=D2NN,\n",
    "                mode_index=mode_idx,\n",
    "                eigenmode_field=MMF_data_ts[mode_idx],\n",
    "                label_field=label_tensor,\n",
    "                layer_size=layer_size,\n",
    "                output_dir=triptych_dir,\n",
    "                tag=mode_tag,\n",
    "            )\n",
    "            mode_triptych_records.append(\n",
    "                {\n",
    "                    \"mode\": mode_idx + 1,\n",
    "                    \"fig\": record[\"fig_path\"],\n",
    "                    \"mat\": record[\"mat_path\"],\n",
    "                }\n",
    "            )\n",
    "            print(\n",
    "                f\"✔ Saved mode {mode_idx + 1} triptych -> {record['fig_path']}\\n\"\n",
    "                f\"  MAT -> {record['mat_path']}\"\n",
    "            )\n",
    "\n",
    "    all_training_summaries.append(\n",
    "        {\n",
    "            \"num_layers\": num_layer,\n",
    "            \"total_time\": total_training_time,\n",
    "            \"loss_plot\": str(loss_plot_path),\n",
    "            \"time_plot\": str(time_plot_path),\n",
    "            \"mat_path\": str(mat_path),\n",
    "            \"propagation_fig\": propagation_summary[\"fig_path\"],\n",
    "            \"propagation_mat\": propagation_summary[\"mat_path\"],\n",
    "            \"mode_triptychs\": mode_triptych_records,\n",
    "        }\n",
    "    )\n",
    "   \n",
    "    # === after training ===\n",
    "    ckpt_dir = \"checkpoints\"\n",
    "    os.makedirs(ckpt_dir, exist_ok=True)\n",
    "\n",
    "    ckpt = {\n",
    "        \"state_dict\": D2NN.state_dict(),\n",
    "        \"meta\": {\n",
    "            \"num_layers\":        len(D2NN.layers),\n",
    "            \"layer_size\":        layer_size,\n",
    "            \"z_layers\":          z_layers,\n",
    "            \"z_prop\":            z_prop,\n",
    "            \"pixel_size\":        pixel_size,\n",
    "            \"wavelength\":        wavelength,\n",
    "            \"padding_ratio\":     0.5,         \n",
    "            \"field_size\":        field_size,  \n",
    "            \"num_modes\":         num_modes, \n",
    "            \"z_input_to_first\":  z_input_to_first, \n",
    "        }\n",
    "    }\n",
    "    save_path = os.path.join(ckpt_dir, f\"odnn_{len(D2NN.layers)}layers.pth\")\n",
    "    torch.save(ckpt, save_path)\n",
    "    print(\"✔ Saved model ->\", save_path)\n",
    "    # Free GPU memory\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Cache phase masks for later visualization/export\n",
    "    phase_masks = []\n",
    "    for layer in D2NN.layers:\n",
    "        phase_np = layer.phase.detach().cpu().numpy()\n",
    "        phase_masks.append(np.remainder(phase_np, 2 * np.pi))\n",
    "    all_phase_masks.append(phase_masks)\n",
    "\n",
    "    # Collect evaluation metrics for this model\n",
    "    metrics = evaluate_spot_metrics(\n",
    "        D2NN,\n",
    "        test_loader,\n",
    "        evaluation_regions,\n",
    "        detect_radius=detectsize,\n",
    "        device=device,\n",
    "        pred_case=pred_case,\n",
    "        num_modes=num_modes,\n",
    "        phase_option=phase_option,\n",
    "        amplitudes=eval_amplitudes,\n",
    "        amplitudes_phases=eval_amplitudes_phases,\n",
    "        phases=eval_phases,\n",
    "        mmf_modes=MMF_data_ts,\n",
    "        field_size=field_size,\n",
    "        image_test_data=image_test_data,\n",
    "    )\n",
    "\n",
    "    model_metrics.append(metrics)\n",
    "    all_amplitudes_diff.append(metrics.get(\"amplitudes_diff\", np.array([])))\n",
    "    all_average_amplitudes_diff.append(float(metrics.get(\"avg_amplitudes_diff\", float(\"nan\"))))\n",
    "    all_amplitudes_relative_diff.append(float(metrics.get(\"avg_relative_amp_err\", float(\"nan\"))))\n",
    "    all_complex_weights_pred.append(metrics.get(\"complex_weights_pred\", np.array([])))\n",
    "    all_image_data_pred.append(metrics.get(\"image_data_pred\", np.array([])))\n",
    "    all_cc_recon_amp.append(metrics.get(\"cc_recon_amp\", np.array([])))\n",
    "    all_cc_recon_phase.append(metrics.get(\"cc_recon_phase\", np.array([])))\n",
    "    all_cc_real.append(metrics.get(\"cc_real\", np.array([])))\n",
    "    all_cc_imag.append(metrics.get(\"cc_imag\", np.array([])))\n",
    "\n",
    "    print(\n",
    "        format_metric_report(\n",
    "            num_modes=num_modes,\n",
    "            phase_option=phase_option,\n",
    "            pred_case=pred_case,\n",
    "            label=f\"{num_layer} layers\",\n",
    "            metrics=metrics,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dca16ea-c960-4d6a-bcbb-e84800c048e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training duration summary:\n",
      " - 3 layers: 10.28 s (~0.17 min)\n",
      "   Loss curve: results/training_analysis/loss_curve_layers3_20251103_181746.png\n",
      "   Time curve: results/training_analysis/epoch_time_layers3_20251103_181746.png\n",
      "   Data (.mat): results/training_analysis/training_curves_layers3_20251103_181746.mat\n",
      "   Propagation plot: results/propagation_slices/propagation_mode3_layers3_20251103_181746.png\n",
      "   Propagation data (.mat): results/propagation_slices/propagation_mode3_layers3_20251103_181746.mat\n",
      "✔ Saved: /home/ydzhang/Desktop/odnn_code/results/plots/Amp_3layers.png\n"
     ]
    }
   ],
   "source": [
    "if all_training_summaries:\n",
    "    print(\"\\nTraining duration summary:\")\n",
    "    for summary in all_training_summaries:\n",
    "        minutes = summary[\"total_time\"] / 60\n",
    "        print(\n",
    "            f\" - {summary['num_layers']} layers: {summary['total_time']:.2f} s \"\n",
    "            f\"(~{minutes:.2f} min)\"\n",
    "        )\n",
    "        print(f\"   Loss curve: {summary['loss_plot']}\")\n",
    "        print(f\"   Time curve: {summary['time_plot']}\")\n",
    "        print(f\"   Data (.mat): {summary['mat_path']}\")\n",
    "        print(f\"   Propagation plot: {summary['propagation_fig']}\")\n",
    "        print(f\"   Propagation data (.mat): {summary['propagation_mat']}\")\n",
    "        mode_triptychs = summary.get(\"mode_triptychs\", [])\n",
    "        if mode_triptychs:\n",
    "            print(\"   Mode triptychs:\")\n",
    "            for trip in mode_triptychs:\n",
    "                print(\n",
    "                    f\"     Mode {trip['mode']}: fig={trip['fig']}, mat={trip['mat']}\"\n",
    "                )\n",
    "\n",
    "save_dir = \"results/plots\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "num_samples_to_display = 6\n",
    "for idx, num_layer in enumerate(num_layer_option):\n",
    "    plot_amplitude_comparison_grid(\n",
    "        image_test_data,\n",
    "        all_image_data_pred[idx],\n",
    "        all_cc_recon_amp[idx],\n",
    "        max_samples=num_samples_to_display,\n",
    "        save_path=os.path.join(save_dir, f\"Amp_{num_layer}layers.png\"),\n",
    "        title=f\"Amp. distribution of Real and Predicted Images({num_layer}_layer_ODNN)\",\n",
    "    )\n",
    "\n",
    "# #直观的看看输出和label的差异\n",
    "# for s in [0, 1, 2, 5]:\n",
    "#     plot_sys_vs_label_strict(\n",
    "#         D2NN,\n",
    "#         test_dataset,\n",
    "#         sample_idx=s,\n",
    "#         evaluation_regions=evaluation_regions,\n",
    "#         detect_radius=detectsize,\n",
    "#         save_path=f\"results/plots/IO_Pred_Label_RAW_{s}.png\",\n",
    "#         device=device,\n",
    "#         use_big_canvas=False,\n",
    "#         sys_scale=\"bg_pct\",\n",
    "#         sys_pct=99.5,\n",
    "#         clip_pct=99.5,\n",
    "#         mask_roi_for_scale=True,\n",
    "#         show_signed=True,\n",
    "#     )\n",
    "#     plot_reconstruction_vs_input(\n",
    "#         image_test_data=image_test_data,\n",
    "#         reconstructed_fields=all_image_data_pred,\n",
    "#         sample_idx=s,\n",
    "#         model_idx=0,\n",
    "#         save_path=f\"results/plots/Reconstruction_vs_Input_{s}.png\",\n",
    "#     )\n",
    "\n",
    "# #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd56b78-79f7-45d4-b124-e4bc0779c557",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ydzhang/Desktop/odnn_code/odnn_training_visualization.py:534: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "  plt.tight_layout()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved figure -> /home/ydzhang/Desktop/odnn_code/results_MD/m1/m1_scan_input.png\n",
      "Saved figure -> /home/ydzhang/Desktop/odnn_code/results_MD/m1/m1_scan_layer1.png\n",
      "Saved figure -> /home/ydzhang/Desktop/odnn_code/results_MD/m1/m1_scan_layer2.png\n",
      "Saved figure -> /home/ydzhang/Desktop/odnn_code/results_MD/m1/m1_scan_layer3.png\n",
      "Saved figure -> /home/ydzhang/Desktop/odnn_code/results_MD/m1/m1_scan_to_camera.png\n",
      "Saved (v5 plus): results_MD/m1/ODNN_vis_20251103_181752_LIGHT_m1.mat\n",
      "Saved -> results_MD/m1/ODNN_vis_20251103_181752_LIGHT_m1.mat\n"
     ]
    }
   ],
   "source": [
    "temp_dataset = test_dataset\n",
    "FIXED_E_INDEX = 4\n",
    "\n",
    "def get_fixed_input(dataset, idx, device):\n",
    "    if isinstance(dataset, list):\n",
    "        sample = dataset[idx][0]\n",
    "    else:\n",
    "        sample = dataset.tensors[0][idx]\n",
    "    return sample.squeeze(0).to(device)\n",
    "\n",
    "\n",
    "assert len(temp_dataset) > 0, \"test_dataset 为空\"\n",
    "temp_E = get_fixed_input(temp_dataset, FIXED_E_INDEX % len(temp_dataset), device)\n",
    "\n",
    "z_start = 0.0\n",
    "z_step = 5e-6\n",
    "z_prop_plus = z_prop\n",
    "\n",
    "save_root = Path(\"results_MD\")\n",
    "save_root.mkdir(parents=True, exist_ok=True)\n",
    "run_stamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "filename_prefix = f\"ODNN_vis_{run_stamp}\"\n",
    "\n",
    "for i_model, phase_masks in enumerate(all_phase_masks, start=1):\n",
    "    model_dir = save_root / f\"m{i_model}\"\n",
    "    scans, camera_field = visualize_model_slices(\n",
    "        D2NN,\n",
    "        phase_masks,\n",
    "        temp_E,\n",
    "        output_dir=model_dir,\n",
    "        sample_tag=f\"m{i_model}\",\n",
    "        z_input_to_first=z_input_to_first,\n",
    "        z_layers=z_layers,\n",
    "        z_prop_plus=z_prop_plus,\n",
    "        z_step=z_step,\n",
    "        pixel_size=pixel_size,\n",
    "        wavelength=wavelength,\n",
    "    )\n",
    "\n",
    "    phase_stack = np.stack([np.asarray(mask, dtype=np.float32) for mask in phase_masks], axis=0)\n",
    "    meta = {\n",
    "        \"z_start\": float(z_start),\n",
    "        \"z_step\": float(z_step),\n",
    "        \"z_layers\": float(z_layers),\n",
    "        \"z_prop\": float(z_prop),\n",
    "        \"z_prop_plus\": float(z_prop_plus),\n",
    "        \"pixel_size\": float(pixel_size),\n",
    "        \"wavelength\": float(wavelength),\n",
    "        \"layer_size\": int(layer_size),\n",
    "        \"padding_ratio\": 0.5,\n",
    "    }\n",
    "\n",
    "    mat_path = model_dir / f\"{filename_prefix}_LIGHT_m{i_model}.mat\"\n",
    "    save_to_mat_light_plus(\n",
    "        mat_path,\n",
    "        phase_stack=phase_stack,\n",
    "        input_field=temp_E.detach().cpu().numpy(),\n",
    "        scans=scans,\n",
    "        camera_field=camera_field,\n",
    "        sample_stacks_kmax=20,\n",
    "        save_amplitude_only=False,\n",
    "        meta=meta,\n",
    "    )\n",
    "    print(\"Saved ->\", mat_path)\n",
    "\n",
    "    save_masks_one_file_per_layer(\n",
    "        phase_masks,\n",
    "        out_dir=model_dir,\n",
    "        base_name=f\"{filename_prefix}_MASK\",\n",
    "        save_degree=False,\n",
    "        use_xlsx=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37dc51bc-205c-4679-bb3a-fb342d3e23db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Superposition sample 1/20 -> results_superposition/super_triptych_20251103_181758_s00.png\n",
      "  MAT saved -> results_superposition/super_triptych_20251103_181758_s00.mat\n",
      "Superposition sample 2/20 -> results_superposition/super_triptych_20251103_181758_s01.png\n",
      "  MAT saved -> results_superposition/super_triptych_20251103_181758_s01.mat\n",
      "Superposition sample 3/20 -> results_superposition/super_triptych_20251103_181758_s02.png\n",
      "  MAT saved -> results_superposition/super_triptych_20251103_181758_s02.mat\n",
      "Superposition sample 4/20 -> results_superposition/super_triptych_20251103_181758_s03.png\n",
      "  MAT saved -> results_superposition/super_triptych_20251103_181758_s03.mat\n",
      "Superposition sample 5/20 -> results_superposition/super_triptych_20251103_181758_s04.png\n",
      "  MAT saved -> results_superposition/super_triptych_20251103_181758_s04.mat\n",
      "Superposition sample 6/20 -> results_superposition/super_triptych_20251103_181758_s05.png\n",
      "  MAT saved -> results_superposition/super_triptych_20251103_181758_s05.mat\n",
      "Superposition sample 7/20 -> results_superposition/super_triptych_20251103_181758_s06.png\n",
      "  MAT saved -> results_superposition/super_triptych_20251103_181758_s06.mat\n",
      "Superposition sample 8/20 -> results_superposition/super_triptych_20251103_181758_s07.png\n",
      "  MAT saved -> results_superposition/super_triptych_20251103_181758_s07.mat\n",
      "Superposition sample 9/20 -> results_superposition/super_triptych_20251103_181758_s08.png\n",
      "  MAT saved -> results_superposition/super_triptych_20251103_181758_s08.mat\n",
      "Superposition sample 10/20 -> results_superposition/super_triptych_20251103_181758_s09.png\n",
      "  MAT saved -> results_superposition/super_triptych_20251103_181758_s09.mat\n",
      "Superposition sample 11/20 -> results_superposition/super_triptych_20251103_181758_s10.png\n",
      "  MAT saved -> results_superposition/super_triptych_20251103_181758_s10.mat\n",
      "Superposition sample 12/20 -> results_superposition/super_triptych_20251103_181758_s11.png\n",
      "  MAT saved -> results_superposition/super_triptych_20251103_181758_s11.mat\n",
      "Superposition sample 13/20 -> results_superposition/super_triptych_20251103_181758_s12.png\n",
      "  MAT saved -> results_superposition/super_triptych_20251103_181758_s12.mat\n",
      "Superposition sample 14/20 -> results_superposition/super_triptych_20251103_181758_s13.png\n",
      "  MAT saved -> results_superposition/super_triptych_20251103_181758_s13.mat\n",
      "Superposition sample 15/20 -> results_superposition/super_triptych_20251103_181758_s14.png\n",
      "  MAT saved -> results_superposition/super_triptych_20251103_181758_s14.mat\n",
      "Superposition sample 16/20 -> results_superposition/super_triptych_20251103_181758_s15.png\n",
      "  MAT saved -> results_superposition/super_triptych_20251103_181758_s15.mat\n",
      "Superposition sample 17/20 -> results_superposition/super_triptych_20251103_181758_s16.png\n",
      "  MAT saved -> results_superposition/super_triptych_20251103_181758_s16.mat\n",
      "Superposition sample 18/20 -> results_superposition/super_triptych_20251103_181758_s17.png\n",
      "  MAT saved -> results_superposition/super_triptych_20251103_181758_s17.mat\n",
      "Superposition sample 19/20 -> results_superposition/super_triptych_20251103_181758_s18.png\n",
      "  MAT saved -> results_superposition/super_triptych_20251103_181758_s18.mat\n",
      "Superposition sample 20/20 -> results_superposition/super_triptych_20251103_181758_s19.png\n",
      "  MAT saved -> results_superposition/super_triptych_20251103_181758_s19.mat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ydzhang/Desktop/odnn_code/odnn_training_visualization.py:534: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "  plt.tight_layout()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved figure -> /home/ydzhang/Desktop/odnn_code/results_superposition/slices_20251103_181758/m1/superposition_m1_scan_input.png\n",
      "Saved figure -> /home/ydzhang/Desktop/odnn_code/results_superposition/slices_20251103_181758/m1/superposition_m1_scan_layer1.png\n",
      "Saved figure -> /home/ydzhang/Desktop/odnn_code/results_superposition/slices_20251103_181758/m1/superposition_m1_scan_layer2.png\n",
      "Saved figure -> /home/ydzhang/Desktop/odnn_code/results_superposition/slices_20251103_181758/m1/superposition_m1_scan_layer3.png\n",
      "Saved figure -> /home/ydzhang/Desktop/odnn_code/results_superposition/slices_20251103_181758/m1/superposition_m1_scan_to_camera.png\n",
      "Superposition slices saved -> /home/ydzhang/Desktop/odnn_code/results_superposition/slices_20251103_181758/m1\n",
      "\n",
      "Superposition sample outputs:\n",
      " - Sample 01 (20251103_181758_s00): fig=results_superposition/super_triptych_20251103_181758_s00.png, mat=results_superposition/super_triptych_20251103_181758_s00.mat\n",
      " - Sample 02 (20251103_181758_s01): fig=results_superposition/super_triptych_20251103_181758_s01.png, mat=results_superposition/super_triptych_20251103_181758_s01.mat\n",
      " - Sample 03 (20251103_181758_s02): fig=results_superposition/super_triptych_20251103_181758_s02.png, mat=results_superposition/super_triptych_20251103_181758_s02.mat\n",
      " - Sample 04 (20251103_181758_s03): fig=results_superposition/super_triptych_20251103_181758_s03.png, mat=results_superposition/super_triptych_20251103_181758_s03.mat\n",
      " - Sample 05 (20251103_181758_s04): fig=results_superposition/super_triptych_20251103_181758_s04.png, mat=results_superposition/super_triptych_20251103_181758_s04.mat\n",
      " - Sample 06 (20251103_181758_s05): fig=results_superposition/super_triptych_20251103_181758_s05.png, mat=results_superposition/super_triptych_20251103_181758_s05.mat\n",
      " - Sample 07 (20251103_181758_s06): fig=results_superposition/super_triptych_20251103_181758_s06.png, mat=results_superposition/super_triptych_20251103_181758_s06.mat\n",
      " - Sample 08 (20251103_181758_s07): fig=results_superposition/super_triptych_20251103_181758_s07.png, mat=results_superposition/super_triptych_20251103_181758_s07.mat\n",
      " - Sample 09 (20251103_181758_s08): fig=results_superposition/super_triptych_20251103_181758_s08.png, mat=results_superposition/super_triptych_20251103_181758_s08.mat\n",
      " - Sample 10 (20251103_181758_s09): fig=results_superposition/super_triptych_20251103_181758_s09.png, mat=results_superposition/super_triptych_20251103_181758_s09.mat\n",
      " - Sample 11 (20251103_181758_s10): fig=results_superposition/super_triptych_20251103_181758_s10.png, mat=results_superposition/super_triptych_20251103_181758_s10.mat\n",
      " - Sample 12 (20251103_181758_s11): fig=results_superposition/super_triptych_20251103_181758_s11.png, mat=results_superposition/super_triptych_20251103_181758_s11.mat\n",
      " - Sample 13 (20251103_181758_s12): fig=results_superposition/super_triptych_20251103_181758_s12.png, mat=results_superposition/super_triptych_20251103_181758_s12.mat\n",
      " - Sample 14 (20251103_181758_s13): fig=results_superposition/super_triptych_20251103_181758_s13.png, mat=results_superposition/super_triptych_20251103_181758_s13.mat\n",
      " - Sample 15 (20251103_181758_s14): fig=results_superposition/super_triptych_20251103_181758_s14.png, mat=results_superposition/super_triptych_20251103_181758_s14.mat\n",
      " - Sample 16 (20251103_181758_s15): fig=results_superposition/super_triptych_20251103_181758_s15.png, mat=results_superposition/super_triptych_20251103_181758_s15.mat\n",
      " - Sample 17 (20251103_181758_s16): fig=results_superposition/super_triptych_20251103_181758_s16.png, mat=results_superposition/super_triptych_20251103_181758_s16.mat\n",
      " - Sample 18 (20251103_181758_s17): fig=results_superposition/super_triptych_20251103_181758_s17.png, mat=results_superposition/super_triptych_20251103_181758_s17.mat\n",
      " - Sample 19 (20251103_181758_s18): fig=results_superposition/super_triptych_20251103_181758_s18.png, mat=results_superposition/super_triptych_20251103_181758_s18.mat\n",
      " - Sample 20 (20251103_181758_s19): fig=results_superposition/super_triptych_20251103_181758_s19.png, mat=results_superposition/super_triptych_20251103_181758_s19.mat\n"
     ]
    }
   ],
   "source": [
    "if pred_case == 1 and run_superposition_debug:\n",
    "    super_dir = Path(\"results_superposition\")\n",
    "    super_dir.mkdir(parents=True, exist_ok=True)\n",
    "    super_tag = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    super_records: list[dict[str, str | int]] = []\n",
    "    slice_reference_input: torch.Tensor | None = None\n",
    "\n",
    "    for sample_idx in range(num_superposition_visual_samples):\n",
    "        super_sample = generate_superposition_sample(\n",
    "            num_modes=num_modes,\n",
    "            field_size=field_size,\n",
    "            layer_size=layer_size,\n",
    "            mmf_modes=MMF_data_ts,\n",
    "            mmf_label_data=MMF_Label_data,\n",
    "        )\n",
    "        super_output_map = infer_superposition_output(\n",
    "            D2NN,\n",
    "            super_sample[\"padded_image\"],\n",
    "            device,\n",
    "        )\n",
    "\n",
    "        sample_tag = f\"{super_tag}_s{sample_idx:02d}\"\n",
    "        triptych_paths = save_superposition_triptych(\n",
    "            input_field=super_sample[\"padded_image\"][0],\n",
    "            output_intensity_map=super_output_map,\n",
    "            amplitudes=super_sample[\"amplitudes\"],\n",
    "            phases=super_sample[\"phases\"],\n",
    "            complex_weights=super_sample[\"complex_weights\"],\n",
    "            label_map=super_sample[\"padded_label\"][0],\n",
    "            evaluation_regions=evaluation_regions,\n",
    "            detect_radius=detectsize,\n",
    "            output_dir=super_dir,\n",
    "            tag=sample_tag,\n",
    "            save_plot=save_superposition_plots,\n",
    "        )\n",
    "        if triptych_paths[\"fig_path\"]:\n",
    "            print(\n",
    "                f\"Superposition sample {sample_idx + 1}/{num_superposition_visual_samples} -> \"\n",
    "                f\"{triptych_paths['fig_path']}\"\n",
    "            )\n",
    "        print(f\"  MAT saved -> {triptych_paths['mat_path']}\")\n",
    "\n",
    "        super_records.append(\n",
    "            {\n",
    "                \"index\": sample_idx,\n",
    "                \"tag\": sample_tag,\n",
    "                \"fig\": triptych_paths[\"fig_path\"] if triptych_paths else \"\",\n",
    "                \"mat\": triptych_paths[\"mat_path\"] if triptych_paths else \"\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "        if slice_reference_input is None:\n",
    "            slice_reference_input = (\n",
    "                super_sample[\"padded_image\"].squeeze(0).to(device, dtype=torch.complex64)\n",
    "            )\n",
    "\n",
    "    if save_superposition_slices and all_phase_masks and slice_reference_input is not None:\n",
    "        slices_root = super_dir / f\"slices_{super_tag}\"\n",
    "        export_superposition_slices(\n",
    "            D2NN,\n",
    "            all_phase_masks,\n",
    "            slice_reference_input,\n",
    "            slices_root,\n",
    "            sample_tag=\"superposition\",\n",
    "            z_input_to_first=z_input_to_first,\n",
    "            z_layers=z_layers,\n",
    "            z_prop=z_prop,\n",
    "            z_step=z_step,\n",
    "            pixel_size=pixel_size,\n",
    "            wavelength=wavelength,\n",
    "        )\n",
    "\n",
    "    if super_records:\n",
    "        print(\"\\nSuperposition sample outputs:\")\n",
    "        for record in super_records:\n",
    "            print(\n",
    "                f\" - Sample {record['index'] + 1:02d} ({record['tag']}): \"\n",
    "                f\"fig={record['fig']}, mat={record['mat']}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9136d56-ced5-4e06-b928-40113fe5be2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running misalignment robustness sweep (±200 µm, 5 µm steps)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ydzhang/odnn_venv/lib/python3.13/site-packages/numpy/lib/_function_base_impl.py:2999: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "/home/ydzhang/odnn_venv/lib/python3.13/site-packages/numpy/lib/_function_base_impl.py:3000: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Completed shift row 1/41 (Δy = -200.0 µm)\n",
      "  Completed shift row 2/41 (Δy = -190.0 µm)\n",
      "  Completed shift row 3/41 (Δy = -180.0 µm)\n",
      "  Completed shift row 4/41 (Δy = -170.0 µm)\n",
      "  Completed shift row 5/41 (Δy = -160.0 µm)\n",
      "  Completed shift row 6/41 (Δy = -150.0 µm)\n",
      "  Completed shift row 7/41 (Δy = -140.0 µm)\n",
      "  Completed shift row 8/41 (Δy = -130.0 µm)\n",
      "  Completed shift row 9/41 (Δy = -120.0 µm)\n",
      "  Completed shift row 10/41 (Δy = -110.0 µm)\n",
      "  Completed shift row 11/41 (Δy = -100.0 µm)\n",
      "  Completed shift row 12/41 (Δy = -90.0 µm)\n",
      "  Completed shift row 13/41 (Δy = -80.0 µm)\n",
      "  Completed shift row 14/41 (Δy = -70.0 µm)\n",
      "  Completed shift row 15/41 (Δy = -60.0 µm)\n",
      "  Completed shift row 16/41 (Δy = -50.0 µm)\n",
      "  Completed shift row 17/41 (Δy = -40.0 µm)\n",
      "  Completed shift row 18/41 (Δy = -30.0 µm)\n",
      "  Completed shift row 19/41 (Δy = -20.0 µm)\n",
      "  Completed shift row 20/41 (Δy = -10.0 µm)\n",
      "  Completed shift row 21/41 (Δy = 0.0 µm)\n",
      "  Completed shift row 22/41 (Δy = 10.0 µm)\n",
      "  Completed shift row 23/41 (Δy = 20.0 µm)\n",
      "  Completed shift row 24/41 (Δy = 30.0 µm)\n",
      "  Completed shift row 25/41 (Δy = 40.0 µm)\n",
      "  Completed shift row 26/41 (Δy = 50.0 µm)\n",
      "  Completed shift row 27/41 (Δy = 60.0 µm)\n",
      "  Completed shift row 28/41 (Δy = 70.0 µm)\n",
      "  Completed shift row 29/41 (Δy = 80.0 µm)\n",
      "  Completed shift row 30/41 (Δy = 90.0 µm)\n",
      "  Completed shift row 31/41 (Δy = 100.0 µm)\n",
      "  Completed shift row 32/41 (Δy = 110.0 µm)\n",
      "  Completed shift row 33/41 (Δy = 120.0 µm)\n",
      "  Completed shift row 34/41 (Δy = 130.0 µm)\n",
      "  Completed shift row 35/41 (Δy = 140.0 µm)\n",
      "  Completed shift row 36/41 (Δy = 150.0 µm)\n",
      "  Completed shift row 37/41 (Δy = 160.0 µm)\n",
      "  Completed shift row 38/41 (Δy = 170.0 µm)\n",
      "  Completed shift row 39/41 (Δy = 180.0 µm)\n",
      "  Completed shift row 40/41 (Δy = 190.0 µm)\n",
      "  Completed shift row 41/41 (Δy = 200.0 µm)\n",
      "\n",
      "✔ Misalignment robustness surface saved -> results/robustness_analysis/misalignment_surface_20251103_181834.png\n",
      "✔ Misalignment robustness data (.mat) -> results/robustness_analysis/misalignment_surface_20251103_181834.mat\n"
     ]
    }
   ],
   "source": [
    "if run_misalignment_robustness and pred_case == 1:\n",
    "    robustness_dir = Path(\"results/robustness_analysis\")\n",
    "    robustness_dir.mkdir(parents=True, exist_ok=True)\n",
    "    robustness_tag = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    dx_um_values = np.arange(-200.0, 205.0, 10.0, dtype=np.float32)\n",
    "    dy_um_values = np.arange(-200.0, 205.0, 10.0, dtype=np.float32)\n",
    "    amp_err_surface = np.zeros((len(dy_um_values), len(dx_um_values)), dtype=np.float64)\n",
    "\n",
    "    def um_to_pixels(shift_um: float) -> int:\n",
    "        return int(round((shift_um * 1e-6) / pixel_size))\n",
    "\n",
    "    print(\"\\nRunning misalignment robustness sweep (±200 µm, 5 µm steps)...\")\n",
    "    for iy, dy_um in enumerate(dy_um_values):\n",
    "        shift_y_px = um_to_pixels(float(dy_um))\n",
    "        for ix, dx_um in enumerate(dx_um_values):\n",
    "            shift_x_px = um_to_pixels(float(dx_um))\n",
    "            metrics = compute_amp_relative_error_with_shift(\n",
    "                D2NN,\n",
    "                test_loader,\n",
    "                shift_y_px=shift_y_px,\n",
    "                shift_x_px=shift_x_px,\n",
    "                evaluation_regions=evaluation_regions,\n",
    "                pred_case=pred_case,\n",
    "                num_modes=num_modes,\n",
    "                eval_amplitudes=eval_amplitudes,\n",
    "                eval_amplitudes_phases=eval_amplitudes_phases,\n",
    "                eval_phases=eval_phases,\n",
    "                phase_option=phase_option,\n",
    "                mmf_modes=MMF_data_ts,\n",
    "                field_size=field_size,\n",
    "                image_test_data=image_test_data,\n",
    "                device=device,\n",
    "            )\n",
    "            amp_err_surface[iy, ix] = float(metrics.get(\"avg_relative_amp_err\", float(\"nan\")))\n",
    "        print(f\"  Completed shift row {iy + 1}/{len(dy_um_values)} (Δy = {dy_um:.1f} µm)\")\n",
    "\n",
    "    DX, DY = np.meshgrid(dx_um_values, dy_um_values)\n",
    "    fig = plt.figure(figsize=(9, 7))\n",
    "    ax = fig.add_subplot(111, projection=\"3d\")\n",
    "    surf = ax.plot_surface(DX, DY, amp_err_surface, cmap=\"viridis\")\n",
    "    ax.set_xlabel(\"Δx (µm)\")\n",
    "    ax.set_ylabel(\"Δy (µm)\")\n",
    "    ax.set_zlabel(\"Relative amplitude error\")\n",
    "    ax.set_title(\"Amplitude error vs. input-mask misalignment\")\n",
    "    fig.colorbar(surf, shrink=0.6, aspect=12)\n",
    "    fig.tight_layout()\n",
    "\n",
    "    robustness_fig_path = robustness_dir / f\"misalignment_surface_{robustness_tag}.png\"\n",
    "    fig.savefig(robustness_fig_path, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "    dx_px_values = np.rint(dx_um_values * 1e-6 / pixel_size).astype(np.int32)\n",
    "    dy_px_values = np.rint(dy_um_values * 1e-6 / pixel_size).astype(np.int32)\n",
    "    robustness_mat_path = robustness_dir / f\"misalignment_surface_{robustness_tag}.mat\"\n",
    "    savemat(\n",
    "        str(robustness_mat_path),\n",
    "        {\n",
    "            \"dx_um\": dx_um_values.astype(np.float32),\n",
    "            \"dy_um\": dy_um_values.astype(np.float32),\n",
    "            \"dx_pixels\": dx_px_values,\n",
    "            \"dy_pixels\": dy_px_values,\n",
    "            \"relative_amp_error\": amp_err_surface.astype(np.float64),\n",
    "            \"pixel_size_m\": np.array([pixel_size], dtype=np.float64),\n",
    "            \"step_um\": np.array([5.0], dtype=np.float32),\n",
    "            \"range_um\": np.array([200.0], dtype=np.float32),\n",
    "        },\n",
    "    )\n",
    "\n",
    "    if all_training_summaries:\n",
    "        all_training_summaries[-1][\"robustness_fig\"] = str(robustness_fig_path)\n",
    "        all_training_summaries[-1][\"robustness_mat\"] = str(robustness_mat_path)\n",
    "\n",
    "    print(f\"\\n✔ Misalignment robustness surface saved -> {robustness_fig_path}\")\n",
    "    print(f\"✔ Misalignment robustness data (.mat) -> {robustness_mat_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d572e2f3-1654-40bc-b285-a0a0443a4cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running misalignment robustness sweep (±200 µm, 5 µm steps)...\n",
      "  Completed shift row 1/20 (Δy = -20.0 µm)\n",
      "  Completed shift row 2/20 (Δy = -18.0 µm)\n",
      "  Completed shift row 3/20 (Δy = -16.0 µm)\n",
      "  Completed shift row 4/20 (Δy = -14.0 µm)\n",
      "  Completed shift row 5/20 (Δy = -12.0 µm)\n",
      "  Completed shift row 6/20 (Δy = -10.0 µm)\n",
      "  Completed shift row 7/20 (Δy = -8.0 µm)\n",
      "  Completed shift row 8/20 (Δy = -6.0 µm)\n",
      "  Completed shift row 9/20 (Δy = -4.0 µm)\n",
      "  Completed shift row 10/20 (Δy = -2.0 µm)\n",
      "  Completed shift row 11/20 (Δy = 0.0 µm)\n",
      "  Completed shift row 12/20 (Δy = 2.0 µm)\n",
      "  Completed shift row 13/20 (Δy = 4.0 µm)\n",
      "  Completed shift row 14/20 (Δy = 6.0 µm)\n",
      "  Completed shift row 15/20 (Δy = 8.0 µm)\n",
      "  Completed shift row 16/20 (Δy = 10.0 µm)\n",
      "  Completed shift row 17/20 (Δy = 12.0 µm)\n",
      "  Completed shift row 18/20 (Δy = 14.0 µm)\n",
      "  Completed shift row 19/20 (Δy = 16.0 µm)\n",
      "  Completed shift row 20/20 (Δy = 18.0 µm)\n",
      "\n",
      "✔ Misalignment robustness surface saved -> results/robustness_analysis/misalignment_surface_20251103_184050.png\n",
      "✔ Misalignment robustness data (.mat) -> results/robustness_analysis/misalignment_surface_20251103_184050.mat\n"
     ]
    }
   ],
   "source": [
    "if run_misalignment_robustness and pred_case == 1:\n",
    "    robustness_dir = Path(\"results/robustness_analysis\")\n",
    "    robustness_dir.mkdir(parents=True, exist_ok=True)\n",
    "    robustness_tag = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    dx_um_values = np.arange(-20.0, 20.0, 2.0, dtype=np.float32)\n",
    "    dy_um_values = np.arange(-20.0, 20.0, 2.0, dtype=np.float32)\n",
    "    amp_err_surface = np.zeros((len(dy_um_values), len(dx_um_values)), dtype=np.float64)\n",
    "\n",
    "    def um_to_pixels(shift_um: float) -> int:\n",
    "        return int(round((shift_um * 1e-6) / pixel_size))\n",
    "\n",
    "    print(\"\\nRunning misalignment robustness sweep (±200 µm, 5 µm steps)...\")\n",
    "    for iy, dy_um in enumerate(dy_um_values):\n",
    "        shift_y_px = um_to_pixels(float(dy_um))\n",
    "        for ix, dx_um in enumerate(dx_um_values):\n",
    "            shift_x_px = um_to_pixels(float(dx_um))\n",
    "            metrics = compute_amp_relative_error_with_shift(\n",
    "                D2NN,\n",
    "                test_loader,\n",
    "                shift_y_px=shift_y_px,\n",
    "                shift_x_px=shift_x_px,\n",
    "                evaluation_regions=evaluation_regions,\n",
    "                pred_case=pred_case,\n",
    "                num_modes=num_modes,\n",
    "                eval_amplitudes=eval_amplitudes,\n",
    "                eval_amplitudes_phases=eval_amplitudes_phases,\n",
    "                eval_phases=eval_phases,\n",
    "                phase_option=phase_option,\n",
    "                mmf_modes=MMF_data_ts,\n",
    "                field_size=field_size,\n",
    "                image_test_data=image_test_data,\n",
    "                device=device,\n",
    "            )\n",
    "            amp_err_surface[iy, ix] = float(metrics.get(\"avg_relative_amp_err\", float(\"nan\")))\n",
    "        print(f\"  Completed shift row {iy + 1}/{len(dy_um_values)} (Δy = {dy_um:.1f} µm)\")\n",
    "\n",
    "    DX, DY = np.meshgrid(dx_um_values, dy_um_values)\n",
    "    fig = plt.figure(figsize=(9, 7))\n",
    "    ax = fig.add_subplot(111, projection=\"3d\")\n",
    "    surf = ax.plot_surface(DX, DY, amp_err_surface, cmap=\"viridis\")\n",
    "    ax.set_xlabel(\"Δx (µm)\")\n",
    "    ax.set_ylabel(\"Δy (µm)\")\n",
    "    ax.set_zlabel(\"Relative amplitude error\")\n",
    "    ax.set_title(\"Amplitude error vs. input-mask misalignment\")\n",
    "    fig.colorbar(surf, shrink=0.6, aspect=12)\n",
    "    fig.tight_layout()\n",
    "\n",
    "    robustness_fig_path = robustness_dir / f\"misalignment_surface_{robustness_tag}.png\"\n",
    "    fig.savefig(robustness_fig_path, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "    dx_px_values = np.rint(dx_um_values * 1e-6 / pixel_size).astype(np.int32)\n",
    "    dy_px_values = np.rint(dy_um_values * 1e-6 / pixel_size).astype(np.int32)\n",
    "    robustness_mat_path = robustness_dir / f\"misalignment_surface_{robustness_tag}.mat\"\n",
    "    savemat(\n",
    "        str(robustness_mat_path),\n",
    "        {\n",
    "            \"dx_um\": dx_um_values.astype(np.float32),\n",
    "            \"dy_um\": dy_um_values.astype(np.float32),\n",
    "            \"dx_pixels\": dx_px_values,\n",
    "            \"dy_pixels\": dy_px_values,\n",
    "            \"relative_amp_error\": amp_err_surface.astype(np.float64),\n",
    "            \"pixel_size_m\": np.array([pixel_size], dtype=np.float64),\n",
    "            \"step_um\": np.array([5.0], dtype=np.float32),\n",
    "            \"range_um\": np.array([200.0], dtype=np.float32),\n",
    "        },\n",
    "    )\n",
    "\n",
    "    if all_training_summaries:\n",
    "        all_training_summaries[-1][\"robustness_fig\"] = str(robustness_fig_path)\n",
    "        all_training_summaries[-1][\"robustness_mat\"] = str(robustness_mat_path)\n",
    "\n",
    "    print(f\"\\n✔ Misalignment robustness surface saved -> {robustness_fig_path}\")\n",
    "    print(f\"✔ Misalignment robustness data (.mat) -> {robustness_mat_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "已连接到 odnn_venv (Python 3.13.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4817a165-8668-484e-bfea-4bf03e0d89d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device: cuda:5\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from scipy.io import savemat\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from ODNN_functions import (\n",
    "    create_evaluation_regions,\n",
    "    generate_complex_weights,\n",
    "    generate_fields_ts,\n",
    ")\n",
    "from odnn_generate_label import (\n",
    "    compute_label_centers,\n",
    "    compose_labels_from_patterns,\n",
    "    generate_detector_patterns,\n",
    ")\n",
    "from odnn_io import load_complex_modes_from_mat\n",
    "from odnn_model import D2NNModel\n",
    "from odnn_processing import prepare_sample\n",
    "from odnn_training_eval import (\n",
    "    build_superposition_eval_context,\n",
    "    compute_model_prediction_metrics,\n",
    "    evaluate_spot_metrics,\n",
    "    format_metric_report,\n",
    "    generate_superposition_sample,\n",
    "    infer_superposition_output,\n",
    ")\n",
    "from odnn_training_io import save_masks_one_file_per_layer, save_to_mat_light_plus\n",
    "from odnn_training_visualization import (\n",
    "    capture_eigenmode_propagation,\n",
    "    export_superposition_slices,\n",
    "    plot_amplitude_comparison_grid,\n",
    "    plot_reconstruction_vs_input,\n",
    "    plot_sys_vs_label_strict,\n",
    "    save_superposition_triptych,\n",
    "    save_mode_triptych,\n",
    "    visualize_model_slices,\n",
    ")\n",
    "\n",
    "SEED = 424242\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# 让 cuDNN/算子走确定性分支\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.use_deterministic_algorithms(True)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:5')           # 或者 'cuda:0'\n",
    "    print('Using Device:', device)\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('Using Device: CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e381f40-825a-43ca-b291-b4a7bc83999d",
   "metadata": {},
   "outputs": [],
   "source": [
    "field_size = 25  #the field size in eigenmodes_OM4 is 50 pixels\n",
    "layer_size = 100 #400#300#100\n",
    "num_data = 1000 # options: 1. random datas 2.eigenmodes\n",
    "num_modes = 6 #the mode number of MMF 3 6 10\n",
    "circle_focus_radius = 5 # radius when using uniform circular detectors\n",
    "circle_detectsize = 10  # square window size for circular detectors\n",
    "eigenmode_focus_radius = 12.5  # radius when using eigenmode patterns\n",
    "eigenmode_detectsize = 27    # square window size for eigenmode patterns\n",
    "focus_radius = circle_focus_radius\n",
    "detectsize = circle_detectsize\n",
    "batch_size = 16\n",
    "\n",
    "# Evaluation selection: \"eigenmode\" uses the base modes, \"superposition\" samples random mixtures\n",
    "evaluation_mode = \"superposition\"  # options: \"eigenmode\", \"superposition\"\n",
    "num_superposition_eval_samples = 1000\n",
    "num_superposition_visual_samples = 20\n",
    "run_superposition_debug = True\n",
    "save_superposition_plots = True\n",
    "save_superposition_slices = True\n",
    "run_misalignment_robustness = True\n",
    "label_pattern_mode = \"circle\"  # options: \"eigenmode\", \"circle\"\n",
    "# Define multiple D2NN models \n",
    "num_layer_option = [3]   #, 3]#, 4]  # Define the different layer-number ODNN\n",
    "all_losses = [] #the loss for each epoch of each ODNN model\n",
    "all_phase_masks = [] #the phase masks field of each ODNN model\n",
    "all_predictions = [] #the output light field of each ODNN model\n",
    "model_metrics: list[dict] = []\n",
    "all_amplitudes_diff: list[np.ndarray] = []\n",
    "all_average_amplitudes_diff: list[float] = []\n",
    "all_amplitudes_relative_diff: list[float] = []\n",
    "all_complex_weights_pred: list[np.ndarray] = []\n",
    "all_image_data_pred: list[np.ndarray] = []\n",
    "all_cc_real: list[np.ndarray] = []\n",
    "all_cc_imag: list[np.ndarray] = []\n",
    "all_cc_recon_amp: list[np.ndarray] = []\n",
    "all_cc_recon_phase: list[np.ndarray] = []\n",
    "all_training_summaries: list[dict] = []\n",
    "# SLM\n",
    "z_layers   = 40e-6        # 原 47.571e-3  -> 40 μm\n",
    "pixel_size = 1e-6\n",
    "z_prop     = 120e-6        # 原 16.74e-2   -> 60 μm plus 40（最后一层到相机）\n",
    "wavelength = 1568e-9      # 原 1568     -> 1550 nm\n",
    "z_input_to_first = 40e-6  # 40 μm # 新增：输入面到第一层的传播距离"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf897506-d765-4f9b-afbc-3d316f8968fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded modes shape: (25, 25, 6) dtype: complex64\n"
     ]
    }
   ],
   "source": [
    "eigenmodes_OM4 = load_complex_modes_from_mat(\n",
    "    'mmf_6modes_25_PD_1.15.mat',\n",
    "    key='modes_field'\n",
    ")\n",
    "# (H, W, M)\n",
    "print(\"Loaded modes shape:\", eigenmodes_OM4.shape, \"dtype:\", eigenmodes_OM4.dtype)\n",
    "\n",
    "# 取前 num_modes 个 → (H, W, M_sel) → (M_sel, H, W)\n",
    "MMF_data = eigenmodes_OM4[:, :, :num_modes].transpose(2, 0, 1)\n",
    "MMF_data_amp_norm = (np.abs(MMF_data) - np.min(np.abs(MMF_data))) / (np.max(np.abs(MMF_data)) - np.min(np.abs(MMF_data)))\n",
    "\n",
    "MMF_data = MMF_data_amp_norm * np.exp(1j * np.angle(MMF_data))\n",
    "\n",
    "#要是以后确定了用4我在想要不要去掉其他选项\n",
    "phase_option = 4\n",
    "#phase_option 1: (0,0,...,0)\n",
    "#phase_option 2: (0,2pi,...,2pi)\n",
    "#phase_option 3: (0,pi,...,2pi)\n",
    "#phase_option 4: eigenmodes\n",
    "#phase_option 5: (0,pi,...,pi)\n",
    "\n",
    "if phase_option in [1, 2, 3, 5]:\n",
    "    amplitudes,phases = generate_complex_weights(num_data,num_modes,phase_option)\n",
    "\n",
    "if phase_option == 4:\n",
    "    num_data = num_modes # use the eigenmodes to train ODNN\n",
    "    amplitudes = np.eye(num_modes)#[[1,0,0][0,1,0][0,0,1]]\n",
    "    phases = np.eye(num_modes)\n",
    "\n",
    "amplitudes_phases_ori = np.hstack((amplitudes[:, :], phases[:, 1:]))  # amplitudes (l2 norm) phases\n",
    "amplitudes_phases = np.hstack((amplitudes[:, :], phases[:, 1:]/(2*np.pi)))  # amplitudes (l2 norm) phases (0-1)\n",
    "\n",
    "# Generate complex weights vector with specified amplitudes and phases\n",
    "complex_weights = amplitudes * np.exp(1j * phases)\n",
    "MMF_data_ts = torch.from_numpy(MMF_data)\n",
    "complex_weights_ts = torch.from_numpy(complex_weights)\n",
    "image_data = generate_fields_ts(complex_weights_ts, MMF_data_ts, num_data, num_modes, field_size).to(torch.complex64)\n",
    "image_test_data = image_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c80be97-6b21-4361-b7dc-02e180bcb51a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "相邻图案边缘间距： 行=26.67, 列=17.50\n",
      "相邻图案中心间距： 行=36.67, 列=27.50\n",
      "中心坐标： [(32, 22), (32, 50), (32, 78), (68, 22), (68, 50), (68, 78)]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "pred_case = 1: only amplitudes prediction\n",
    "pred_case = 2: only phases prediction\n",
    "pred_case = 3: amplitudes and phases prediction\n",
    "pred_case = 4: amplitudes and phases prediction (extra energy phase area)\n",
    "'''\n",
    "#\n",
    "pred_case = 1\n",
    "label_data = torch.zeros([num_data,1,layer_size,layer_size])\n",
    "label_size = layer_size\n",
    "\n",
    "if pred_case == 1: # 3\n",
    "    num_detector = num_modes\n",
    "    detector_focus_radius = focus_radius\n",
    "    detector_detectsize = detectsize\n",
    "    if label_pattern_mode == \"eigenmode\":\n",
    "        pattern_stack = np.transpose(np.abs(MMF_data), (1, 2, 0))\n",
    "        pattern_h, pattern_w, _ = pattern_stack.shape\n",
    "        if pattern_h > label_size or pattern_w > label_size:\n",
    "            raise ValueError(\n",
    "                f\"Eigenmode pattern size ({pattern_h}x{pattern_w}) exceeds label canvas {label_size}.\"\n",
    "            )\n",
    "        layout_radius = math.ceil(max(pattern_h, pattern_w) / 2)\n",
    "        detector_focus_radius = eigenmode_focus_radius\n",
    "        detector_detectsize = eigenmode_detectsize\n",
    "    elif label_pattern_mode == \"circle\":\n",
    "        circle_radius = circle_focus_radius\n",
    "        pattern_size = circle_radius * 2\n",
    "        if pattern_size % 2 == 0:\n",
    "            pattern_size += 1  \n",
    "        pattern_stack = generate_detector_patterns(pattern_size, pattern_size, num_detector, shape=\"circle\")\n",
    "        layout_radius = circle_radius\n",
    "        detector_focus_radius = circle_radius\n",
    "        detector_detectsize = circle_detectsize\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown label_pattern_mode: {label_pattern_mode}\")\n",
    "\n",
    "    centers, _, _ = compute_label_centers(label_size, label_size, num_detector, layout_radius)\n",
    "    mode_label_maps = [\n",
    "        compose_labels_from_patterns(\n",
    "            label_size,\n",
    "            label_size,\n",
    "            pattern_stack,\n",
    "            centers,\n",
    "            Index=i + 1,\n",
    "            visualize=False,\n",
    "        )\n",
    "        for i in range(num_detector)\n",
    "    ]\n",
    "    MMF_Label_data = torch.from_numpy(\n",
    "        np.stack(mode_label_maps, axis=2).astype(np.float32)\n",
    "    )\n",
    "    amplitude_weights = torch.from_numpy(amplitudes_phases[:num_data, 0:num_modes]).float()\n",
    "    combined_labels = (\n",
    "        amplitude_weights[:, None, None, :] * MMF_Label_data.unsqueeze(0)\n",
    "    ).sum(dim=3)\n",
    "    label_data[:, 0, :, :] = combined_labels\n",
    "    focus_radius = detector_focus_radius\n",
    "    detectsize = detector_detectsize\n",
    "\n",
    "label_test_data = label_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c238edf-8eed-4ec5-a6e4-bf8fd12856c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detection Regions: [(17, 27, 27, 37), (45, 55, 27, 37), (73, 83, 27, 37), (17, 27, 63, 73), (45, 55, 63, 73), (73, 83, 63, 73)]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = [\n",
    "    prepare_sample(image_data[i], label_data[i], layer_size) for i in range(len(label_data))\n",
    "]\n",
    "train_tensor_data = TensorDataset(*[torch.stack(tensors) for tensors in zip(*train_dataset)])\n",
    "g = torch.Generator()\n",
    "g.manual_seed(SEED)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_tensor_data,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,               # 顺序会被 g 固定\n",
    "    generator=g,                # 固定打乱\n",
    "   \n",
    ")\n",
    "\n",
    "superposition_eval_ctx: dict | None = None\n",
    "if evaluation_mode == \"eigenmode\":\n",
    "    test_dataset = train_dataset\n",
    "    test_tensor_data = train_tensor_data\n",
    "    test_loader = DataLoader(test_tensor_data, batch_size=batch_size, shuffle=False)\n",
    "    eval_amplitudes = amplitudes\n",
    "    eval_amplitudes_phases = amplitudes_phases\n",
    "    eval_phases = phases\n",
    "    image_test_data = image_data\n",
    "elif evaluation_mode == \"superposition\":\n",
    "    if pred_case != 1:\n",
    "        raise ValueError(\"Superposition evaluation mode currently supports pred_case == 1 only.\")\n",
    "    super_ctx = build_superposition_eval_context(\n",
    "        num_superposition_eval_samples,\n",
    "        num_modes=num_modes,\n",
    "        field_size=field_size,\n",
    "        layer_size=layer_size,\n",
    "        mmf_modes=MMF_data_ts,\n",
    "        mmf_label_data=MMF_Label_data,\n",
    "        batch_size=batch_size,\n",
    "        second_mode_half_range=True,\n",
    "    )\n",
    "    test_dataset = super_ctx[\"dataset\"]\n",
    "    test_tensor_data = super_ctx[\"tensor_dataset\"]\n",
    "    test_loader = super_ctx[\"loader\"]\n",
    "    image_test_data = super_ctx[\"image_data\"]\n",
    "    eval_amplitudes = super_ctx[\"amplitudes\"]\n",
    "    eval_amplitudes_phases = super_ctx[\"amplitudes_phases\"]\n",
    "    eval_phases = super_ctx[\"phases\"]\n",
    "    superposition_eval_ctx = super_ctx\n",
    "else:\n",
    "    raise ValueError(f\"Unknown evaluation_mode: {evaluation_mode}\")\n",
    "\n",
    "# Generate detection regions using existing function\n",
    "if pred_case ==1:\n",
    "    evaluation_regions = create_evaluation_regions(layer_size, layer_size, num_detector, focus_radius, detectsize)\n",
    "    print(\"Detection Regions:\", evaluation_regions)\n",
    "\n",
    "\n",
    "def shift_complex_batch(batch: torch.Tensor, shift_y: int, shift_x: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Translate a batch of complex fields by (shift_y, shift_x) pixels with zero padding.\n",
    "    Positive shift_y moves downward; positive shift_x moves right.\n",
    "    \"\"\"\n",
    "    if shift_y == 0 and shift_x == 0:\n",
    "        return batch\n",
    "\n",
    "    _, _, height, width = batch.shape\n",
    "    if abs(shift_y) >= height or abs(shift_x) >= width:\n",
    "        return torch.zeros_like(batch)\n",
    "\n",
    "    real_imag = torch.view_as_real(batch)\n",
    "    shifted = torch.zeros_like(real_imag)\n",
    "\n",
    "    if shift_y >= 0:\n",
    "        src_y = slice(0, height - shift_y)\n",
    "        dst_y = slice(shift_y, height)\n",
    "    else:\n",
    "        src_y = slice(-shift_y, height)\n",
    "        dst_y = slice(0, height + shift_y)\n",
    "\n",
    "    if shift_x >= 0:\n",
    "        src_x = slice(0, width - shift_x)\n",
    "        dst_x = slice(shift_x, width)\n",
    "    else:\n",
    "        src_x = slice(-shift_x, width)\n",
    "        dst_x = slice(0, width + shift_x)\n",
    "\n",
    "    shifted[:, :, dst_y, dst_x, :] = real_imag[:, :, src_y, src_x, :]\n",
    "    return torch.view_as_complex(shifted)\n",
    "\n",
    "\n",
    "def compute_amp_relative_error_with_shift(\n",
    "    model: torch.nn.Module,\n",
    "    loader,\n",
    "    *,\n",
    "    shift_y_px: int,\n",
    "    shift_x_px: int,\n",
    "    evaluation_regions,\n",
    "    pred_case: int,\n",
    "    num_modes: int,\n",
    "    eval_amplitudes: np.ndarray,\n",
    "    eval_amplitudes_phases: np.ndarray,\n",
    "    eval_phases: np.ndarray,\n",
    "    phase_option: int,\n",
    "    mmf_modes: torch.Tensor,\n",
    "    field_size: int,\n",
    "    image_test_data: torch.Tensor,\n",
    "    device: torch.device,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Evaluate amplitude-related metrics when the input field is shifted by (shift_y_px, shift_x_px).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_weights_pred: list[np.ndarray] = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, _ in loader:\n",
    "            images = images.to(device, dtype=torch.complex64, non_blocking=True)\n",
    "            shifted_images = shift_complex_batch(images, shift_y_px, shift_x_px)\n",
    "            preds = model(shifted_images)\n",
    "            preds_np = preds.detach().cpu().numpy()\n",
    "\n",
    "            for sample_idx in range(preds_np.shape[0]):\n",
    "                intensity_map = preds_np[sample_idx, 0]\n",
    "                weights = []\n",
    "                for (x0, x1, y0, y1) in evaluation_regions:\n",
    "                    weights.append(float(intensity_map[y0:y1, x0:x1].mean()))\n",
    "                weights = np.asarray(weights, dtype=np.float64)\n",
    "\n",
    "                if pred_case == 3 and num_modes <= len(weights):\n",
    "                    norm_val = np.linalg.norm(weights[:num_modes])\n",
    "                    if norm_val > 0:\n",
    "                        weights[:num_modes] /= norm_val\n",
    "                else:\n",
    "                    norm_val = np.linalg.norm(weights)\n",
    "                    if norm_val > 0:\n",
    "                        weights /= norm_val\n",
    "\n",
    "                all_weights_pred.append(weights)\n",
    "\n",
    "    metrics = compute_model_prediction_metrics(\n",
    "        all_weights_pred,\n",
    "        eval_amplitudes,\n",
    "        eval_amplitudes_phases,\n",
    "        eval_phases,\n",
    "        phase_option,\n",
    "        pred_case,\n",
    "        num_modes,\n",
    "        mmf_modes,\n",
    "        field_size,\n",
    "        image_test_data,\n",
    "    )\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688d7af3-0077-4e72-b75f-33a3ec549e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training D2NN with 3 layers...\n",
      "\n",
      "D2NNModel(\n",
      "  (pre_propagation): Propagation()\n",
      "  (layers): ModuleList(\n",
      "    (0-2): 3 x DiffractionLayer()\n",
      "  )\n",
      "  (propagation): Propagation()\n",
      "  (regression): RegressionDetector()\n",
      ")\n",
      "Epoch [1/1000], Loss: 0.008090289309620857, Epoch Time: 0.10 seconds\n",
      "Epoch [100/1000], Loss: 0.002760213334113359, Epoch Time: 0.01 seconds\n",
      "Epoch [200/1000], Loss: 0.002684592502191663, Epoch Time: 0.01 seconds\n",
      "Epoch [300/1000], Loss: 0.002658451907336712, Epoch Time: 0.01 seconds\n",
      "Epoch [400/1000], Loss: 0.002648203633725643, Epoch Time: 0.01 seconds\n",
      "Epoch [500/1000], Loss: 0.002644307911396027, Epoch Time: 0.01 seconds\n",
      "Epoch [600/1000], Loss: 0.002642895793542266, Epoch Time: 0.01 seconds\n",
      "Epoch [700/1000], Loss: 0.002642374718561769, Epoch Time: 0.01 seconds\n",
      "Epoch [800/1000], Loss: 0.002642178907990456, Epoch Time: 0.01 seconds\n",
      "Epoch [900/1000], Loss: 0.002642104867845774, Epoch Time: 0.01 seconds\n",
      "Epoch [1000/1000], Loss: 0.002642077626660466, Epoch Time: 0.01 seconds\n",
      "Total training time for 3-layer model: 10.29 seconds (~0.17 minutes)\n"
     ]
    }
   ],
   "source": [
    "for num_layer in num_layer_option:\n",
    "    print(f\"\\nTraining D2NN with {num_layer} layers...\\n\")\n",
    "\n",
    "    D2NN = D2NNModel(\n",
    "        num_layers=num_layer,\n",
    "        layer_size=layer_size,\n",
    "        z_layers=z_layers,\n",
    "        z_prop=z_prop,\n",
    "        pixel_size=pixel_size,\n",
    "        wavelength=wavelength,\n",
    "        device=device,\n",
    "        padding_ratio=0.5,\n",
    "        z_input_to_first=z_input_to_first,   # NEW\n",
    "    ).to(device)\n",
    "\n",
    "    print(D2NN)\n",
    "\n",
    "    # Training\n",
    "    criterion = nn.MSELoss()  # Define loss function (对比的是loss)\n",
    "    optimizer = optim.Adam(D2NN.parameters(), lr=1.99) \n",
    "    scheduler = ExponentialLR(optimizer, gamma=0.99)  \n",
    "    epochs = 1000\n",
    "    losses = []\n",
    "    epoch_durations: list[float] = []\n",
    "    training_start_time = time.time()\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        D2NN.train()\n",
    "        epoch_loss = 0\n",
    "        for images, labels in train_loader:\n",
    "            images = images.to(device, dtype=torch.complex64, non_blocking=True)\n",
    "            labels = labels.to(device, dtype=torch.float32,   non_blocking=True)\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            outputs = D2NN(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        scheduler.step()\n",
    "        avg_loss = epoch_loss / len(train_loader)  # Calculate average loss for the epoch\n",
    "        losses.append(avg_loss)  # the loss for each model\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.synchronize(device)\n",
    "        epoch_duration = time.time() - epoch_start_time\n",
    "        epoch_durations.append(epoch_duration)\n",
    "\n",
    "        if epoch % 100 == 0 or epoch == 1 or epoch == epochs:\n",
    "            print(\n",
    "                f'Epoch [{epoch}/{epochs}], Loss: {avg_loss:.18f}, '\n",
    "                f'Epoch Time: {epoch_duration:.2f} seconds'\n",
    "            )\n",
    "\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.synchronize(device)\n",
    "    total_training_time = time.time() - training_start_time\n",
    "    print(\n",
    "        f'Total training time for {num_layer}-layer model: {total_training_time:.2f} seconds '\n",
    "        f'(~{total_training_time / 60:.2f} minutes)'\n",
    "    )\n",
    "    all_losses.append(losses)  # save the loss for each model\n",
    "    training_output_dir = Path(\"results/training_analysis\")\n",
    "    training_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    epochs_array = np.arange(1, epochs + 1, dtype=np.int32)\n",
    "    cumulative_epoch_times = np.cumsum(epoch_durations)\n",
    "    timestamp_tag = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(epochs_array, losses, label=\"Training Loss\")\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.set_title(f\"D2NN Training Loss ({num_layer} layers)\")\n",
    "    ax.grid(True, which=\"both\", linestyle=\"--\", linewidth=0.5)\n",
    "    ax.legend()\n",
    "    loss_plot_path = training_output_dir / f\"loss_curve_layers{num_layer}_{timestamp_tag}.png\"\n",
    "    fig.savefig(loss_plot_path, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "    fig_time, ax_time = plt.subplots()\n",
    "    ax_time.plot(epochs_array, cumulative_epoch_times, label=\"Cumulative Time\")\n",
    "    ax_time.set_xlabel(\"Epoch\")\n",
    "    ax_time.set_ylabel(\"Time (seconds)\")\n",
    "    ax_time.set_title(f\"Cumulative Training Time ({num_layer} layers)\")\n",
    "    ax_time.grid(True, which=\"both\", linestyle=\"--\", linewidth=0.5)\n",
    "    ax_time.legend()\n",
    "    time_plot_path = training_output_dir / f\"epoch_time_layers{num_layer}_{timestamp_tag}.png\"\n",
    "    fig_time.savefig(time_plot_path, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close(fig_time)\n",
    "\n",
    "    mat_path = training_output_dir / f\"training_curves_layers{num_layer}_{timestamp_tag}.mat\"\n",
    "    savemat(\n",
    "        str(mat_path),\n",
    "        {\n",
    "            \"epochs\": epochs_array,\n",
    "            \"losses\": np.array(losses, dtype=np.float64),\n",
    "            \"epoch_durations\": np.array(epoch_durations, dtype=np.float64),\n",
    "            \"cumulative_epoch_times\": np.array(cumulative_epoch_times, dtype=np.float64),\n",
    "            \"total_training_time\": np.array([total_training_time], dtype=np.float64),\n",
    "            \"num_layers\": np.array([num_layer], dtype=np.int32),\n",
    "        },\n",
    "    )\n",
    "\n",
    "    print(f\"✔ Saved training loss plot -> {loss_plot_path}\")\n",
    "    print(f\"✔ Saved cumulative time plot -> {time_plot_path}\")\n",
    "    print(f\"✔ Saved training log data (.mat) -> {mat_path}\")\n",
    "\n",
    "    propagation_dir = Path(\"results/propagation_slices\")\n",
    "    eigenmode_index = min(2, MMF_data_ts.shape[0] - 1)\n",
    "    propagation_summary = capture_eigenmode_propagation(\n",
    "        model=D2NN,\n",
    "        eigenmode_field=MMF_data_ts[eigenmode_index],\n",
    "        mode_index=eigenmode_index,\n",
    "        layer_size=layer_size,\n",
    "        z_input_to_first=z_input_to_first,\n",
    "        z_layers=z_layers,\n",
    "        z_prop=z_prop,\n",
    "        pixel_size=pixel_size,\n",
    "        wavelength=wavelength,\n",
    "        output_dir=propagation_dir,\n",
    "        tag=f\"layers{num_layer}_{timestamp_tag}\",\n",
    "    )\n",
    "    print(f\"✔ Saved eigenmode-{eigenmode_index + 1} propagation plot -> {propagation_summary['fig_path']}\")\n",
    "    print(f\"✔ Saved eigenmode-{eigenmode_index + 1} propagation data (.mat) -> {propagation_summary['mat_path']}\")\n",
    "\n",
    "    mode_triptych_records: list[dict[str, str | int]] = []\n",
    "    if evaluation_mode == \"eigenmode\":\n",
    "        triptych_dir = Path(\"results/mode_triptychs\")\n",
    "        mode_tag = f\"layers{num_layer}_{timestamp_tag}\"\n",
    "        for mode_idx in range(min(num_modes, len(MMF_data_ts))):\n",
    "            label_tensor = label_data[mode_idx, 0]\n",
    "            record = save_mode_triptych(\n",
    "                model=D2NN,\n",
    "                mode_index=mode_idx,\n",
    "                eigenmode_field=MMF_data_ts[mode_idx],\n",
    "                label_field=label_tensor,\n",
    "                layer_size=layer_size,\n",
    "                output_dir=triptych_dir,\n",
    "                tag=mode_tag,\n",
    "            )\n",
    "            mode_triptych_records.append(\n",
    "                {\n",
    "                    \"mode\": mode_idx + 1,\n",
    "                    \"fig\": record[\"fig_path\"],\n",
    "                    \"mat\": record[\"mat_path\"],\n",
    "                }\n",
    "            )\n",
    "            print(\n",
    "                f\"✔ Saved mode {mode_idx + 1} triptych -> {record['fig_path']}\\n\"\n",
    "                f\"  MAT -> {record['mat_path']}\"\n",
    "            )\n",
    "\n",
    "    all_training_summaries.append(\n",
    "        {\n",
    "            \"num_layers\": num_layer,\n",
    "            \"total_time\": total_training_time,\n",
    "            \"loss_plot\": str(loss_plot_path),\n",
    "            \"time_plot\": str(time_plot_path),\n",
    "            \"mat_path\": str(mat_path),\n",
    "            \"propagation_fig\": propagation_summary[\"fig_path\"],\n",
    "            \"propagation_mat\": propagation_summary[\"mat_path\"],\n",
    "            \"mode_triptychs\": mode_triptych_records,\n",
    "        }\n",
    "    )\n",
    "   \n",
    "    # === after training ===\n",
    "    ckpt_dir = \"checkpoints\"\n",
    "    os.makedirs(ckpt_dir, exist_ok=True)\n",
    "\n",
    "    ckpt = {\n",
    "        \"state_dict\": D2NN.state_dict(),\n",
    "        \"meta\": {\n",
    "            \"num_layers\":        len(D2NN.layers),\n",
    "            \"layer_size\":        layer_size,\n",
    "            \"z_layers\":          z_layers,\n",
    "            \"z_prop\":            z_prop,\n",
    "            \"pixel_size\":        pixel_size,\n",
    "            \"wavelength\":        wavelength,\n",
    "            \"padding_ratio\":     0.5,         \n",
    "            \"field_size\":        field_size,  \n",
    "            \"num_modes\":         num_modes, \n",
    "            \"z_input_to_first\":  z_input_to_first, \n",
    "        }\n",
    "    }\n",
    "    save_path = os.path.join(ckpt_dir, f\"odnn_{len(D2NN.layers)}layers.pth\")\n",
    "    torch.save(ckpt, save_path)\n",
    "    print(\"✔ Saved model ->\", save_path)\n",
    "    # Free GPU memory\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Cache phase masks for later visualization/export\n",
    "    phase_masks = []\n",
    "    for layer in D2NN.layers:\n",
    "        phase_np = layer.phase.detach().cpu().numpy()\n",
    "        phase_masks.append(np.remainder(phase_np, 2 * np.pi))\n",
    "    all_phase_masks.append(phase_masks)\n",
    "\n",
    "    # Collect evaluation metrics for this model\n",
    "    metrics = evaluate_spot_metrics(\n",
    "        D2NN,\n",
    "        test_loader,\n",
    "        evaluation_regions,\n",
    "        detect_radius=detectsize,\n",
    "        device=device,\n",
    "        pred_case=pred_case,\n",
    "        num_modes=num_modes,\n",
    "        phase_option=phase_option,\n",
    "        amplitudes=eval_amplitudes,\n",
    "        amplitudes_phases=eval_amplitudes_phases,\n",
    "        phases=eval_phases,\n",
    "        mmf_modes=MMF_data_ts,\n",
    "        field_size=field_size,\n",
    "        image_test_data=image_test_data,\n",
    "    )\n",
    "\n",
    "    model_metrics.append(metrics)\n",
    "    all_amplitudes_diff.append(metrics.get(\"amplitudes_diff\", np.array([])))\n",
    "    all_average_amplitudes_diff.append(float(metrics.get(\"avg_amplitudes_diff\", float(\"nan\"))))\n",
    "    all_amplitudes_relative_diff.append(float(metrics.get(\"avg_relative_amp_err\", float(\"nan\"))))\n",
    "    all_complex_weights_pred.append(metrics.get(\"complex_weights_pred\", np.array([])))\n",
    "    all_image_data_pred.append(metrics.get(\"image_data_pred\", np.array([])))\n",
    "    all_cc_recon_amp.append(metrics.get(\"cc_recon_amp\", np.array([])))\n",
    "    all_cc_recon_phase.append(metrics.get(\"cc_recon_phase\", np.array([])))\n",
    "    all_cc_real.append(metrics.get(\"cc_real\", np.array([])))\n",
    "    all_cc_imag.append(metrics.get(\"cc_imag\", np.array([])))\n",
    "\n",
    "    print(\n",
    "        format_metric_report(\n",
    "            num_modes=num_modes,\n",
    "            phase_option=phase_option,\n",
    "            pred_case=pred_case,\n",
    "            label=f\"{num_layer} layers\",\n",
    "            metrics=metrics,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97deefda-1a9c-426a-b16d-7762694bbb1a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_training_summaries' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#%%存结果画图\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mall_training_summaries\u001b[49m:\n\u001b[32m      3\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTraining duration summary:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m summary \u001b[38;5;129;01min\u001b[39;00m all_training_summaries:\n",
      "\u001b[31mNameError\u001b[39m: name 'all_training_summaries' is not defined"
     ]
    }
   ],
   "source": [
    "if all_training_summaries:\n",
    "    print(\"\\nTraining duration summary:\")\n",
    "    for summary in all_training_summaries:\n",
    "        minutes = summary[\"total_time\"] / 60\n",
    "        print(\n",
    "            f\" - {summary['num_layers']} layers: {summary['total_time']:.2f} s \"\n",
    "            f\"(~{minutes:.2f} min)\"\n",
    "        )\n",
    "        print(f\"   Loss curve: {summary['loss_plot']}\")\n",
    "        print(f\"   Time curve: {summary['time_plot']}\")\n",
    "        print(f\"   Data (.mat): {summary['mat_path']}\")\n",
    "        print(f\"   Propagation plot: {summary['propagation_fig']}\")\n",
    "        print(f\"   Propagation data (.mat): {summary['propagation_mat']}\")\n",
    "        mode_triptychs = summary.get(\"mode_triptychs\", [])\n",
    "        if mode_triptychs:\n",
    "            print(\"   Mode triptychs:\")\n",
    "            for trip in mode_triptychs:\n",
    "                print(\n",
    "                    f\"     Mode {trip['mode']}: fig={trip['fig']}, mat={trip['mat']}\"\n",
    "                )\n",
    "\n",
    "save_dir = \"results/plots\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "num_samples_to_display = 6\n",
    "for idx, num_layer in enumerate(num_layer_option):\n",
    "    plot_amplitude_comparison_grid(\n",
    "        image_test_data,\n",
    "        all_image_data_pred[idx],\n",
    "        all_cc_recon_amp[idx],\n",
    "        max_samples=num_samples_to_display,\n",
    "        save_path=os.path.join(save_dir, f\"Amp_{num_layer}layers.png\"),\n",
    "        title=f\"Amp. distribution of Real and Predicted Images({num_layer}_layer_ODNN)\",\n",
    "    )\n",
    "\n",
    "# #直观的看看输出和label的差异\n",
    "# for s in [0, 1, 2, 5]:\n",
    "#     plot_sys_vs_label_strict(\n",
    "#         D2NN,\n",
    "#         test_dataset,\n",
    "#         sample_idx=s,\n",
    "#         evaluation_regions=evaluation_regions,\n",
    "#         detect_radius=detectsize,\n",
    "#         save_path=f\"results/plots/IO_Pred_Label_RAW_{s}.png\",\n",
    "#         device=device,\n",
    "#         use_big_canvas=False,\n",
    "#         sys_scale=\"bg_pct\",\n",
    "#         sys_pct=99.5,\n",
    "#         clip_pct=99.5,\n",
    "#         mask_roi_for_scale=True,\n",
    "#         show_signed=True,\n",
    "#     )\n",
    "#     plot_reconstruction_vs_input(\n",
    "#         image_test_data=image_test_data,\n",
    "#         reconstructed_fields=all_image_data_pred,\n",
    "#         sample_idx=s,\n",
    "#         model_idx=0,\n",
    "#         save_path=f\"results/plots/Reconstruction_vs_Input_{s}.png\",\n",
    "#     )\n",
    "\n",
    "# #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d282a0f-b41b-447d-9f85-5c304cc97648",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m由于上一个单元格中出现错误，单元格已取消。"
     ]
    }
   ],
   "source": [
    "temp_dataset = test_dataset\n",
    "FIXED_E_INDEX = 4\n",
    "\n",
    "def get_fixed_input(dataset, idx, device):\n",
    "    if isinstance(dataset, list):\n",
    "        sample = dataset[idx][0]\n",
    "    else:\n",
    "        sample = dataset.tensors[0][idx]\n",
    "    return sample.squeeze(0).to(device)\n",
    "\n",
    "\n",
    "assert len(temp_dataset) > 0, \"test_dataset 为空\"\n",
    "temp_E = get_fixed_input(temp_dataset, FIXED_E_INDEX % len(temp_dataset), device)\n",
    "\n",
    "z_start = 0.0\n",
    "z_step = 5e-6\n",
    "z_prop_plus = z_prop\n",
    "\n",
    "save_root = Path(\"results_MD\")\n",
    "save_root.mkdir(parents=True, exist_ok=True)\n",
    "run_stamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "filename_prefix = f\"ODNN_vis_{run_stamp}\"\n",
    "\n",
    "for i_model, phase_masks in enumerate(all_phase_masks, start=1):\n",
    "    model_dir = save_root / f\"m{i_model}\"\n",
    "    scans, camera_field = visualize_model_slices(\n",
    "        D2NN,\n",
    "        phase_masks,\n",
    "        temp_E,\n",
    "        output_dir=model_dir,\n",
    "        sample_tag=f\"m{i_model}\",\n",
    "        z_input_to_first=z_input_to_first,\n",
    "        z_layers=z_layers,\n",
    "        z_prop_plus=z_prop_plus,\n",
    "        z_step=z_step,\n",
    "        pixel_size=pixel_size,\n",
    "        wavelength=wavelength,\n",
    "    )\n",
    "\n",
    "    phase_stack = np.stack([np.asarray(mask, dtype=np.float32) for mask in phase_masks], axis=0)\n",
    "    meta = {\n",
    "        \"z_start\": float(z_start),\n",
    "        \"z_step\": float(z_step),\n",
    "        \"z_layers\": float(z_layers),\n",
    "        \"z_prop\": float(z_prop),\n",
    "        \"z_prop_plus\": float(z_prop_plus),\n",
    "        \"pixel_size\": float(pixel_size),\n",
    "        \"wavelength\": float(wavelength),\n",
    "        \"layer_size\": int(layer_size),\n",
    "        \"padding_ratio\": 0.5,\n",
    "    }\n",
    "\n",
    "    mat_path = model_dir / f\"{filename_prefix}_LIGHT_m{i_model}.mat\"\n",
    "    save_to_mat_light_plus(\n",
    "        mat_path,\n",
    "        phase_stack=phase_stack,\n",
    "        input_field=temp_E.detach().cpu().numpy(),\n",
    "        scans=scans,\n",
    "        camera_field=camera_field,\n",
    "        sample_stacks_kmax=20,\n",
    "        save_amplitude_only=False,\n",
    "        meta=meta,\n",
    "    )\n",
    "    print(\"Saved ->\", mat_path)\n",
    "\n",
    "    save_masks_one_file_per_layer(\n",
    "        phase_masks,\n",
    "        out_dir=model_dir,\n",
    "        base_name=f\"{filename_prefix}_MASK\",\n",
    "        save_degree=False,\n",
    "        use_xlsx=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "已重启 odnn_venv (Python 3.13.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c488d3-c155-498b-bb93-da601989ed3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device: cuda:5\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from scipy.io import savemat\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from ODNN_functions import (\n",
    "    create_evaluation_regions,\n",
    "    generate_complex_weights,\n",
    "    generate_fields_ts,\n",
    ")\n",
    "from odnn_generate_label import (\n",
    "    compute_label_centers,\n",
    "    compose_labels_from_patterns,\n",
    "    generate_detector_patterns,\n",
    ")\n",
    "from odnn_io import load_complex_modes_from_mat\n",
    "from odnn_model import D2NNModel\n",
    "from odnn_processing import prepare_sample\n",
    "from odnn_training_eval import (\n",
    "    build_superposition_eval_context,\n",
    "    compute_model_prediction_metrics,\n",
    "    evaluate_spot_metrics,\n",
    "    format_metric_report,\n",
    "    generate_superposition_sample,\n",
    "    infer_superposition_output,\n",
    ")\n",
    "from odnn_training_io import save_masks_one_file_per_layer, save_to_mat_light_plus\n",
    "from odnn_training_visualization import (\n",
    "    capture_eigenmode_propagation,\n",
    "    export_superposition_slices,\n",
    "    plot_amplitude_comparison_grid,\n",
    "    plot_reconstruction_vs_input,\n",
    "    plot_sys_vs_label_strict,\n",
    "    save_superposition_triptych,\n",
    "    save_mode_triptych,\n",
    "    visualize_model_slices,\n",
    ")\n",
    "\n",
    "SEED = 424242\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# 让 cuDNN/算子走确定性分支\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.use_deterministic_algorithms(True)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:5')           # 或者 'cuda:0'\n",
    "    print('Using Device:', device)\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('Using Device: CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f954334c-9122-4928-95c0-e6312626708f",
   "metadata": {},
   "outputs": [],
   "source": [
    "field_size = 25  #the field size in eigenmodes_OM4 is 50 pixels\n",
    "layer_size = 100 #400#300#100\n",
    "num_data = 1000 # options: 1. random datas 2.eigenmodes\n",
    "num_modes = 6 #the mode number of MMF 3 6 10\n",
    "circle_focus_radius = 5 # radius when using uniform circular detectors\n",
    "circle_detectsize = 10  # square window size for circular detectors\n",
    "eigenmode_focus_radius = 12.5  # radius when using eigenmode patterns\n",
    "eigenmode_detectsize = 27    # square window size for eigenmode patterns\n",
    "focus_radius = circle_focus_radius\n",
    "detectsize = circle_detectsize\n",
    "batch_size = 16\n",
    "\n",
    "# Evaluation selection: \"eigenmode\" uses the base modes, \"superposition\" samples random mixtures\n",
    "evaluation_mode = \"superposition\"  # options: \"eigenmode\", \"superposition\"\n",
    "num_superposition_eval_samples = 1000\n",
    "num_superposition_visual_samples = 20\n",
    "run_superposition_debug = True\n",
    "save_superposition_plots = True\n",
    "save_superposition_slices = True\n",
    "run_misalignment_robustness = True\n",
    "label_pattern_mode = \"circle\"  # options: \"eigenmode\", \"circle\"\n",
    "# Define multiple D2NN models \n",
    "num_layer_option = [3]   #, 3]#, 4]  # Define the different layer-number ODNN\n",
    "all_losses = [] #the loss for each epoch of each ODNN model\n",
    "all_phase_masks = [] #the phase masks field of each ODNN model\n",
    "all_predictions = [] #the output light field of each ODNN model\n",
    "model_metrics: list[dict] = []\n",
    "all_amplitudes_diff: list[np.ndarray] = []\n",
    "all_average_amplitudes_diff: list[float] = []\n",
    "all_amplitudes_relative_diff: list[float] = []\n",
    "all_complex_weights_pred: list[np.ndarray] = []\n",
    "all_image_data_pred: list[np.ndarray] = []\n",
    "all_cc_real: list[np.ndarray] = []\n",
    "all_cc_imag: list[np.ndarray] = []\n",
    "all_cc_recon_amp: list[np.ndarray] = []\n",
    "all_cc_recon_phase: list[np.ndarray] = []\n",
    "all_training_summaries: list[dict] = []\n",
    "# SLM\n",
    "z_layers   = 40e-6        # 原 47.571e-3  -> 40 μm\n",
    "pixel_size = 1e-6\n",
    "z_prop     = 120e-6        # 原 16.74e-2   -> 60 μm plus 40（最后一层到相机）\n",
    "wavelength = 1568e-9      # 原 1568     -> 1550 nm\n",
    "z_input_to_first = 40e-6  # 40 μm # 新增：输入面到第一层的传播距离"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b902257e-f332-46f4-9b74-f654b6745653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded modes shape: (25, 25, 6) dtype: complex64\n"
     ]
    }
   ],
   "source": [
    "eigenmodes_OM4 = load_complex_modes_from_mat(\n",
    "    'mmf_6modes_25_PD_1.15.mat',\n",
    "    key='modes_field'\n",
    ")\n",
    "# (H, W, M)\n",
    "print(\"Loaded modes shape:\", eigenmodes_OM4.shape, \"dtype:\", eigenmodes_OM4.dtype)\n",
    "\n",
    "# 取前 num_modes 个 → (H, W, M_sel) → (M_sel, H, W)\n",
    "MMF_data = eigenmodes_OM4[:, :, :num_modes].transpose(2, 0, 1)\n",
    "MMF_data_amp_norm = (np.abs(MMF_data) - np.min(np.abs(MMF_data))) / (np.max(np.abs(MMF_data)) - np.min(np.abs(MMF_data)))\n",
    "\n",
    "MMF_data = MMF_data_amp_norm * np.exp(1j * np.angle(MMF_data))\n",
    "\n",
    "#要是以后确定了用4我在想要不要去掉其他选项\n",
    "phase_option = 4\n",
    "#phase_option 1: (0,0,...,0)\n",
    "#phase_option 2: (0,2pi,...,2pi)\n",
    "#phase_option 3: (0,pi,...,2pi)\n",
    "#phase_option 4: eigenmodes\n",
    "#phase_option 5: (0,pi,...,pi)\n",
    "\n",
    "if phase_option in [1, 2, 3, 5]:\n",
    "    amplitudes,phases = generate_complex_weights(num_data,num_modes,phase_option)\n",
    "\n",
    "if phase_option == 4:\n",
    "    num_data = num_modes # use the eigenmodes to train ODNN\n",
    "    amplitudes = np.eye(num_modes)#[[1,0,0][0,1,0][0,0,1]]\n",
    "    phases = np.eye(num_modes)\n",
    "\n",
    "amplitudes_phases_ori = np.hstack((amplitudes[:, :], phases[:, 1:]))  # amplitudes (l2 norm) phases\n",
    "amplitudes_phases = np.hstack((amplitudes[:, :], phases[:, 1:]/(2*np.pi)))  # amplitudes (l2 norm) phases (0-1)\n",
    "\n",
    "# Generate complex weights vector with specified amplitudes and phases\n",
    "complex_weights = amplitudes * np.exp(1j * phases)\n",
    "MMF_data_ts = torch.from_numpy(MMF_data)\n",
    "complex_weights_ts = torch.from_numpy(complex_weights)\n",
    "image_data = generate_fields_ts(complex_weights_ts, MMF_data_ts, num_data, num_modes, field_size).to(torch.complex64)\n",
    "image_test_data = image_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11b2fac-21e4-4d03-8b05-3eed4418d3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "相邻图案边缘间距： 行=26.67, 列=17.50\n",
      "相邻图案中心间距： 行=36.67, 列=27.50\n",
      "中心坐标： [(32, 22), (32, 50), (32, 78), (68, 22), (68, 50), (68, 78)]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "pred_case = 1: only amplitudes prediction\n",
    "pred_case = 2: only phases prediction\n",
    "pred_case = 3: amplitudes and phases prediction\n",
    "pred_case = 4: amplitudes and phases prediction (extra energy phase area)\n",
    "'''\n",
    "#\n",
    "pred_case = 1\n",
    "label_data = torch.zeros([num_data,1,layer_size,layer_size])\n",
    "label_size = layer_size\n",
    "\n",
    "if pred_case == 1: # 3\n",
    "    num_detector = num_modes\n",
    "    detector_focus_radius = focus_radius\n",
    "    detector_detectsize = detectsize\n",
    "    if label_pattern_mode == \"eigenmode\":\n",
    "        pattern_stack = np.transpose(np.abs(MMF_data), (1, 2, 0))\n",
    "        pattern_h, pattern_w, _ = pattern_stack.shape\n",
    "        if pattern_h > label_size or pattern_w > label_size:\n",
    "            raise ValueError(\n",
    "                f\"Eigenmode pattern size ({pattern_h}x{pattern_w}) exceeds label canvas {label_size}.\"\n",
    "            )\n",
    "        layout_radius = math.ceil(max(pattern_h, pattern_w) / 2)\n",
    "        detector_focus_radius = eigenmode_focus_radius\n",
    "        detector_detectsize = eigenmode_detectsize\n",
    "    elif label_pattern_mode == \"circle\":\n",
    "        circle_radius = circle_focus_radius\n",
    "        pattern_size = circle_radius * 2\n",
    "        if pattern_size % 2 == 0:\n",
    "            pattern_size += 1  \n",
    "        pattern_stack = generate_detector_patterns(pattern_size, pattern_size, num_detector, shape=\"circle\")\n",
    "        layout_radius = circle_radius\n",
    "        detector_focus_radius = circle_radius\n",
    "        detector_detectsize = circle_detectsize\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown label_pattern_mode: {label_pattern_mode}\")\n",
    "\n",
    "    centers, _, _ = compute_label_centers(label_size, label_size, num_detector, layout_radius)\n",
    "    mode_label_maps = [\n",
    "        compose_labels_from_patterns(\n",
    "            label_size,\n",
    "            label_size,\n",
    "            pattern_stack,\n",
    "            centers,\n",
    "            Index=i + 1,\n",
    "            visualize=False,\n",
    "        )\n",
    "        for i in range(num_detector)\n",
    "    ]\n",
    "    MMF_Label_data = torch.from_numpy(\n",
    "        np.stack(mode_label_maps, axis=2).astype(np.float32)\n",
    "    )\n",
    "    amplitude_weights = torch.from_numpy(amplitudes_phases[:num_data, 0:num_modes]).float()\n",
    "    combined_labels = (\n",
    "        amplitude_weights[:, None, None, :] * MMF_Label_data.unsqueeze(0)\n",
    "    ).sum(dim=3)\n",
    "    label_data[:, 0, :, :] = combined_labels\n",
    "    focus_radius = detector_focus_radius\n",
    "    detectsize = detector_detectsize\n",
    "\n",
    "label_test_data = label_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29ed02a-0b95-4c8f-82b3-c02264b4763c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detection Regions: [(17, 27, 27, 37), (45, 55, 27, 37), (73, 83, 27, 37), (17, 27, 63, 73), (45, 55, 63, 73), (73, 83, 63, 73)]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = [\n",
    "    prepare_sample(image_data[i], label_data[i], layer_size) for i in range(len(label_data))\n",
    "]\n",
    "train_tensor_data = TensorDataset(*[torch.stack(tensors) for tensors in zip(*train_dataset)])\n",
    "g = torch.Generator()\n",
    "g.manual_seed(SEED)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_tensor_data,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,               # 顺序会被 g 固定\n",
    "    generator=g,                # 固定打乱\n",
    "   \n",
    ")\n",
    "\n",
    "superposition_eval_ctx: dict | None = None\n",
    "if evaluation_mode == \"eigenmode\":\n",
    "    test_dataset = train_dataset\n",
    "    test_tensor_data = train_tensor_data\n",
    "    test_loader = DataLoader(test_tensor_data, batch_size=batch_size, shuffle=False)\n",
    "    eval_amplitudes = amplitudes\n",
    "    eval_amplitudes_phases = amplitudes_phases\n",
    "    eval_phases = phases\n",
    "    image_test_data = image_data\n",
    "elif evaluation_mode == \"superposition\":\n",
    "    if pred_case != 1:\n",
    "        raise ValueError(\"Superposition evaluation mode currently supports pred_case == 1 only.\")\n",
    "    super_ctx = build_superposition_eval_context(\n",
    "        num_superposition_eval_samples,\n",
    "        num_modes=num_modes,\n",
    "        field_size=field_size,\n",
    "        layer_size=layer_size,\n",
    "        mmf_modes=MMF_data_ts,\n",
    "        mmf_label_data=MMF_Label_data,\n",
    "        batch_size=batch_size,\n",
    "        second_mode_half_range=True,\n",
    "    )\n",
    "    test_dataset = super_ctx[\"dataset\"]\n",
    "    test_tensor_data = super_ctx[\"tensor_dataset\"]\n",
    "    test_loader = super_ctx[\"loader\"]\n",
    "    image_test_data = super_ctx[\"image_data\"]\n",
    "    eval_amplitudes = super_ctx[\"amplitudes\"]\n",
    "    eval_amplitudes_phases = super_ctx[\"amplitudes_phases\"]\n",
    "    eval_phases = super_ctx[\"phases\"]\n",
    "    superposition_eval_ctx = super_ctx\n",
    "else:\n",
    "    raise ValueError(f\"Unknown evaluation_mode: {evaluation_mode}\")\n",
    "\n",
    "# Generate detection regions using existing function\n",
    "if pred_case ==1:\n",
    "    evaluation_regions = create_evaluation_regions(layer_size, layer_size, num_detector, focus_radius, detectsize)\n",
    "    print(\"Detection Regions:\", evaluation_regions)\n",
    "\n",
    "\n",
    "def shift_complex_batch(batch: torch.Tensor, shift_y: int, shift_x: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Translate a batch of complex fields by (shift_y, shift_x) pixels with zero padding.\n",
    "    Positive shift_y moves downward; positive shift_x moves right.\n",
    "    \"\"\"\n",
    "    if shift_y == 0 and shift_x == 0:\n",
    "        return batch\n",
    "\n",
    "    _, _, height, width = batch.shape\n",
    "    if abs(shift_y) >= height or abs(shift_x) >= width:\n",
    "        return torch.zeros_like(batch)\n",
    "\n",
    "    real_imag = torch.view_as_real(batch)\n",
    "    shifted = torch.zeros_like(real_imag)\n",
    "\n",
    "    if shift_y >= 0:\n",
    "        src_y = slice(0, height - shift_y)\n",
    "        dst_y = slice(shift_y, height)\n",
    "    else:\n",
    "        src_y = slice(-shift_y, height)\n",
    "        dst_y = slice(0, height + shift_y)\n",
    "\n",
    "    if shift_x >= 0:\n",
    "        src_x = slice(0, width - shift_x)\n",
    "        dst_x = slice(shift_x, width)\n",
    "    else:\n",
    "        src_x = slice(-shift_x, width)\n",
    "        dst_x = slice(0, width + shift_x)\n",
    "\n",
    "    shifted[:, :, dst_y, dst_x, :] = real_imag[:, :, src_y, src_x, :]\n",
    "    return torch.view_as_complex(shifted)\n",
    "\n",
    "\n",
    "def compute_amp_relative_error_with_shift(\n",
    "    model: torch.nn.Module,\n",
    "    loader,\n",
    "    *,\n",
    "    shift_y_px: int,\n",
    "    shift_x_px: int,\n",
    "    evaluation_regions,\n",
    "    pred_case: int,\n",
    "    num_modes: int,\n",
    "    eval_amplitudes: np.ndarray,\n",
    "    eval_amplitudes_phases: np.ndarray,\n",
    "    eval_phases: np.ndarray,\n",
    "    phase_option: int,\n",
    "    mmf_modes: torch.Tensor,\n",
    "    field_size: int,\n",
    "    image_test_data: torch.Tensor,\n",
    "    device: torch.device,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Evaluate amplitude-related metrics when the input field is shifted by (shift_y_px, shift_x_px).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_weights_pred: list[np.ndarray] = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, _ in loader:\n",
    "            images = images.to(device, dtype=torch.complex64, non_blocking=True)\n",
    "            shifted_images = shift_complex_batch(images, shift_y_px, shift_x_px)\n",
    "            preds = model(shifted_images)\n",
    "            preds_np = preds.detach().cpu().numpy()\n",
    "\n",
    "            for sample_idx in range(preds_np.shape[0]):\n",
    "                intensity_map = preds_np[sample_idx, 0]\n",
    "                weights = []\n",
    "                for (x0, x1, y0, y1) in evaluation_regions:\n",
    "                    weights.append(float(intensity_map[y0:y1, x0:x1].mean()))\n",
    "                weights = np.asarray(weights, dtype=np.float64)\n",
    "\n",
    "                if pred_case == 3 and num_modes <= len(weights):\n",
    "                    norm_val = np.linalg.norm(weights[:num_modes])\n",
    "                    if norm_val > 0:\n",
    "                        weights[:num_modes] /= norm_val\n",
    "                else:\n",
    "                    norm_val = np.linalg.norm(weights)\n",
    "                    if norm_val > 0:\n",
    "                        weights /= norm_val\n",
    "\n",
    "                all_weights_pred.append(weights)\n",
    "\n",
    "    metrics = compute_model_prediction_metrics(\n",
    "        all_weights_pred,\n",
    "        eval_amplitudes,\n",
    "        eval_amplitudes_phases,\n",
    "        eval_phases,\n",
    "        phase_option,\n",
    "        pred_case,\n",
    "        num_modes,\n",
    "        mmf_modes,\n",
    "        field_size,\n",
    "        image_test_data,\n",
    "    )\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3128f728-7893-4992-8f09-cd4a7d391201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training D2NN with 3 layers...\n",
      "\n",
      "D2NNModel(\n",
      "  (pre_propagation): Propagation()\n",
      "  (layers): ModuleList(\n",
      "    (0-2): 3 x DiffractionLayer()\n",
      "  )\n",
      "  (propagation): Propagation()\n",
      "  (regression): RegressionDetector()\n",
      ")\n",
      "Epoch [1/1000], Loss: 0.008090289309620857, Epoch Time: 0.09 seconds\n",
      "Epoch [100/1000], Loss: 0.002760213334113359, Epoch Time: 0.01 seconds\n",
      "Epoch [200/1000], Loss: 0.002684592502191663, Epoch Time: 0.01 seconds\n",
      "Epoch [300/1000], Loss: 0.002658451907336712, Epoch Time: 0.01 seconds\n",
      "Epoch [400/1000], Loss: 0.002648203633725643, Epoch Time: 0.01 seconds\n",
      "Epoch [500/1000], Loss: 0.002644307911396027, Epoch Time: 0.01 seconds\n",
      "Epoch [600/1000], Loss: 0.002642895793542266, Epoch Time: 0.01 seconds\n",
      "Epoch [700/1000], Loss: 0.002642374718561769, Epoch Time: 0.01 seconds\n",
      "Epoch [800/1000], Loss: 0.002642178907990456, Epoch Time: 0.01 seconds\n",
      "Epoch [900/1000], Loss: 0.002642104867845774, Epoch Time: 0.01 seconds\n",
      "Epoch [1000/1000], Loss: 0.002642077626660466, Epoch Time: 0.01 seconds\n",
      "Total training time for 3-layer model: 10.74 seconds (~0.18 minutes)\n",
      "✔ Saved training loss plot -> results/training_analysis/loss_curve_layers3_20251104_142909.png\n",
      "✔ Saved cumulative time plot -> results/training_analysis/epoch_time_layers3_20251104_142909.png\n",
      "✔ Saved training log data (.mat) -> results/training_analysis/training_curves_layers3_20251104_142909.mat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ydzhang/Desktop/odnn_code/odnn_training_visualization.py:916: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "  plt.tight_layout(rect=[0, 0, 1, 0.97])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Saved eigenmode-3 propagation plot -> results/propagation_slices/propagation_mode3_layers3_20251104_142909.png\n",
      "✔ Saved eigenmode-3 propagation data (.mat) -> results/propagation_slices/propagation_mode3_layers3_20251104_142909.mat\n",
      "✔ Saved model -> checkpoints/odnn_3layers.pth\n",
      "3 layers: modes=6, phase_opt=4, pred_case=1\n",
      "  amp_err=0.085485, amp_err_rel=0.209394\n",
      "  snr_full=0.590977, snr_crop=0.831166, throughput=0.710960\n",
      "  cc_amp=0.963570±0.021786, cc_phase=0.793870±0.137667, cc_real=0.969919±0.025222, cc_imag=0.970586±0.030134\n"
     ]
    }
   ],
   "source": [
    "for num_layer in num_layer_option:\n",
    "    print(f\"\\nTraining D2NN with {num_layer} layers...\\n\")\n",
    "\n",
    "    D2NN = D2NNModel(\n",
    "        num_layers=num_layer,\n",
    "        layer_size=layer_size,\n",
    "        z_layers=z_layers,\n",
    "        z_prop=z_prop,\n",
    "        pixel_size=pixel_size,\n",
    "        wavelength=wavelength,\n",
    "        device=device,\n",
    "        padding_ratio=0.5,\n",
    "        z_input_to_first=z_input_to_first,   # NEW\n",
    "    ).to(device)\n",
    "\n",
    "    print(D2NN)\n",
    "\n",
    "    # Training\n",
    "    criterion = nn.MSELoss()  # Define loss function (对比的是loss)\n",
    "    optimizer = optim.Adam(D2NN.parameters(), lr=1.99) \n",
    "    scheduler = ExponentialLR(optimizer, gamma=0.99)  \n",
    "    epochs = 1000\n",
    "    losses = []\n",
    "    epoch_durations: list[float] = []\n",
    "    training_start_time = time.time()\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        D2NN.train()\n",
    "        epoch_loss = 0\n",
    "        for images, labels in train_loader:\n",
    "            images = images.to(device, dtype=torch.complex64, non_blocking=True)\n",
    "            labels = labels.to(device, dtype=torch.float32,   non_blocking=True)\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            outputs = D2NN(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        scheduler.step()\n",
    "        avg_loss = epoch_loss / len(train_loader)  # Calculate average loss for the epoch\n",
    "        losses.append(avg_loss)  # the loss for each model\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.synchronize(device)\n",
    "        epoch_duration = time.time() - epoch_start_time\n",
    "        epoch_durations.append(epoch_duration)\n",
    "\n",
    "        if epoch % 100 == 0 or epoch == 1 or epoch == epochs:\n",
    "            print(\n",
    "                f'Epoch [{epoch}/{epochs}], Loss: {avg_loss:.18f}, '\n",
    "                f'Epoch Time: {epoch_duration:.2f} seconds'\n",
    "            )\n",
    "\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.synchronize(device)\n",
    "    total_training_time = time.time() - training_start_time\n",
    "    print(\n",
    "        f'Total training time for {num_layer}-layer model: {total_training_time:.2f} seconds '\n",
    "        f'(~{total_training_time / 60:.2f} minutes)'\n",
    "    )\n",
    "    all_losses.append(losses)  # save the loss for each model\n",
    "    training_output_dir = Path(\"results/training_analysis\")\n",
    "    training_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    epochs_array = np.arange(1, epochs + 1, dtype=np.int32)\n",
    "    cumulative_epoch_times = np.cumsum(epoch_durations)\n",
    "    timestamp_tag = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(epochs_array, losses, label=\"Training Loss\")\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.set_title(f\"D2NN Training Loss ({num_layer} layers)\")\n",
    "    ax.grid(True, which=\"both\", linestyle=\"--\", linewidth=0.5)\n",
    "    ax.legend()\n",
    "    loss_plot_path = training_output_dir / f\"loss_curve_layers{num_layer}_{timestamp_tag}.png\"\n",
    "    fig.savefig(loss_plot_path, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "    fig_time, ax_time = plt.subplots()\n",
    "    ax_time.plot(epochs_array, cumulative_epoch_times, label=\"Cumulative Time\")\n",
    "    ax_time.set_xlabel(\"Epoch\")\n",
    "    ax_time.set_ylabel(\"Time (seconds)\")\n",
    "    ax_time.set_title(f\"Cumulative Training Time ({num_layer} layers)\")\n",
    "    ax_time.grid(True, which=\"both\", linestyle=\"--\", linewidth=0.5)\n",
    "    ax_time.legend()\n",
    "    time_plot_path = training_output_dir / f\"epoch_time_layers{num_layer}_{timestamp_tag}.png\"\n",
    "    fig_time.savefig(time_plot_path, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close(fig_time)\n",
    "\n",
    "    mat_path = training_output_dir / f\"training_curves_layers{num_layer}_{timestamp_tag}.mat\"\n",
    "    savemat(\n",
    "        str(mat_path),\n",
    "        {\n",
    "            \"epochs\": epochs_array,\n",
    "            \"losses\": np.array(losses, dtype=np.float64),\n",
    "            \"epoch_durations\": np.array(epoch_durations, dtype=np.float64),\n",
    "            \"cumulative_epoch_times\": np.array(cumulative_epoch_times, dtype=np.float64),\n",
    "            \"total_training_time\": np.array([total_training_time], dtype=np.float64),\n",
    "            \"num_layers\": np.array([num_layer], dtype=np.int32),\n",
    "        },\n",
    "    )\n",
    "\n",
    "    print(f\"✔ Saved training loss plot -> {loss_plot_path}\")\n",
    "    print(f\"✔ Saved cumulative time plot -> {time_plot_path}\")\n",
    "    print(f\"✔ Saved training log data (.mat) -> {mat_path}\")\n",
    "\n",
    "    propagation_dir = Path(\"results/propagation_slices\")\n",
    "    eigenmode_index = min(2, MMF_data_ts.shape[0] - 1)\n",
    "    propagation_summary = capture_eigenmode_propagation(\n",
    "        model=D2NN,\n",
    "        eigenmode_field=MMF_data_ts[eigenmode_index],\n",
    "        mode_index=eigenmode_index,\n",
    "        layer_size=layer_size,\n",
    "        z_input_to_first=z_input_to_first,\n",
    "        z_layers=z_layers,\n",
    "        z_prop=z_prop,\n",
    "        pixel_size=pixel_size,\n",
    "        wavelength=wavelength,\n",
    "        output_dir=propagation_dir,\n",
    "        tag=f\"layers{num_layer}_{timestamp_tag}\",\n",
    "    )\n",
    "    print(f\"✔ Saved eigenmode-{eigenmode_index + 1} propagation plot -> {propagation_summary['fig_path']}\")\n",
    "    print(f\"✔ Saved eigenmode-{eigenmode_index + 1} propagation data (.mat) -> {propagation_summary['mat_path']}\")\n",
    "\n",
    "    mode_triptych_records: list[dict[str, str | int]] = []\n",
    "    if evaluation_mode == \"eigenmode\":\n",
    "        triptych_dir = Path(\"results/mode_triptychs\")\n",
    "        mode_tag = f\"layers{num_layer}_{timestamp_tag}\"\n",
    "        for mode_idx in range(min(num_modes, len(MMF_data_ts))):\n",
    "            label_tensor = label_data[mode_idx, 0]\n",
    "            record = save_mode_triptych(\n",
    "                model=D2NN,\n",
    "                mode_index=mode_idx,\n",
    "                eigenmode_field=MMF_data_ts[mode_idx],\n",
    "                label_field=label_tensor,\n",
    "                layer_size=layer_size,\n",
    "                output_dir=triptych_dir,\n",
    "                tag=mode_tag,\n",
    "            )\n",
    "            mode_triptych_records.append(\n",
    "                {\n",
    "                    \"mode\": mode_idx + 1,\n",
    "                    \"fig\": record[\"fig_path\"],\n",
    "                    \"mat\": record[\"mat_path\"],\n",
    "                }\n",
    "            )\n",
    "            print(\n",
    "                f\"✔ Saved mode {mode_idx + 1} triptych -> {record['fig_path']}\\n\"\n",
    "                f\"  MAT -> {record['mat_path']}\"\n",
    "            )\n",
    "\n",
    "    all_training_summaries.append(\n",
    "        {\n",
    "            \"num_layers\": num_layer,\n",
    "            \"total_time\": total_training_time,\n",
    "            \"loss_plot\": str(loss_plot_path),\n",
    "            \"time_plot\": str(time_plot_path),\n",
    "            \"mat_path\": str(mat_path),\n",
    "            \"propagation_fig\": propagation_summary[\"fig_path\"],\n",
    "            \"propagation_mat\": propagation_summary[\"mat_path\"],\n",
    "            \"mode_triptychs\": mode_triptych_records,\n",
    "        }\n",
    "    )\n",
    "   \n",
    "    # === after training ===\n",
    "    ckpt_dir = \"checkpoints\"\n",
    "    os.makedirs(ckpt_dir, exist_ok=True)\n",
    "\n",
    "    ckpt = {\n",
    "        \"state_dict\": D2NN.state_dict(),\n",
    "        \"meta\": {\n",
    "            \"num_layers\":        len(D2NN.layers),\n",
    "            \"layer_size\":        layer_size,\n",
    "            \"z_layers\":          z_layers,\n",
    "            \"z_prop\":            z_prop,\n",
    "            \"pixel_size\":        pixel_size,\n",
    "            \"wavelength\":        wavelength,\n",
    "            \"padding_ratio\":     0.5,         \n",
    "            \"field_size\":        field_size,  \n",
    "            \"num_modes\":         num_modes, \n",
    "            \"z_input_to_first\":  z_input_to_first, \n",
    "        }\n",
    "    }\n",
    "    save_path = os.path.join(ckpt_dir, f\"odnn_{len(D2NN.layers)}layers.pth\")\n",
    "    torch.save(ckpt, save_path)\n",
    "    print(\"✔ Saved model ->\", save_path)\n",
    "    # Free GPU memory\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Cache phase masks for later visualization/export\n",
    "    phase_masks = []\n",
    "    for layer in D2NN.layers:\n",
    "        phase_np = layer.phase.detach().cpu().numpy()\n",
    "        phase_masks.append(np.remainder(phase_np, 2 * np.pi))\n",
    "    all_phase_masks.append(phase_masks)\n",
    "\n",
    "    # Collect evaluation metrics for this model\n",
    "    metrics = evaluate_spot_metrics(\n",
    "        D2NN,\n",
    "        test_loader,\n",
    "        evaluation_regions,\n",
    "        detect_radius=detectsize,\n",
    "        device=device,\n",
    "        pred_case=pred_case,\n",
    "        num_modes=num_modes,\n",
    "        phase_option=phase_option,\n",
    "        amplitudes=eval_amplitudes,\n",
    "        amplitudes_phases=eval_amplitudes_phases,\n",
    "        phases=eval_phases,\n",
    "        mmf_modes=MMF_data_ts,\n",
    "        field_size=field_size,\n",
    "        image_test_data=image_test_data,\n",
    "    )\n",
    "\n",
    "    model_metrics.append(metrics)\n",
    "    all_amplitudes_diff.append(metrics.get(\"amplitudes_diff\", np.array([])))\n",
    "    all_average_amplitudes_diff.append(float(metrics.get(\"avg_amplitudes_diff\", float(\"nan\"))))\n",
    "    all_amplitudes_relative_diff.append(float(metrics.get(\"avg_relative_amp_err\", float(\"nan\"))))\n",
    "    all_complex_weights_pred.append(metrics.get(\"complex_weights_pred\", np.array([])))\n",
    "    all_image_data_pred.append(metrics.get(\"image_data_pred\", np.array([])))\n",
    "    all_cc_recon_amp.append(metrics.get(\"cc_recon_amp\", np.array([])))\n",
    "    all_cc_recon_phase.append(metrics.get(\"cc_recon_phase\", np.array([])))\n",
    "    all_cc_real.append(metrics.get(\"cc_real\", np.array([])))\n",
    "    all_cc_imag.append(metrics.get(\"cc_imag\", np.array([])))\n",
    "\n",
    "    print(\n",
    "        format_metric_report(\n",
    "            num_modes=num_modes,\n",
    "            phase_option=phase_option,\n",
    "            pred_case=pred_case,\n",
    "            label=f\"{num_layer} layers\",\n",
    "            metrics=metrics,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf556c4-00e7-449a-8cb4-5dbf1ce77a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training duration summary:\n",
      " - 3 layers: 10.74 s (~0.18 min)\n",
      "   Loss curve: results/training_analysis/loss_curve_layers3_20251104_142909.png\n",
      "   Time curve: results/training_analysis/epoch_time_layers3_20251104_142909.png\n",
      "   Data (.mat): results/training_analysis/training_curves_layers3_20251104_142909.mat\n",
      "   Propagation plot: results/propagation_slices/propagation_mode3_layers3_20251104_142909.png\n",
      "   Propagation data (.mat): results/propagation_slices/propagation_mode3_layers3_20251104_142909.mat\n",
      "✔ Saved: /home/ydzhang/Desktop/odnn_code/results/plots/Amp_3layers.png\n"
     ]
    }
   ],
   "source": [
    "if all_training_summaries:\n",
    "    print(\"\\nTraining duration summary:\")\n",
    "    for summary in all_training_summaries:\n",
    "        minutes = summary[\"total_time\"] / 60\n",
    "        print(\n",
    "            f\" - {summary['num_layers']} layers: {summary['total_time']:.2f} s \"\n",
    "            f\"(~{minutes:.2f} min)\"\n",
    "        )\n",
    "        print(f\"   Loss curve: {summary['loss_plot']}\")\n",
    "        print(f\"   Time curve: {summary['time_plot']}\")\n",
    "        print(f\"   Data (.mat): {summary['mat_path']}\")\n",
    "        print(f\"   Propagation plot: {summary['propagation_fig']}\")\n",
    "        print(f\"   Propagation data (.mat): {summary['propagation_mat']}\")\n",
    "        mode_triptychs = summary.get(\"mode_triptychs\", [])\n",
    "        if mode_triptychs:\n",
    "            print(\"   Mode triptychs:\")\n",
    "            for trip in mode_triptychs:\n",
    "                print(\n",
    "                    f\"     Mode {trip['mode']}: fig={trip['fig']}, mat={trip['mat']}\"\n",
    "                )\n",
    "\n",
    "save_dir = \"results/plots\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "num_samples_to_display = 6\n",
    "for idx, num_layer in enumerate(num_layer_option):\n",
    "    plot_amplitude_comparison_grid(\n",
    "        image_test_data,\n",
    "        all_image_data_pred[idx],\n",
    "        all_cc_recon_amp[idx],\n",
    "        max_samples=num_samples_to_display,\n",
    "        save_path=os.path.join(save_dir, f\"Amp_{num_layer}layers.png\"),\n",
    "        title=f\"Amp. distribution of Real and Predicted Images({num_layer}_layer_ODNN)\",\n",
    "    )\n",
    "\n",
    "# #直观的看看输出和label的差异\n",
    "# for s in [0, 1, 2, 5]:\n",
    "#     plot_sys_vs_label_strict(\n",
    "#         D2NN,\n",
    "#         test_dataset,\n",
    "#         sample_idx=s,\n",
    "#         evaluation_regions=evaluation_regions,\n",
    "#         detect_radius=detectsize,\n",
    "#         save_path=f\"results/plots/IO_Pred_Label_RAW_{s}.png\",\n",
    "#         device=device,\n",
    "#         use_big_canvas=False,\n",
    "#         sys_scale=\"bg_pct\",\n",
    "#         sys_pct=99.5,\n",
    "#         clip_pct=99.5,\n",
    "#         mask_roi_for_scale=True,\n",
    "#         show_signed=True,\n",
    "#     )\n",
    "#     plot_reconstruction_vs_input(\n",
    "#         image_test_data=image_test_data,\n",
    "#         reconstructed_fields=all_image_data_pred,\n",
    "#         sample_idx=s,\n",
    "#         model_idx=0,\n",
    "#         save_path=f\"results/plots/Reconstruction_vs_Input_{s}.png\",\n",
    "#     )\n",
    "\n",
    "# #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f86bf1-b382-4fe5-9d08-f86f95884c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ydzhang/Desktop/odnn_code/odnn_training_visualization.py:534: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "  plt.tight_layout()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved figure -> /home/ydzhang/Desktop/odnn_code/results_MD/m1/m1_scan_input.png\n",
      "Saved figure -> /home/ydzhang/Desktop/odnn_code/results_MD/m1/m1_scan_layer1.png\n",
      "Saved figure -> /home/ydzhang/Desktop/odnn_code/results_MD/m1/m1_scan_layer2.png\n",
      "Saved figure -> /home/ydzhang/Desktop/odnn_code/results_MD/m1/m1_scan_layer3.png\n",
      "Saved figure -> /home/ydzhang/Desktop/odnn_code/results_MD/m1/m1_scan_to_camera.png\n",
      "Saved (v5 plus): results_MD/m1/ODNN_vis_20251104_142915_LIGHT_m1.mat\n",
      "Saved -> results_MD/m1/ODNN_vis_20251104_142915_LIGHT_m1.mat\n"
     ]
    }
   ],
   "source": [
    "temp_dataset = test_dataset\n",
    "FIXED_E_INDEX = 4\n",
    "\n",
    "def get_fixed_input(dataset, idx, device):\n",
    "    if isinstance(dataset, list):\n",
    "        sample = dataset[idx][0]\n",
    "    else:\n",
    "        sample = dataset.tensors[0][idx]\n",
    "    return sample.squeeze(0).to(device)\n",
    "\n",
    "\n",
    "assert len(temp_dataset) > 0, \"test_dataset 为空\"\n",
    "temp_E = get_fixed_input(temp_dataset, FIXED_E_INDEX % len(temp_dataset), device)\n",
    "\n",
    "z_start = 0.0\n",
    "z_step = 5e-6\n",
    "z_prop_plus = z_prop\n",
    "\n",
    "save_root = Path(\"results_MD\")\n",
    "save_root.mkdir(parents=True, exist_ok=True)\n",
    "run_stamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "filename_prefix = f\"ODNN_vis_{run_stamp}\"\n",
    "\n",
    "for i_model, phase_masks in enumerate(all_phase_masks, start=1):\n",
    "    model_dir = save_root / f\"m{i_model}\"\n",
    "    scans, camera_field = visualize_model_slices(\n",
    "        D2NN,\n",
    "        phase_masks,\n",
    "        temp_E,\n",
    "        output_dir=model_dir,\n",
    "        sample_tag=f\"m{i_model}\",\n",
    "        z_input_to_first=z_input_to_first,\n",
    "        z_layers=z_layers,\n",
    "        z_prop_plus=z_prop_plus,\n",
    "        z_step=z_step,\n",
    "        pixel_size=pixel_size,\n",
    "        wavelength=wavelength,\n",
    "    )\n",
    "\n",
    "    phase_stack = np.stack([np.asarray(mask, dtype=np.float32) for mask in phase_masks], axis=0)\n",
    "    meta = {\n",
    "        \"z_start\": float(z_start),\n",
    "        \"z_step\": float(z_step),\n",
    "        \"z_layers\": float(z_layers),\n",
    "        \"z_prop\": float(z_prop),\n",
    "        \"z_prop_plus\": float(z_prop_plus),\n",
    "        \"pixel_size\": float(pixel_size),\n",
    "        \"wavelength\": float(wavelength),\n",
    "        \"layer_size\": int(layer_size),\n",
    "        \"padding_ratio\": 0.5,\n",
    "    }\n",
    "\n",
    "    mat_path = model_dir / f\"{filename_prefix}_LIGHT_m{i_model}.mat\"\n",
    "    save_to_mat_light_plus(\n",
    "        mat_path,\n",
    "        phase_stack=phase_stack,\n",
    "        input_field=temp_E.detach().cpu().numpy(),\n",
    "        scans=scans,\n",
    "        camera_field=camera_field,\n",
    "        sample_stacks_kmax=20,\n",
    "        save_amplitude_only=False,\n",
    "        meta=meta,\n",
    "    )\n",
    "    print(\"Saved ->\", mat_path)\n",
    "\n",
    "    save_masks_one_file_per_layer(\n",
    "        phase_masks,\n",
    "        out_dir=model_dir,\n",
    "        base_name=f\"{filename_prefix}_MASK\",\n",
    "        save_degree=False,\n",
    "        use_xlsx=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71ff548-b508-4263-8c38-a631e09f5c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Superposition] 20251104_142934_s00 label weights: [0.6808 0.1229 0.4798 0.2562 0.1763 0.441 ] (sum_sq=1.000000)\n",
      "[Superposition] 20251104_142934_s00 predicted weights: [0.6607 0.1947 0.463  0.2374 0.1971 0.4648] (sum_sq=1.000000)\n",
      "Superposition sample 1/20 -> results_superposition/super_triptych_20251104_142934_s00.png\n",
      "  MAT saved -> results_superposition/super_triptych_20251104_142934_s00.mat\n",
      "[Superposition] 20251104_142934_s01 label weights: [0.5822 0.5824 0.1089 0.1084 0.2438 0.4887] (sum_sq=1.000000)\n",
      "[Superposition] 20251104_142934_s01 predicted weights: [0.5665 0.6057 0.1407 0.1843 0.2586 0.4377] (sum_sq=1.000000)\n",
      "Superposition sample 2/20 -> results_superposition/super_triptych_20251104_142934_s01.png\n",
      "  MAT saved -> results_superposition/super_triptych_20251104_142934_s01.mat\n",
      "[Superposition] 20251104_142934_s02 label weights: [0.5402 0.2013 0.0856 0.2285 0.401  0.6688] (sum_sq=1.000000)\n",
      "[Superposition] 20251104_142934_s02 predicted weights: [0.5394 0.1929 0.1665 0.2175 0.4406 0.6346] (sum_sq=1.000000)\n",
      "Superposition sample 3/20 -> results_superposition/super_triptych_20251104_142934_s02.png\n",
      "  MAT saved -> results_superposition/super_triptych_20251104_142934_s02.mat\n",
      "[Superposition] 20251104_142934_s03 label weights: [0.1373 0.0699 0.0968 0.9749 0.0755 0.1034] (sum_sq=1.000000)\n",
      "[Superposition] 20251104_142934_s03 predicted weights: [0.1907 0.1568 0.1318 0.9344 0.1343 0.1746] (sum_sq=1.000000)\n",
      "Superposition sample 4/20 -> results_superposition/super_triptych_20251104_142934_s03.png\n",
      "  MAT saved -> results_superposition/super_triptych_20251104_142934_s03.mat\n",
      "[Superposition] 20251104_142934_s04 label weights: [0.4262 0.4744 0.1902 0.4431 0.1108 0.5903] (sum_sq=1.000000)\n",
      "[Superposition] 20251104_142934_s04 predicted weights: [0.4162 0.4755 0.1926 0.4874 0.1127 0.5597] (sum_sq=1.000000)\n",
      "Superposition sample 5/20 -> results_superposition/super_triptych_20251104_142934_s04.png\n",
      "  MAT saved -> results_superposition/super_triptych_20251104_142934_s04.mat\n",
      "[Superposition] 20251104_142934_s05 label weights: [0.6811 0.5378 0.4401 0.0339 0.2098 0.0897] (sum_sq=1.000000)\n",
      "[Superposition] 20251104_142934_s05 predicted weights: [0.6183 0.598  0.4228 0.1285 0.2131 0.1392] (sum_sq=1.000000)\n",
      "Superposition sample 6/20 -> results_superposition/super_triptych_20251104_142934_s05.png\n",
      "  MAT saved -> results_superposition/super_triptych_20251104_142934_s05.mat\n",
      "[Superposition] 20251104_142934_s06 label weights: [0.0583 0.2723 0.0164 0.6513 0.4637 0.532 ] (sum_sq=1.000000)\n",
      "[Superposition] 20251104_142934_s06 predicted weights: [0.1344 0.2899 0.115  0.6015 0.4756 0.5446] (sum_sq=1.000000)\n",
      "Superposition sample 7/20 -> results_superposition/super_triptych_20251104_142934_s06.png\n",
      "  MAT saved -> results_superposition/super_triptych_20251104_142934_s06.mat\n",
      "[Superposition] 20251104_142934_s07 label weights: [0.2912 0.5366 0.4507 0.4624 0.4551 0.057 ] (sum_sq=1.000000)\n",
      "[Superposition] 20251104_142934_s07 predicted weights: [0.3032 0.5206 0.4132 0.5233 0.4228 0.1171] (sum_sq=1.000000)\n",
      "Superposition sample 8/20 -> results_superposition/super_triptych_20251104_142934_s07.png\n",
      "  MAT saved -> results_superposition/super_triptych_20251104_142934_s07.mat\n",
      "[Superposition] 20251104_142934_s08 label weights: [0.5056 0.4579 0.2459 0.177  0.3887 0.5402] (sum_sq=1.000000)\n",
      "[Superposition] 20251104_142934_s08 predicted weights: [0.4802 0.4355 0.2473 0.2132 0.4333 0.5343] (sum_sq=1.000000)\n",
      "Superposition sample 9/20 -> results_superposition/super_triptych_20251104_142934_s08.png\n",
      "  MAT saved -> results_superposition/super_triptych_20251104_142934_s08.mat\n",
      "[Superposition] 20251104_142934_s09 label weights: [0.3151 0.1009 0.4114 0.5255 0.3573 0.5635] (sum_sq=1.000000)\n",
      "[Superposition] 20251104_142934_s09 predicted weights: [0.329  0.1363 0.3941 0.5688 0.3437 0.5256] (sum_sq=1.000000)\n",
      "Superposition sample 10/20 -> results_superposition/super_triptych_20251104_142934_s09.png\n",
      "  MAT saved -> results_superposition/super_triptych_20251104_142934_s09.mat\n",
      "[Superposition] 20251104_142934_s10 label weights: [0.3225 0.3144 0.2544 0.1185 0.1044 0.8411] (sum_sq=1.000000)\n",
      "[Superposition] 20251104_142934_s10 predicted weights: [0.3557 0.3402 0.2768 0.1428 0.137  0.8012] (sum_sq=1.000000)\n",
      "Superposition sample 11/20 -> results_superposition/super_triptych_20251104_142934_s10.png\n",
      "  MAT saved -> results_superposition/super_triptych_20251104_142934_s10.mat\n",
      "[Superposition] 20251104_142934_s11 label weights: [0.2962 0.5527 0.0321 0.5136 0.4807 0.333 ] (sum_sq=1.000000)\n",
      "[Superposition] 20251104_142934_s11 predicted weights: [0.2646 0.5953 0.1136 0.4471 0.4681 0.3791] (sum_sq=1.000000)\n",
      "Superposition sample 12/20 -> results_superposition/super_triptych_20251104_142934_s11.png\n",
      "  MAT saved -> results_superposition/super_triptych_20251104_142934_s11.mat\n",
      "[Superposition] 20251104_142934_s12 label weights: [0.3564 0.5978 0.392  0.4296 0.4052 0.1151] (sum_sq=1.000000)\n",
      "[Superposition] 20251104_142934_s12 predicted weights: [0.3496 0.626  0.3933 0.4018 0.397  0.1102] (sum_sq=1.000000)\n",
      "Superposition sample 13/20 -> results_superposition/super_triptych_20251104_142934_s12.png\n",
      "  MAT saved -> results_superposition/super_triptych_20251104_142934_s12.mat\n",
      "[Superposition] 20251104_142934_s13 label weights: [0.249  0.2125 0.6079 0.2883 0.6634 0.0114] (sum_sq=1.000000)\n",
      "[Superposition] 20251104_142934_s13 predicted weights: [0.244  0.2551 0.5928 0.2346 0.6745 0.118 ] (sum_sq=1.000000)\n",
      "Superposition sample 14/20 -> results_superposition/super_triptych_20251104_142934_s13.png\n",
      "  MAT saved -> results_superposition/super_triptych_20251104_142934_s13.mat\n",
      "[Superposition] 20251104_142934_s14 label weights: [0.632  0.3087 0.3531 0.0643 0.6004 0.1264] (sum_sq=1.000000)\n",
      "[Superposition] 20251104_142934_s14 predicted weights: [0.5588 0.3416 0.3801 0.0997 0.622  0.1724] (sum_sq=1.000000)\n",
      "Superposition sample 15/20 -> results_superposition/super_triptych_20251104_142934_s14.png\n",
      "  MAT saved -> results_superposition/super_triptych_20251104_142934_s14.mat\n",
      "[Superposition] 20251104_142934_s15 label weights: [0.5311 0.1812 0.3345 0.5062 0.5349 0.1757] (sum_sq=1.000000)\n",
      "[Superposition] 20251104_142934_s15 predicted weights: [0.4907 0.2443 0.3832 0.4904 0.535  0.1608] (sum_sq=1.000000)\n",
      "Superposition sample 16/20 -> results_superposition/super_triptych_20251104_142934_s15.png\n",
      "  MAT saved -> results_superposition/super_triptych_20251104_142934_s15.mat\n",
      "[Superposition] 20251104_142934_s16 label weights: [0.463  0.2636 0.6372 0.1885 0.5235 0.023 ] (sum_sq=1.000000)\n",
      "[Superposition] 20251104_142934_s16 predicted weights: [0.407  0.2944 0.6204 0.1977 0.5487 0.1505] (sum_sq=1.000000)\n",
      "Superposition sample 17/20 -> results_superposition/super_triptych_20251104_142934_s16.png\n",
      "  MAT saved -> results_superposition/super_triptych_20251104_142934_s16.mat\n",
      "[Superposition] 20251104_142934_s17 label weights: [0.4591 0.3591 0.3431 0.3802 0.2795 0.5656] (sum_sq=1.000000)\n",
      "[Superposition] 20251104_142934_s17 predicted weights: [0.3883 0.3764 0.3629 0.3821 0.2849 0.5905] (sum_sq=1.000000)\n",
      "Superposition sample 18/20 -> results_superposition/super_triptych_20251104_142934_s17.png\n",
      "  MAT saved -> results_superposition/super_triptych_20251104_142934_s17.mat\n",
      "[Superposition] 20251104_142934_s18 label weights: [0.0054 0.1808 0.5336 0.2823 0.1223 0.7668] (sum_sq=1.000000)\n",
      "[Superposition] 20251104_142934_s18 predicted weights: [0.1144 0.1974 0.5419 0.2584 0.1569 0.7502] (sum_sq=1.000000)\n",
      "Superposition sample 19/20 -> results_superposition/super_triptych_20251104_142934_s18.png\n",
      "  MAT saved -> results_superposition/super_triptych_20251104_142934_s18.mat\n",
      "[Superposition] 20251104_142934_s19 label weights: [0.2487 0.5179 0.5697 0.0354 0.5629 0.165 ] (sum_sq=1.000000)\n",
      "[Superposition] 20251104_142934_s19 predicted weights: [0.2835 0.5039 0.5422 0.1332 0.5695 0.1723] (sum_sq=1.000000)\n",
      "Superposition sample 20/20 -> results_superposition/super_triptych_20251104_142934_s19.png\n",
      "  MAT saved -> results_superposition/super_triptych_20251104_142934_s19.mat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ydzhang/Desktop/odnn_code/odnn_training_visualization.py:534: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "  plt.tight_layout()\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 60\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m save_superposition_slices \u001b[38;5;129;01mand\u001b[39;00m all_phase_masks \u001b[38;5;129;01mand\u001b[39;00m slice_reference_input \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     59\u001b[39m     slices_root = super_dir / \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mslices_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msuper_tag\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m     \u001b[43mexport_superposition_slices\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m        \u001b[49m\u001b[43mD2NN\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[43m        \u001b[49m\u001b[43mall_phase_masks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[43m        \u001b[49m\u001b[43mslice_reference_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m        \u001b[49m\u001b[43mslices_root\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_tag\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msuperposition\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m        \u001b[49m\u001b[43mz_input_to_first\u001b[49m\u001b[43m=\u001b[49m\u001b[43mz_input_to_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m        \u001b[49m\u001b[43mz_layers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mz_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m        \u001b[49m\u001b[43mz_prop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mz_prop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m        \u001b[49m\u001b[43mz_step\u001b[49m\u001b[43m=\u001b[49m\u001b[43mz_step\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpixel_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpixel_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwavelength\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwavelength\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m super_records:\n\u001b[32m     75\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mSuperposition sample outputs:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/odnn_code/odnn_training_visualization.py:1196\u001b[39m, in \u001b[36mexport_superposition_slices\u001b[39m\u001b[34m(model, phase_masks_list, input_field, output_dir, sample_tag, z_input_to_first, z_layers, z_prop, z_step, pixel_size, wavelength)\u001b[39m\n\u001b[32m   1194\u001b[39m model_dir = output_dir / \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mm\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi_model\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1195\u001b[39m model_dir.mkdir(parents=\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m1196\u001b[39m _scans, camera_field = \u001b[43mvisualize_model_slices\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1197\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1198\u001b[39m \u001b[43m    \u001b[49m\u001b[43mphase_masks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1199\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_field\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1200\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1201\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_tag\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43msample_tag\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m_m\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi_model\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1202\u001b[39m \u001b[43m    \u001b[49m\u001b[43mz_input_to_first\u001b[49m\u001b[43m=\u001b[49m\u001b[43mz_input_to_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1203\u001b[39m \u001b[43m    \u001b[49m\u001b[43mz_layers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mz_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1204\u001b[39m \u001b[43m    \u001b[49m\u001b[43mz_prop_plus\u001b[49m\u001b[43m=\u001b[49m\u001b[43mz_prop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1205\u001b[39m \u001b[43m    \u001b[49m\u001b[43mz_step\u001b[49m\u001b[43m=\u001b[49m\u001b[43mz_step\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1206\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpixel_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpixel_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1207\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwavelength\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwavelength\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1208\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1209\u001b[39m np.savez(model_dir / \u001b[33m\"\u001b[39m\u001b[33mcamera_field_superposition.npz\u001b[39m\u001b[33m\"\u001b[39m, camera_field=camera_field)\n\u001b[32m   1210\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSuperposition slices saved -> \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_dir.resolve()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/odnn_code/odnn_training_visualization.py:615\u001b[39m, in \u001b[36mvisualize_model_slices\u001b[39m\u001b[34m(model, phase_layers, input_field, output_dir, sample_tag, z_input_to_first, z_layers, z_prop_plus, z_step, pixel_size, wavelength, kmax, ncols, cmap)\u001b[39m\n\u001b[32m    612\u001b[39m device = input_field.device\n\u001b[32m    613\u001b[39m scans: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, np.ndarray]] = {}\n\u001b[32m--> \u001b[39m\u001b[32m615\u001b[39m scan_input_stack, scan_input_z = \u001b[43mplot_propagated_field_padded\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    616\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_field\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    617\u001b[39m \u001b[43m    \u001b[49m\u001b[43mz_start\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    618\u001b[39m \u001b[43m    \u001b[49m\u001b[43mz_end\u001b[49m\u001b[43m=\u001b[49m\u001b[43mz_input_to_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    619\u001b[39m \u001b[43m    \u001b[49m\u001b[43mz_step\u001b[49m\u001b[43m=\u001b[49m\u001b[43mz_step\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    620\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpixel_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpixel_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    621\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwavelength\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwavelength\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpad_px\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpropagation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpad_px\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    623\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkmax\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkmax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    624\u001b[39m \u001b[43m    \u001b[49m\u001b[43mncols\u001b[49m\u001b[43m=\u001b[49m\u001b[43mncols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    625\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43msample_tag\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m_scan_input.png\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcmap\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcmap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    628\u001b[39m scans[\u001b[33m\"\u001b[39m\u001b[33mscan_input\u001b[39m\u001b[33m\"\u001b[39m] = {\n\u001b[32m    629\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mstack\u001b[39m\u001b[33m\"\u001b[39m: scan_input_stack.detach().cpu().numpy(),\n\u001b[32m    630\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mz\u001b[39m\u001b[33m\"\u001b[39m: scan_input_z.copy(),\n\u001b[32m    631\u001b[39m }\n\u001b[32m    633\u001b[39m field = input_field\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/odnn_code/odnn_training_visualization.py:537\u001b[39m, in \u001b[36mplot_propagated_field_padded\u001b[39m\u001b[34m(field, z_start, z_end, z_step, pixel_size, wavelength, pad_px, plot, kmax, ncols, save_path, mode, dpi, cmap, add_colorbar)\u001b[39m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m save_path:\n\u001b[32m    536\u001b[39m     Path(save_path).parent.mkdir(parents=\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m537\u001b[39m     \u001b[43mfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43msavefig\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdpi\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdpi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    538\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSaved figure ->\u001b[39m\u001b[33m\"\u001b[39m, Path(save_path).resolve())\n\u001b[32m    539\u001b[39m plt.close(fig)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/odnn_venv/lib/python3.13/site-packages/matplotlib/figure.py:3490\u001b[39m, in \u001b[36mFigure.savefig\u001b[39m\u001b[34m(self, fname, transparent, **kwargs)\u001b[39m\n\u001b[32m   3488\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m ax \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.axes:\n\u001b[32m   3489\u001b[39m         _recursively_make_axes_transparent(stack, ax)\n\u001b[32m-> \u001b[39m\u001b[32m3490\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcanvas\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprint_figure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/odnn_venv/lib/python3.13/site-packages/matplotlib/backend_bases.py:2186\u001b[39m, in \u001b[36mFigureCanvasBase.print_figure\u001b[39m\u001b[34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[39m\n\u001b[32m   2182\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   2183\u001b[39m     \u001b[38;5;66;03m# _get_renderer may change the figure dpi (as vector formats\u001b[39;00m\n\u001b[32m   2184\u001b[39m     \u001b[38;5;66;03m# force the figure dpi to 72), so we need to set it again here.\u001b[39;00m\n\u001b[32m   2185\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m cbook._setattr_cm(\u001b[38;5;28mself\u001b[39m.figure, dpi=dpi):\n\u001b[32m-> \u001b[39m\u001b[32m2186\u001b[39m         result = \u001b[43mprint_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2187\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2188\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfacecolor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfacecolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2189\u001b[39m \u001b[43m            \u001b[49m\u001b[43medgecolor\u001b[49m\u001b[43m=\u001b[49m\u001b[43medgecolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2190\u001b[39m \u001b[43m            \u001b[49m\u001b[43morientation\u001b[49m\u001b[43m=\u001b[49m\u001b[43morientation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2191\u001b[39m \u001b[43m            \u001b[49m\u001b[43mbbox_inches_restore\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_bbox_inches_restore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2192\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2193\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   2194\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m bbox_inches \u001b[38;5;129;01mand\u001b[39;00m restore_bbox:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/odnn_venv/lib/python3.13/site-packages/matplotlib/backend_bases.py:2042\u001b[39m, in \u001b[36mFigureCanvasBase._switch_canvas_and_return_print_method.<locals>.<lambda>\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   2038\u001b[39m     optional_kws = {  \u001b[38;5;66;03m# Passed by print_figure for other renderers.\u001b[39;00m\n\u001b[32m   2039\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mdpi\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfacecolor\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33medgecolor\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33morientation\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2040\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mbbox_inches_restore\u001b[39m\u001b[33m\"\u001b[39m}\n\u001b[32m   2041\u001b[39m     skip = optional_kws - {*inspect.signature(meth).parameters}\n\u001b[32m-> \u001b[39m\u001b[32m2042\u001b[39m     print_method = functools.wraps(meth)(\u001b[38;5;28;01mlambda\u001b[39;00m *args, **kwargs: \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2043\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mskip\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m   2044\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Let third-parties do as they see fit.\u001b[39;00m\n\u001b[32m   2045\u001b[39m     print_method = meth\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/odnn_venv/lib/python3.13/site-packages/matplotlib/backends/backend_agg.py:481\u001b[39m, in \u001b[36mFigureCanvasAgg.print_png\u001b[39m\u001b[34m(self, filename_or_obj, metadata, pil_kwargs)\u001b[39m\n\u001b[32m    434\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprint_png\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename_or_obj, *, metadata=\u001b[38;5;28;01mNone\u001b[39;00m, pil_kwargs=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    435\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    436\u001b[39m \u001b[33;03m    Write the figure to a PNG file.\u001b[39;00m\n\u001b[32m    437\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    479\u001b[39m \u001b[33;03m        *metadata*, including the default 'Software' key.\u001b[39;00m\n\u001b[32m    480\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m481\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_print_pil\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpng\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpil_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/odnn_venv/lib/python3.13/site-packages/matplotlib/backends/backend_agg.py:429\u001b[39m, in \u001b[36mFigureCanvasAgg._print_pil\u001b[39m\u001b[34m(self, filename_or_obj, fmt, pil_kwargs, metadata)\u001b[39m\n\u001b[32m    424\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_print_pil\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename_or_obj, fmt, pil_kwargs, metadata=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    425\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    426\u001b[39m \u001b[33;03m    Draw the canvas, then save it using `.image.imsave` (to which\u001b[39;00m\n\u001b[32m    427\u001b[39m \u001b[33;03m    *pil_kwargs* and *metadata* are forwarded).\u001b[39;00m\n\u001b[32m    428\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m429\u001b[39m     \u001b[43mFigureCanvasAgg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    430\u001b[39m     mpl.image.imsave(\n\u001b[32m    431\u001b[39m         filename_or_obj, \u001b[38;5;28mself\u001b[39m.buffer_rgba(), \u001b[38;5;28mformat\u001b[39m=fmt, origin=\u001b[33m\"\u001b[39m\u001b[33mupper\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    432\u001b[39m         dpi=\u001b[38;5;28mself\u001b[39m.figure.dpi, metadata=metadata, pil_kwargs=pil_kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/odnn_venv/lib/python3.13/site-packages/matplotlib/backends/backend_agg.py:382\u001b[39m, in \u001b[36mFigureCanvasAgg.draw\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    379\u001b[39m \u001b[38;5;66;03m# Acquire a lock on the shared font cache.\u001b[39;00m\n\u001b[32m    380\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.toolbar._wait_cursor_for_draw_cm() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.toolbar\n\u001b[32m    381\u001b[39m       \u001b[38;5;28;01melse\u001b[39;00m nullcontext()):\n\u001b[32m--> \u001b[39m\u001b[32m382\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfigure\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    383\u001b[39m     \u001b[38;5;66;03m# A GUI class may be need to update a window using this draw, so\u001b[39;00m\n\u001b[32m    384\u001b[39m     \u001b[38;5;66;03m# don't forget to call the superclass.\u001b[39;00m\n\u001b[32m    385\u001b[39m     \u001b[38;5;28msuper\u001b[39m().draw()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/odnn_venv/lib/python3.13/site-packages/matplotlib/artist.py:94\u001b[39m, in \u001b[36m_finalize_rasterization.<locals>.draw_wrapper\u001b[39m\u001b[34m(artist, renderer, *args, **kwargs)\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(draw)\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdraw_wrapper\u001b[39m(artist, renderer, *args, **kwargs):\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m     result = \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m renderer._rasterizing:\n\u001b[32m     96\u001b[39m         renderer.stop_rasterizing()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/odnn_venv/lib/python3.13/site-packages/matplotlib/artist.py:71\u001b[39m, in \u001b[36mallow_rasterization.<locals>.draw_wrapper\u001b[39m\u001b[34m(artist, renderer)\u001b[39m\n\u001b[32m     68\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m artist.get_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     69\u001b[39m         renderer.start_filter()\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     73\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m artist.get_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/odnn_venv/lib/python3.13/site-packages/matplotlib/figure.py:3257\u001b[39m, in \u001b[36mFigure.draw\u001b[39m\u001b[34m(self, renderer)\u001b[39m\n\u001b[32m   3254\u001b[39m             \u001b[38;5;66;03m# ValueError can occur when resizing a window.\u001b[39;00m\n\u001b[32m   3256\u001b[39m     \u001b[38;5;28mself\u001b[39m.patch.draw(renderer)\n\u001b[32m-> \u001b[39m\u001b[32m3257\u001b[39m     \u001b[43mmimage\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_draw_list_compositing_images\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3258\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43martists\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msuppressComposite\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3260\u001b[39m     renderer.close_group(\u001b[33m'\u001b[39m\u001b[33mfigure\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m   3261\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/odnn_venv/lib/python3.13/site-packages/matplotlib/image.py:134\u001b[39m, in \u001b[36m_draw_list_compositing_images\u001b[39m\u001b[34m(renderer, parent, artists, suppress_composite)\u001b[39m\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m not_composite \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_images:\n\u001b[32m    133\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m artists:\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m         \u001b[43ma\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    136\u001b[39m     \u001b[38;5;66;03m# Composite any adjacent images together\u001b[39;00m\n\u001b[32m    137\u001b[39m     image_group = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/odnn_venv/lib/python3.13/site-packages/matplotlib/artist.py:71\u001b[39m, in \u001b[36mallow_rasterization.<locals>.draw_wrapper\u001b[39m\u001b[34m(artist, renderer)\u001b[39m\n\u001b[32m     68\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m artist.get_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     69\u001b[39m         renderer.start_filter()\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     73\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m artist.get_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/odnn_venv/lib/python3.13/site-packages/matplotlib/axes/_base.py:3190\u001b[39m, in \u001b[36m_AxesBase.draw\u001b[39m\u001b[34m(self, renderer)\u001b[39m\n\u001b[32m   3187\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m spine \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.spines.values():\n\u001b[32m   3188\u001b[39m         artists.remove(spine)\n\u001b[32m-> \u001b[39m\u001b[32m3190\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_update_title_position\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3192\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.axison:\n\u001b[32m   3193\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m _axis \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._axis_map.values():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/odnn_venv/lib/python3.13/site-packages/matplotlib/axes/_base.py:3134\u001b[39m, in \u001b[36m_AxesBase._update_title_position\u001b[39m\u001b[34m(self, renderer)\u001b[39m\n\u001b[32m   3132\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m title.get_text():\n\u001b[32m   3133\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m ax \u001b[38;5;129;01min\u001b[39;00m axs:\n\u001b[32m-> \u001b[39m\u001b[32m3134\u001b[39m         \u001b[43max\u001b[49m\u001b[43m.\u001b[49m\u001b[43myaxis\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_tightbbox\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# update offsetText\u001b[39;00m\n\u001b[32m   3135\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m ax.yaxis.offsetText.get_text():\n\u001b[32m   3136\u001b[39m             bb = ax.yaxis.offsetText.get_tightbbox(renderer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/odnn_venv/lib/python3.13/site-packages/matplotlib/axis.py:1353\u001b[39m, in \u001b[36mAxis.get_tightbbox\u001b[39m\u001b[34m(self, renderer, for_layout_only)\u001b[39m\n\u001b[32m   1350\u001b[39m     renderer = \u001b[38;5;28mself\u001b[39m.get_figure(root=\u001b[38;5;28;01mTrue\u001b[39;00m)._get_renderer()\n\u001b[32m   1351\u001b[39m ticks_to_draw = \u001b[38;5;28mself\u001b[39m._update_ticks()\n\u001b[32m-> \u001b[39m\u001b[32m1353\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_update_label_position\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1355\u001b[39m \u001b[38;5;66;03m# go back to just this axis's tick labels\u001b[39;00m\n\u001b[32m   1356\u001b[39m tlb1, tlb2 = \u001b[38;5;28mself\u001b[39m._get_ticklabel_bboxes(ticks_to_draw, renderer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/odnn_venv/lib/python3.13/site-packages/matplotlib/axis.py:2675\u001b[39m, in \u001b[36mYAxis._update_label_position\u001b[39m\u001b[34m(self, renderer)\u001b[39m\n\u001b[32m   2671\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m   2673\u001b[39m \u001b[38;5;66;03m# get bounding boxes for this axis and any siblings\u001b[39;00m\n\u001b[32m   2674\u001b[39m \u001b[38;5;66;03m# that have been set by `fig.align_ylabels()`\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2675\u001b[39m bboxes, bboxes2 = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_tick_boxes_siblings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2676\u001b[39m x, y = \u001b[38;5;28mself\u001b[39m.label.get_position()\n\u001b[32m   2678\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.label_position == \u001b[33m'\u001b[39m\u001b[33mleft\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m   2679\u001b[39m     \u001b[38;5;66;03m# Union with extents of the left spine if present, of the axes otherwise.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/odnn_venv/lib/python3.13/site-packages/matplotlib/axis.py:2241\u001b[39m, in \u001b[36mAxis._get_tick_boxes_siblings\u001b[39m\u001b[34m(self, renderer)\u001b[39m\n\u001b[32m   2239\u001b[39m axis = ax._axis_map[name]\n\u001b[32m   2240\u001b[39m ticks_to_draw = axis._update_ticks()\n\u001b[32m-> \u001b[39m\u001b[32m2241\u001b[39m tlb, tlb2 = \u001b[43maxis\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_ticklabel_bboxes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mticks_to_draw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2242\u001b[39m bboxes.extend(tlb)\n\u001b[32m   2243\u001b[39m bboxes2.extend(tlb2)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/odnn_venv/lib/python3.13/site-packages/matplotlib/axis.py:1332\u001b[39m, in \u001b[36mAxis._get_ticklabel_bboxes\u001b[39m\u001b[34m(self, ticks, renderer)\u001b[39m\n\u001b[32m   1330\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m renderer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1331\u001b[39m     renderer = \u001b[38;5;28mself\u001b[39m.get_figure(root=\u001b[38;5;28;01mTrue\u001b[39;00m)._get_renderer()\n\u001b[32m-> \u001b[39m\u001b[32m1332\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ([\u001b[43mtick\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlabel1\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_window_extent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1333\u001b[39m          \u001b[38;5;28;01mfor\u001b[39;00m tick \u001b[38;5;129;01min\u001b[39;00m ticks \u001b[38;5;28;01mif\u001b[39;00m tick.label1.get_visible()],\n\u001b[32m   1334\u001b[39m         [tick.label2.get_window_extent(renderer)\n\u001b[32m   1335\u001b[39m          \u001b[38;5;28;01mfor\u001b[39;00m tick \u001b[38;5;129;01min\u001b[39;00m ticks \u001b[38;5;28;01mif\u001b[39;00m tick.label2.get_visible()])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/odnn_venv/lib/python3.13/site-packages/matplotlib/text.py:972\u001b[39m, in \u001b[36mText.get_window_extent\u001b[39m\u001b[34m(self, renderer, dpi)\u001b[39m\n\u001b[32m    970\u001b[39m x, y = \u001b[38;5;28mself\u001b[39m.get_unitless_position()\n\u001b[32m    971\u001b[39m x, y = \u001b[38;5;28mself\u001b[39m.get_transform().transform((x, y))\n\u001b[32m--> \u001b[39m\u001b[32m972\u001b[39m bbox = \u001b[43mbbox\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtranslated\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    973\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m bbox\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/odnn_venv/lib/python3.13/site-packages/matplotlib/transforms.py:619\u001b[39m, in \u001b[36mBboxBase.translated\u001b[39m\u001b[34m(self, tx, ty)\u001b[39m\n\u001b[32m    617\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtranslated\u001b[39m(\u001b[38;5;28mself\u001b[39m, tx, ty):\n\u001b[32m    618\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Construct a `Bbox` by translating this one by *tx* and *ty*.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m619\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBbox\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_points\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mty\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/odnn_venv/lib/python3.13/site-packages/matplotlib/transforms.py:758\u001b[39m, in \u001b[36mBbox.__init__\u001b[39m\u001b[34m(self, points, **kwargs)\u001b[39m\n\u001b[32m    751\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    752\u001b[39m \u001b[33;03mParameters\u001b[39;00m\n\u001b[32m    753\u001b[39m \u001b[33;03m----------\u001b[39;00m\n\u001b[32m    754\u001b[39m \u001b[33;03mpoints : `~numpy.ndarray`\u001b[39;00m\n\u001b[32m    755\u001b[39m \u001b[33;03m    A (2, 2) array of the form ``[[x0, y0], [x1, y1]]``.\u001b[39;00m\n\u001b[32m    756\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    757\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(**kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m758\u001b[39m points = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpoints\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m points.shape != (\u001b[32m2\u001b[39m, \u001b[32m2\u001b[39m):\n\u001b[32m    760\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mBbox points must be of the form \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    761\u001b[39m                      \u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[[x0, y0], [x1, y1]]\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "if pred_case == 1 and run_superposition_debug:\n",
    "    super_dir = Path(\"results_superposition\")\n",
    "    super_dir.mkdir(parents=True, exist_ok=True)\n",
    "    super_tag = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    super_records: list[dict[str, str | int]] = []\n",
    "    slice_reference_input: torch.Tensor | None = None\n",
    "\n",
    "    for sample_idx in range(num_superposition_visual_samples):\n",
    "        super_sample = generate_superposition_sample(\n",
    "            num_modes=num_modes,\n",
    "            field_size=field_size,\n",
    "            layer_size=layer_size,\n",
    "            mmf_modes=MMF_data_ts,\n",
    "            mmf_label_data=MMF_Label_data,\n",
    "        )\n",
    "        super_output_map = infer_superposition_output(\n",
    "            D2NN,\n",
    "            super_sample[\"padded_image\"],\n",
    "            device,\n",
    "        )\n",
    "\n",
    "        sample_tag = f\"{super_tag}_s{sample_idx:02d}\"\n",
    "        triptych_paths = save_superposition_triptych(\n",
    "            input_field=super_sample[\"padded_image\"][0],\n",
    "            output_intensity_map=super_output_map,\n",
    "            amplitudes=super_sample[\"amplitudes\"],\n",
    "            phases=super_sample[\"phases\"],\n",
    "            complex_weights=super_sample[\"complex_weights\"],\n",
    "            label_map=super_sample[\"padded_label\"][0],\n",
    "            evaluation_regions=evaluation_regions,\n",
    "            detect_radius=detectsize,\n",
    "            output_dir=super_dir,\n",
    "            tag=sample_tag,\n",
    "            save_plot=save_superposition_plots,\n",
    "        )\n",
    "        if triptych_paths[\"fig_path\"]:\n",
    "            print(\n",
    "                f\"Superposition sample {sample_idx + 1}/{num_superposition_visual_samples} -> \"\n",
    "                f\"{triptych_paths['fig_path']}\"\n",
    "            )\n",
    "        print(f\"  MAT saved -> {triptych_paths['mat_path']}\")\n",
    "\n",
    "        super_records.append(\n",
    "            {\n",
    "                \"index\": sample_idx,\n",
    "                \"tag\": sample_tag,\n",
    "                \"fig\": triptych_paths[\"fig_path\"] if triptych_paths else \"\",\n",
    "                \"mat\": triptych_paths[\"mat_path\"] if triptych_paths else \"\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "        if slice_reference_input is None:\n",
    "            slice_reference_input = (\n",
    "                super_sample[\"padded_image\"].squeeze(0).to(device, dtype=torch.complex64)\n",
    "            )\n",
    "\n",
    "    if save_superposition_slices and all_phase_masks and slice_reference_input is not None:\n",
    "        slices_root = super_dir / f\"slices_{super_tag}\"\n",
    "        export_superposition_slices(\n",
    "            D2NN,\n",
    "            all_phase_masks,\n",
    "            slice_reference_input,\n",
    "            slices_root,\n",
    "            sample_tag=\"superposition\",\n",
    "            z_input_to_first=z_input_to_first,\n",
    "            z_layers=z_layers,\n",
    "            z_prop=z_prop,\n",
    "            z_step=z_step,\n",
    "            pixel_size=pixel_size,\n",
    "            wavelength=wavelength,\n",
    "        )\n",
    "\n",
    "    if super_records:\n",
    "        print(\"\\nSuperposition sample outputs:\")\n",
    "        for record in super_records:\n",
    "            print(\n",
    "                f\" - Sample {record['index'] + 1:02d} ({record['tag']}): \"\n",
    "                f\"fig={record['fig']}, mat={record['mat']}\"\n",
    "            )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "odnn_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
